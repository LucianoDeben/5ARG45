{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import required libraries and modules\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"src\")))\n",
    "\n",
    "\n",
    "from utils import load_config\n",
    "from preprocess import split_data\n",
    "\n",
    "# Load Config\n",
    "config = load_config(\"../config.yaml\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21 14:15:11,314 - INFO - Loading datasets with sampling...\n",
      "2025-01-21 14:15:11,316 - INFO - Sampling 1000 rows from ..\\data/processed/preprocessed_tf.csv...\n",
      "2025-01-21 14:15:11,550 - INFO - Sampling 1000 rows from ..\\data/processed/preprocessed_landmark.csv...\n",
      "2025-01-21 14:15:11,754 - INFO - Sampling 1000 rows from ..\\data/processed/preprocessed_best_inferred.csv...\n",
      "2025-01-21 14:15:14,471 - INFO - Loading ..\\data/processed/preprocessed_gene.csv in chunks...\n",
      "2025-01-21 14:15:21,553 - INFO - Splitting datasets into train/val/test...\n",
      "2025-01-21 14:15:21,558 - INFO - Train Groups: 22 unique values.\n",
      "2025-01-21 14:15:21,558 - INFO - Validation Groups: 6 unique values.\n",
      "2025-01-21 14:15:21,558 - INFO - Test Groups: 4 unique values.\n",
      "2025-01-21 14:15:21,603 - INFO - Train Groups: 22 unique values.\n",
      "2025-01-21 14:15:21,604 - INFO - Validation Groups: 6 unique values.\n",
      "2025-01-21 14:15:21,606 - INFO - Test Groups: 4 unique values.\n",
      "2025-01-21 14:15:21,718 - INFO - Train Groups: 22 unique values.\n",
      "2025-01-21 14:15:21,718 - INFO - Validation Groups: 6 unique values.\n",
      "2025-01-21 14:15:21,718 - INFO - Test Groups: 4 unique values.\n",
      "2025-01-21 14:15:21,872 - INFO - Train Groups: 22 unique values.\n",
      "2025-01-21 14:15:21,872 - INFO - Validation Groups: 6 unique values.\n",
      "2025-01-21 14:15:21,872 - INFO - Test Groups: 4 unique values.\n"
     ]
    }
   ],
   "source": [
    "## Load, Split, and Preprocess Datase\n",
    "# Configurable parameters\n",
    "sample_size = 1000  # Number of rows to sample from each dataset\n",
    "chunk_size = 1000  # Chunk size for loading large datasets\n",
    "\n",
    "# Load datasets\n",
    "logging.info(\"Loading datasets with sampling...\")\n",
    "\n",
    "\n",
    "def load_sampled_data(file_path, sample_size, use_chunks=False, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Load and sample a dataset, with optional chunked loading for large files.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the dataset file.\n",
    "        sample_size (int): Number of rows to sample.\n",
    "        use_chunks (bool): Whether to load the dataset in chunks.\n",
    "        chunk_size (int, optional): Size of chunks if `use_chunks` is True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Sampled DataFrame.\n",
    "    \"\"\"\n",
    "    if use_chunks:\n",
    "        logging.info(f\"Loading {file_path} in chunks...\")\n",
    "        chunks = []\n",
    "        total_loaded = 0\n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "            if total_loaded >= sample_size:\n",
    "                break\n",
    "\n",
    "            # Determine how many rows to sample from this chunk\n",
    "            sample_rows = min(sample_size - total_loaded, len(chunk))\n",
    "            chunks.append(chunk.sample(sample_rows))\n",
    "            total_loaded += sample_rows\n",
    "\n",
    "        sampled_df = pd.concat(chunks, axis=0)\n",
    "        del chunks  # Free memory\n",
    "    else:\n",
    "        logging.info(f\"Sampling {sample_size} rows from {file_path}...\")\n",
    "        sampled_df = pd.read_csv(file_path, nrows=sample_size)\n",
    "\n",
    "    return sampled_df\n",
    "\n",
    "\n",
    "# Load data with sampling\n",
    "\n",
    "tf_df = load_sampled_data(config[\"data_paths\"][\"preprocessed_tf_file\"], sample_size)\n",
    "landmark_df = load_sampled_data(\n",
    "    config[\"data_paths\"][\"preprocessed_landmark_file\"], sample_size\n",
    ")\n",
    "best_inferred_df = load_sampled_data(\n",
    "    config[\"data_paths\"][\"preprocessed_best_inferred_file\"], sample_size\n",
    ")\n",
    "\n",
    "gene_df = load_sampled_data(\n",
    "    config[\"data_paths\"][\"preprocessed_gene_file\"],\n",
    "    sample_size,\n",
    "    use_chunks=True,\n",
    "    chunk_size=chunk_size,\n",
    ")\n",
    "\n",
    "# Split Data\n",
    "logging.info(\"Splitting datasets into train/val/test...\")\n",
    "\n",
    "X_tf_train, y_tf_train, X_tf_val, y_tf_val, X_tf_test, y_tf_test = split_data(\n",
    "    tf_df, target_name=\"viability\", config=config, stratify_by=\"cell_mfc_name\"\n",
    ")\n",
    "(\n",
    "    X_landmark_train,\n",
    "    y_landmark_train,\n",
    "    X_landmark_val,\n",
    "    y_landmark_val,\n",
    "    X_landmark_test,\n",
    "    y_landmark_test,\n",
    ") = split_data(\n",
    "    landmark_df, target_name=\"viability\", config=config, stratify_by=\"cell_mfc_name\"\n",
    ")\n",
    "(\n",
    "    X_best_inferred_train,\n",
    "    y_best_inferred_train,\n",
    "    X_best_inferred_val,\n",
    "    y_best_inferred_val,\n",
    "    X_best_inferred_test,\n",
    "    y_best_inferred_test,\n",
    ") = split_data(\n",
    "    best_inferred_df,\n",
    "    target_name=\"viability\",\n",
    "    config=config,\n",
    "    stratify_by=\"cell_mfc_name\",\n",
    ")\n",
    "X_gene_train, y_gene_train, X_gene_val, y_gene_val, X_gene_test, y_gene_test = (\n",
    "    split_data(\n",
    "        gene_df, target_name=\"viability\", config=config, stratify_by=\"cell_mfc_name\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 5094\n",
      "Number of edges: 62920\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "def load_ontology(file_path):\n",
    "    dG = nx.DiGraph()\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parent, child, rel_type = line.strip().split(\"\\t\")\n",
    "            dG.add_edge(parent, child, relationship=rel_type)\n",
    "    return dG\n",
    "\n",
    "\n",
    "# Load the ontology\n",
    "ontology_file = \"../data/raw/drugcell_ont.txt\"\n",
    "dG = load_ontology(ontology_file)\n",
    "\n",
    "# Check the structure\n",
    "print(f\"Number of nodes: {len(dG.nodes())}\")\n",
    "print(f\"Number of edges: {len(dG.edges())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 9648 genes are missing from the ontology.\n",
      "2683\n"
     ]
    }
   ],
   "source": [
    "genes_in_ontology = {\n",
    "    node for node, data in dG.nodes(data=True) if dG.out_degree(node) == 0\n",
    "}\n",
    "missing_genes = set(gene_df.columns) - genes_in_ontology\n",
    "if missing_genes:\n",
    "    print(f\"Warning: {len(missing_genes)} genes are missing from the ontology.\")\n",
    "    \n",
    "print(len(gene_df.columns) - len(missing_genes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
