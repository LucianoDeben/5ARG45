{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import required libraries and modules\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"src\")))\n",
    "\n",
    "\n",
    "from utils import load_config\n",
    "from preprocess import split_data\n",
    "\n",
    "# Load Config\n",
    "config = load_config(\"../config.yaml\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_sampled_data\n",
    "\n",
    "landmark_df = load_sampled_data(\n",
    "    config[\"data_paths\"][\"preprocessed_best_inferred_file\"], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AARS', 'ABCF1', 'ABL1', 'ACAA1', 'ACAT2', 'ACLY', 'ADAM10', 'ADH5',\n",
       "       'PARP1', 'ADRB2',\n",
       "       ...\n",
       "       'CCR2', 'TMEM242', 'SMIM27', 'ARMCX4', 'NBPF10', 'TIMM23', 'ZNF783',\n",
       "       'MICA', 'TMEM257', 'C10orf12'],\n",
       "      dtype='object', length=10174)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the viability, cell_mfc_name and pert_dose columns\n",
    "landmark_df = landmark_df.drop(\n",
    "    [\"viability\", \"cell_mfc_name\", \"pert_dose\"], axis=1\n",
    ")\n",
    "\n",
    "landmark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping written to gene2ind.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Suppose you already have your DataFrame, for example:\n",
    "# df = pd.read_csv('your_data.csv')  # or however you load your data\n",
    "# For this example, we mimic a DataFrame with gene columns:\n",
    "\n",
    "\n",
    "# Extract the gene symbols from the DataFrame columns\n",
    "gene_list = list(landmark_df.columns)\n",
    "\n",
    "# Sort the gene list alphabetically\n",
    "sorted_genes = sorted(gene_list)\n",
    "\n",
    "# Create a mapping dictionary: index -> gene\n",
    "gene2ind = {idx: gene for idx, gene in enumerate(sorted_genes)}\n",
    "\n",
    "# Write the mapping to a file in the required format (\"index<TAB>gene\")\n",
    "with open(\"best_inferred2ind.txt\", \"w\") as f:\n",
    "    for idx, gene in gene2ind.items():\n",
    "        f.write(f\"{idx}\\t{gene}\\n\")\n",
    "\n",
    "print(\"Mapping written to gene2ind.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load, Split, and Preprocess Datase\n",
    "# Configurable parameters\n",
    "sample_size = 1000  # Number of rows to sample from each dataset\n",
    "chunk_size = 1000  # Chunk size for loading large datasets\n",
    "\n",
    "# Load datasets\n",
    "logging.info(\"Loading datasets with sampling...\")\n",
    "\n",
    "\n",
    "def load_sampled_data(file_path, sample_size, use_chunks=False, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Load and sample a dataset, with optional chunked loading for large files.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the dataset file.\n",
    "        sample_size (int): Number of rows to sample.\n",
    "        use_chunks (bool): Whether to load the dataset in chunks.\n",
    "        chunk_size (int, optional): Size of chunks if `use_chunks` is True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Sampled DataFrame.\n",
    "    \"\"\"\n",
    "    if use_chunks:\n",
    "        logging.info(f\"Loading {file_path} in chunks...\")\n",
    "        chunks = []\n",
    "        total_loaded = 0\n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "            if total_loaded >= sample_size:\n",
    "                break\n",
    "\n",
    "            # Determine how many rows to sample from this chunk\n",
    "            sample_rows = min(sample_size - total_loaded, len(chunk))\n",
    "            chunks.append(chunk.sample(sample_rows))\n",
    "            total_loaded += sample_rows\n",
    "\n",
    "        sampled_df = pd.concat(chunks, axis=0)\n",
    "        del chunks  # Free memory\n",
    "    else:\n",
    "        logging.info(f\"Sampling {sample_size} rows from {file_path}...\")\n",
    "        sampled_df = pd.read_csv(file_path, nrows=sample_size)\n",
    "\n",
    "    return sampled_df\n",
    "\n",
    "\n",
    "# Load data with sampling\n",
    "\n",
    "tf_df = load_sampled_data(config[\"data_paths\"][\"preprocessed_tf_file\"], sample_size)\n",
    "landmark_df = load_sampled_data(\n",
    "    config[\"data_paths\"][\"preprocessed_landmark_file\"], sample_size\n",
    ")\n",
    "best_inferred_df = load_sampled_data(\n",
    "    config[\"data_paths\"][\"preprocessed_best_inferred_file\"], sample_size\n",
    ")\n",
    "\n",
    "gene_df = load_sampled_data(\n",
    "    config[\"data_paths\"][\"preprocessed_gene_file\"],\n",
    "    sample_size,\n",
    "    use_chunks=True,\n",
    "    chunk_size=chunk_size,\n",
    ")\n",
    "\n",
    "# Split Data\n",
    "logging.info(\"Splitting datasets into train/val/test...\")\n",
    "\n",
    "X_tf_train, y_tf_train, X_tf_val, y_tf_val, X_tf_test, y_tf_test = split_data(\n",
    "    tf_df, target_name=\"viability\", config=config, stratify_by=\"cell_mfc_name\"\n",
    ")\n",
    "(\n",
    "    X_landmark_train,\n",
    "    y_landmark_train,\n",
    "    X_landmark_val,\n",
    "    y_landmark_val,\n",
    "    X_landmark_test,\n",
    "    y_landmark_test,\n",
    ") = split_data(\n",
    "    landmark_df, target_name=\"viability\", config=config, stratify_by=\"cell_mfc_name\"\n",
    ")\n",
    "(\n",
    "    X_best_inferred_train,\n",
    "    y_best_inferred_train,\n",
    "    X_best_inferred_val,\n",
    "    y_best_inferred_val,\n",
    "    X_best_inferred_test,\n",
    "    y_best_inferred_test,\n",
    ") = split_data(\n",
    "    best_inferred_df,\n",
    "    target_name=\"viability\",\n",
    "    config=config,\n",
    "    stratify_by=\"cell_mfc_name\",\n",
    ")\n",
    "X_gene_train, y_gene_train, X_gene_val, y_gene_val, X_gene_test, y_gene_test = (\n",
    "    split_data(\n",
    "        gene_df, target_name=\"viability\", config=config, stratify_by=\"cell_mfc_name\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "def load_ontology(file_path):\n",
    "    dG = nx.DiGraph()\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parent, child, rel_type = line.strip().split(\"\\t\")\n",
    "            dG.add_edge(parent, child, relationship=rel_type)\n",
    "    return dG\n",
    "\n",
    "\n",
    "# Load the ontology\n",
    "ontology_file = \"../data/raw/drugcell_ont.txt\"\n",
    "dG = load_ontology(ontology_file)\n",
    "\n",
    "# Check the structure\n",
    "print(f\"Number of nodes: {len(dG.nodes())}\")\n",
    "print(f\"Number of edges: {len(dG.edges())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_in_ontology = {\n",
    "    node for node, data in dG.nodes(data=True) if dG.out_degree(node) == 0\n",
    "}\n",
    "missing_genes = set(gene_df.columns) - genes_in_ontology\n",
    "if missing_genes:\n",
    "    print(f\"Warning: {len(missing_genes)} genes are missing from the ontology.\")\n",
    "    \n",
    "print(len(gene_df.columns) - len(missing_genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "def filter_ontology(dG, valid_genes):\n",
    "    \"\"\"\n",
    "    Filter the ontology to keep only terms that contain genes from the dataset.\n",
    "\n",
    "    Args:\n",
    "        dG (nx.DiGraph): Original ontology graph.\n",
    "        valid_genes (set): Set of genes present in both the dataset and the ontology.\n",
    "\n",
    "    Returns:\n",
    "        nx.DiGraph: Updated ontology graph with only relevant genes.\n",
    "    \"\"\"\n",
    "    filtered_dG = nx.DiGraph()\n",
    "\n",
    "    for parent, child, data in dG.edges(data=True):\n",
    "        if child in valid_genes or any(\n",
    "            grandchild in valid_genes for grandchild in nx.descendants(dG, child)\n",
    "        ):\n",
    "            filtered_dG.add_edge(parent, child, relationship=data[\"relationship\"])\n",
    "\n",
    "    return filtered_dG\n",
    "\n",
    "\n",
    "# Get the intersection of genes\n",
    "genes_in_ontology = {\n",
    "    node for node, data in dG.nodes(data=True) if dG.out_degree(node) == 0\n",
    "}\n",
    "valid_genes = genes_in_ontology.intersection(set(gene_df.columns))\n",
    "\n",
    "# Apply filtering\n",
    "filtered_dG = filter_ontology(dG, valid_genes)\n",
    "\n",
    "# Check the new structure\n",
    "print(\n",
    "    f\"Updated Ontology: {len(filtered_dG.nodes())} nodes, {len(filtered_dG.edges())} edges\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_term_size_map(dG, valid_genes):\n",
    "    \"\"\"\n",
    "    Build a mapping of ontology terms to the number of genes associated with each term.\n",
    "\n",
    "    Args:\n",
    "        dG (nx.DiGraph): Filtered ontology graph.\n",
    "        valid_genes (set): Set of genes available in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping of term IDs to the number of associated genes.\n",
    "    \"\"\"\n",
    "    term_size_map = {}\n",
    "\n",
    "    for term in dG.nodes():\n",
    "        # Count only genes that are in the dataset\n",
    "        term_genes = [gene for gene in nx.descendants(dG, term) if gene in valid_genes]\n",
    "        term_size_map[term] = len(term_genes)\n",
    "\n",
    "    return term_size_map\n",
    "\n",
    "\n",
    "# Generate term_size_map using filtered ontology\n",
    "term_size_map = build_term_size_map(filtered_dG, valid_genes)\n",
    "\n",
    "\n",
    "def build_term_direct_gene_map(dG, valid_genes):\n",
    "    \"\"\"\n",
    "    Build a mapping of ontology terms to the genes directly annotated with them.\n",
    "\n",
    "    Args:\n",
    "        dG (nx.DiGraph): Filtered ontology graph.\n",
    "        valid_genes (set): Set of genes available in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping of term IDs to directly associated genes.\n",
    "    \"\"\"\n",
    "    term_direct_gene_map = {}\n",
    "\n",
    "    for term in dG.nodes():\n",
    "        # Find direct gene annotations (child nodes that are genes)\n",
    "        direct_genes = [gene for gene in dG.successors(term) if gene in valid_genes]\n",
    "        term_direct_gene_map[term] = direct_genes\n",
    "\n",
    "    return term_direct_gene_map\n",
    "\n",
    "\n",
    "# Generate term_direct_gene_map using filtered ontology\n",
    "term_direct_gene_map = build_term_direct_gene_map(filtered_dG, valid_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "class GeneExpressionDrugCell(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        term_size_map,\n",
    "        term_direct_gene_map,\n",
    "        dG,\n",
    "        ngene,\n",
    "        num_hiddens_genotype,\n",
    "        num_hiddens_final,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Custom DrugCell-like model using gene expression data instead of mutations.\n",
    "\n",
    "        Args:\n",
    "            term_size_map (dict): Mapping of terms to gene counts.\n",
    "            term_direct_gene_map (dict): Mapping of terms to directly associated genes.\n",
    "            dG (nx.DiGraph): Filtered ontology graph.\n",
    "            ngene (int): Number of input genes.\n",
    "            num_hiddens_genotype (int): Number of hidden neurons per biological term.\n",
    "            num_hiddens_final (int): Number of neurons in the final prediction layer.\n",
    "        \"\"\"\n",
    "        super(GeneExpressionDrugCell, self).__init__()\n",
    "\n",
    "        self.term_direct_gene_map = term_direct_gene_map\n",
    "        self.cal_term_dim(term_size_map, num_hiddens_genotype)\n",
    "        self.gene_dim = ngene\n",
    "\n",
    "        # Construct hierarchical neural network from ontology\n",
    "        self.construct_gene_network(dG)\n",
    "\n",
    "        # Final prediction layers\n",
    "        self.add_module(\n",
    "            \"final_linear_layer\", nn.Linear(num_hiddens_genotype, num_hiddens_final)\n",
    "        )\n",
    "        self.add_module(\"final_batchnorm_layer\", nn.BatchNorm1d(num_hiddens_final))\n",
    "        self.add_module(\"final_output_layer\", nn.Linear(num_hiddens_final, 1))\n",
    "\n",
    "    def cal_term_dim(self, term_size_map, num_hiddens_genotype):\n",
    "        \"\"\"Calculate hidden layer sizes for each biological term.\"\"\"\n",
    "        self.term_dim_map = {}\n",
    "        for term, term_size in term_size_map.items():\n",
    "            num_output = int(num_hiddens_genotype)\n",
    "            self.term_dim_map[term] = num_output\n",
    "\n",
    "    def construct_gene_network(self, dG):\n",
    "        \"\"\"Create layers following the hierarchical ontology structure.\"\"\"\n",
    "        self.term_layer_list = []\n",
    "        self.term_neighbor_map = {\n",
    "            term: list(dG.successors(term)) for term in dG.nodes()\n",
    "        }\n",
    "\n",
    "        while True:\n",
    "            leaves = [n for n in dG.nodes() if dG.out_degree(n) == 0]\n",
    "            if not leaves:\n",
    "                break\n",
    "\n",
    "            self.term_layer_list.append(leaves)\n",
    "            for term in leaves:\n",
    "                input_size = sum(\n",
    "                    self.term_dim_map[child] for child in self.term_neighbor_map[term]\n",
    "                )\n",
    "                if term in self.term_direct_gene_map:\n",
    "                    input_size += len(self.term_direct_gene_map[term])\n",
    "\n",
    "                self.add_module(\n",
    "                    term + \"_linear_layer\",\n",
    "                    nn.Linear(input_size, self.term_dim_map[term]),\n",
    "                )\n",
    "                self.add_module(\n",
    "                    term + \"_batchnorm_layer\", nn.BatchNorm1d(self.term_dim_map[term])\n",
    "                )\n",
    "\n",
    "            dG.remove_nodes_from(leaves)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Define forward pass for gene expression input.\"\"\"\n",
    "        term_outputs = {}\n",
    "\n",
    "        # Process gene input\n",
    "        for term in self.term_direct_gene_map:\n",
    "            gene_subset = self.term_direct_gene_map[term]\n",
    "            term_outputs[term] = self._modules[term + \"_linear_layer\"](\n",
    "                x[:, gene_subset]\n",
    "            )\n",
    "\n",
    "        # Propagate through the hierarchical model\n",
    "        for layer in self.term_layer_list:\n",
    "            for term in layer:\n",
    "                inputs = [term_outputs[child] for child in self.term_neighbor_map[term]]\n",
    "                term_input = torch.cat(inputs, 1)\n",
    "                term_output = torch.tanh(\n",
    "                    self._modules[term + \"_linear_layer\"](term_input)\n",
    "                )\n",
    "                term_outputs[term] = self._modules[term + \"_batchnorm_layer\"](\n",
    "                    term_output\n",
    "                )\n",
    "\n",
    "        # Final layer to predict viability\n",
    "        final_output = self._modules[\"final_batchnorm_layer\"](\n",
    "            torch.tanh(\n",
    "                self._modules[\"final_linear_layer\"](\n",
    "                    term_outputs[self.term_layer_list[-1][0]]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        output = self._modules[\"final_output_layer\"](final_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss and optimizer\n",
    "model = GeneExpressionDrugCell(\n",
    "    term_size_map=term_size_map,\n",
    "    term_direct_gene_map=term_direct_gene_map,\n",
    "    dG=filtered_dG,  # Filtered ontology graph\n",
    "    ngene=len(valid_genes),  # Number of genes in the dataset\n",
    "    num_hiddens_genotype=128,\n",
    "    num_hiddens_final=64,\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(torch.tensor(X_gene_train.values, dtype=torch.float32))\n",
    "    loss = criterion(\n",
    "        outputs, torch.tensor(y_gene_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
