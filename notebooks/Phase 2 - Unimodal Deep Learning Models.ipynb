{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - Unimodal Deep Learning Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 19:54:12,697 - Device: cuda\n"
     ]
    }
   ],
   "source": [
    "## Import required libraries and modules\n",
    "# Add src to path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"src\")))\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from utils import load_config\n",
    "\n",
    "# Load Config to ensure reproducibility and syncing with other scripts\n",
    "config = load_config(\"../config.yaml\")\n",
    "\n",
    "# Set logging configurations\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(message)s\",\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logging.info(f\"Device: {device}\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from preprocess.preprocess import split_data\n",
    "\n",
    "# from models import FlexibleFCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load, Split and Preprocess Dataset\n",
    "# Load the dataset\n",
    "tf_df = pd.read_csv(config[\"data_paths\"][\"preprocessed_tf_file\"])\n",
    "landmark_df = pd.read_csv(config[\"data_paths\"][\"preprocessed_landmark_file\"])\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 1000\n",
    "\n",
    "# Initialize an empty list to store processed chunks\n",
    "chunks = []\n",
    "\n",
    "# Read the CSV file in chunks\n",
    "for chunk in pd.read_csv(\n",
    "    config[\"data_paths\"][\"preprocessed_gene_file\"], chunksize=chunk_size):\n",
    "    # Optionally process the chunk (e.g., drop columns, filter rows)\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# Combine chunks into a single DataFrame (if needed)\n",
    "gene_df = pd.concat(chunks, axis=0)\n",
    "\n",
    "# Only sample a subset of the data for faster training\n",
    "# tf_df = tf_df.sample(n=10000, random_state=42)\n",
    "# landmark_df = landmark_df.sample(n=10000, random_state=42)\n",
    "\n",
    "# Split data into train/validation/test sets\n",
    "X_tf_train, y_tf_train, X_tf_val, y_tf_val, X_tf_test, y_tf_test = split_data(\n",
    "    tf_df, target_name=\"viability\", config=config\n",
    ")\n",
    "(\n",
    "    X_landmark_train,\n",
    "    y_landmark_train,\n",
    "    X_landmark_val,\n",
    "    y_landmark_val,\n",
    "    X_landmark_test,\n",
    "    y_landmark_test,\n",
    ") = split_data(landmark_df, target_name=\"viability\", config=config)\n",
    "X_gene_train, y_gene_train, X_gene_val, y_gene_val, X_gene_test, y_gene_test = (\n",
    "    split_data(gene_df, target_name=\"viability\", config=config)\n",
    ")\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "def create_dataloader(X, y, batch_size=32):\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(X.values, dtype=torch.float32),\n",
    "        torch.tensor(y.values, dtype=torch.float32),\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "tf_train_loader = create_dataloader(X_tf_train, y_tf_train)\n",
    "tf_val_loader = create_dataloader(X_tf_val, y_tf_val)\n",
    "tf_test_loader = create_dataloader(X_tf_test, y_tf_test)\n",
    "\n",
    "landmark_train_loader = create_dataloader(X_landmark_train, y_landmark_train)\n",
    "landmark_val_loader = create_dataloader(X_landmark_val, y_landmark_val)\n",
    "landmark_test_loader = create_dataloader(X_landmark_test, y_landmark_test)\n",
    "\n",
    "gene_train_loader = create_dataloader(X_gene_train, y_gene_train)\n",
    "gene_val_loader = create_dataloader(X_gene_val, y_gene_val)\n",
    "gene_test_loader = create_dataloader(X_gene_test, y_gene_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = {\n",
    "    \"TF Data\": (X_tf_train, y_tf_train, tf_train_loader, tf_val_loader, tf_test_loader),\n",
    "    \"Landmark Data\": (\n",
    "        X_landmark_train,\n",
    "        y_landmark_train,\n",
    "        landmark_train_loader,\n",
    "        landmark_val_loader,\n",
    "        landmark_test_loader,\n",
    "    ),\n",
    "    \"Gene Data\": (\n",
    "        X_gene_train,\n",
    "        y_gene_train,\n",
    "        gene_train_loader,\n",
    "        gene_val_loader,\n",
    "        gene_test_loader,\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Initialize a DataFrame to store results\n",
    "combined_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "\n",
    "ACTIVATION_MAP = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"leakyrelu\": nn.LeakyReLU,\n",
    "    \"tanh\": nn.Tanh,\n",
    "    \"sigmoid\": nn.Sigmoid,\n",
    "    \"gelu\": nn.GELU,\n",
    "    \"identity\": nn.Identity,\n",
    "    \"prelu\": nn.PReLU,\n",
    "    \"elu\": nn.ELU,\n",
    "}\n",
    "\n",
    "\n",
    "class FlexibleFCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dims,\n",
    "        output_dim,\n",
    "        activation_fn=\"relu\",\n",
    "        dropout_prob=0.0,\n",
    "        residual=False,\n",
    "        use_batchnorm=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Flexible Fully-Connected Neural Network\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Dimensionality of input features.\n",
    "            hidden_dims (list of int): List with the size of each hidden layer.\n",
    "            output_dim (int): Dimension of the output.\n",
    "            activation_fn (str or callable): Activation function to use.\n",
    "                If str, must be in ACTIVATION_MAP. If callable, it should be a nn.Module or lambda returning Tensor.\n",
    "            dropout_prob (float): Dropout probability. 0.0 means no dropout.\n",
    "            residual (bool): Whether to use residual connections when possible.\n",
    "            use_batchnorm (bool): Whether to use BatchNorm after each linear layer.\n",
    "        \"\"\"\n",
    "        super(FlexibleFCNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.residual = residual\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # Determine the activation function\n",
    "        if isinstance(activation_fn, str):\n",
    "            if activation_fn.lower() not in ACTIVATION_MAP:\n",
    "                raise ValueError(f\"Unknown activation function {activation_fn}\")\n",
    "            self.activation_fn = ACTIVATION_MAP[activation_fn.lower()]()\n",
    "        else:\n",
    "            # Assume callable\n",
    "            self.activation_fn = activation_fn\n",
    "\n",
    "        # Construct layers\n",
    "        layer_dims = [input_dim] + hidden_dims\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList() if use_batchnorm else None\n",
    "        self.dropouts = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            in_dim = layer_dims[i]\n",
    "            out_dim = layer_dims[i + 1]\n",
    "            self.layers.append(nn.Linear(in_dim, out_dim))\n",
    "\n",
    "            if use_batchnorm:\n",
    "                self.bns.append(nn.BatchNorm1d(out_dim))\n",
    "\n",
    "            if dropout_prob > 0.0:\n",
    "                self.dropouts.append(nn.Dropout(dropout_prob))\n",
    "            else:\n",
    "                # Placeholder for consistency\n",
    "                self.dropouts.append(nn.Identity())\n",
    "\n",
    "        self.output_layer = nn.Linear(layer_dims[-1], output_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Xavier initialization\n",
    "        for layer in self.layers:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "        if self.output_layer.bias is not None:\n",
    "            nn.init.zeros_(self.output_layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Keep track of input for possible residual connections\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            residual_input = x\n",
    "            x = layer(x)\n",
    "\n",
    "            # BatchNorm\n",
    "            if self.use_batchnorm:\n",
    "                x = self.bns[i](x)\n",
    "\n",
    "            # Dropout before activation\n",
    "            x = self.dropouts[i](x)\n",
    "\n",
    "            # Activation\n",
    "            x = self.activation_fn(x)\n",
    "\n",
    "            # Residual connection (only if dimensions match)\n",
    "            if self.residual and residual_input.shape == x.shape:\n",
    "                x = x + residual_input\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def get_regularization_loss(self, l1_lambda=0.0, l2_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Compute L1 and L2 regularization losses for all linear layers.\n",
    "        l1_lambda and l2_lambda are coefficients for L1 and L2 penalties respectively.\n",
    "\n",
    "        Args:\n",
    "            l1_lambda (float): Weight for L1 regularization.\n",
    "            l2_lambda (float): Weight for L2 regularization.\n",
    "\n",
    "        Returns:\n",
    "            reg_loss (Tensor): The regularization loss (scalar).\n",
    "        \"\"\"\n",
    "        reg_loss = torch.tensor(0.0, device=next(self.parameters()).device)\n",
    "        if l1_lambda == 0.0 and l2_lambda == 0.0:\n",
    "            return reg_loss\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if l1_lambda > 0.0:\n",
    "                reg_loss += l1_lambda * torch.sum(torch.abs(layer.weight))\n",
    "            if l2_lambda > 0.0:\n",
    "                reg_loss += l2_lambda * torch.sum(layer.weight**2)\n",
    "\n",
    "        # Also consider output layer\n",
    "        if l1_lambda > 0.0:\n",
    "            reg_loss += l1_lambda * torch.sum(torch.abs(self.output_layer.weight))\n",
    "        if l2_lambda > 0.0:\n",
    "            reg_loss += l2_lambda * torch.sum(self.output_layer.weight**2)\n",
    "\n",
    "        return reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([19])) that is different to the input size (torch.Size([19, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Model, Train Loss: 0.1925, Val Loss: 0.0609\n",
      "Epoch 2/20 - Model, Train Loss: 0.0649, Val Loss: 0.0606\n",
      "Epoch 3/20 - Model, Train Loss: 0.0612, Val Loss: 0.0618\n",
      "Epoch 4/20 - Model, Train Loss: 0.0599, Val Loss: 0.0600\n",
      "Epoch 5/20 - Model, Train Loss: 0.0592, Val Loss: 0.0602\n",
      "Epoch 6/20 - Model, Train Loss: 0.0588, Val Loss: 0.0600\n",
      "Epoch 7/20 - Model, Train Loss: 0.0585, Val Loss: 0.0599\n",
      "Epoch 8/20 - Model, Train Loss: 0.0581, Val Loss: 0.0599\n",
      "Epoch 9/20 - Model, Train Loss: 0.0582, Val Loss: 0.0607\n",
      "Epoch 10/20 - Model, Train Loss: 0.0577, Val Loss: 0.0607\n",
      "Epoch 11/20 - Model, Train Loss: 0.0578, Val Loss: 0.0599\n",
      "Epoch 12/20 - Model, Train Loss: 0.0577, Val Loss: 0.0599\n",
      "Epoch 13/20 - Model, Train Loss: 0.0576, Val Loss: 0.0602\n",
      "Epoch 14/20 - Model, Train Loss: 0.0575, Val Loss: 0.0598\n",
      "Epoch 15/20 - Model, Train Loss: 0.0574, Val Loss: 0.0598\n",
      "Epoch 16/20 - Model, Train Loss: 0.0574, Val Loss: 0.0599\n",
      "Epoch 17/20 - Model, Train Loss: 0.0573, Val Loss: 0.0598\n",
      "Epoch 18/20 - Model, Train Loss: 0.0574, Val Loss: 0.0598\n",
      "Epoch 19/20 - Model, Train Loss: 0.0573, Val Loss: 0.0598\n",
      "Epoch 20/20 - Model, Train Loss: 0.0573, Val Loss: 0.0599\n",
      "Test Loss: MSE\n",
      "Evaluation Metrics: Pearson Correlation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from evaluation import evaluate_model\n",
    "from training import train_model\n",
    "\n",
    "# Step 1: Prepare the DataLoaders\n",
    "\n",
    "# Step 2: Model Configuration\n",
    "input_dim = tf_train_loader.dataset[0][0].shape[0]  # Automatically detect input size\n",
    "hidden_layers = (512, 256, 128)  # Define architecture\n",
    "dropout_rate = 0.3  # Dropout rate\n",
    "activation = nn.GELU  # Activation function\n",
    "output_activation = None  # No output activation for regression\n",
    "\n",
    "model = FlexibleFCNN(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dims=[512, 256, 128, 64],\n",
    "    output_dim=1,\n",
    "    activation_fn=\"prelu\",\n",
    "    dropout_prob=0.2,\n",
    "    residual=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Step 3: Define Training Components\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "criterion = nn.MSELoss()  # Loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Optimizer\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", patience=5, verbose=True\n",
    ")  # LR Scheduler\n",
    "\n",
    "# Step 4: Train the Model\n",
    "num_epochs = 20\n",
    "train_losses, val_losses = train_model(\n",
    "    model=model,\n",
    "    train_loader=tf_train_loader,\n",
    "    val_loader=tf_val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    epochs=num_epochs,\n",
    "    device=device,\n",
    "    gradient_clipping=1.0,\n",
    "    early_stopping_patience=10,\n",
    ")\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "test_loss, y_true, y_pred, metrics = evaluate_model(\n",
    "    model=model,\n",
    "    test_loader=tf_test_loader,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    calculate_metrics=True,\n",
    ")\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Evaluation Metrics:\", metrics)\n",
    "\n",
    "# Step 6: Save the Model (Optional)\n",
    "torch.save(model.state_dict(), \"fcnn_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5ARG45",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
