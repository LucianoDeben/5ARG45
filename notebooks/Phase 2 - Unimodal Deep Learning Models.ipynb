{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - Unimodal Deep Learning Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 14:31:23,140 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "## Import required libraries and modules\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import importlib\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"src\")))\n",
    "\n",
    "\n",
    "from utils import load_config\n",
    "from preprocess.preprocess import split_data\n",
    "from models import FlexibleFCNN\n",
    "from pipelines import DLModelsPipeline\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Load Config\n",
    "config = load_config(\"../config.yaml\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 14:31:23,163 - INFO - Loading datasets...\n",
      "2024-12-19 14:31:37,900 - INFO - Loading gene dataset in chunks...\n",
      "2024-12-19 14:33:40,072 - INFO - Splitting datasets into train/val/test...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (19416, 683), Validation Shape: (5379, 683), Test Shape: (6364, 683)\n",
      "Train Shape: (19416, 979), Validation Shape: (5379, 979), Test Shape: (6364, 979)\n",
      "Train Shape: (19416, 12329), Validation Shape: (5379, 12329), Test Shape: (6364, 12329)\n"
     ]
    }
   ],
   "source": [
    "## Load, Split and Preprocess Dataset\n",
    "# Load datasets\n",
    "logging.info(\"Loading datasets...\")\n",
    "tf_df = pd.read_csv(config[\"data_paths\"][\"preprocessed_tf_file\"])\n",
    "landmark_df = pd.read_csv(config[\"data_paths\"][\"preprocessed_landmark_file\"])\n",
    "\n",
    "# For large gene data, read in chunks\n",
    "logging.info(\"Loading gene dataset in chunks...\")\n",
    "chunk_size = 1000\n",
    "chunks = []\n",
    "for chunk in pd.read_csv(\n",
    "    config[\"data_paths\"][\"preprocessed_gene_file\"], chunksize=chunk_size\n",
    "):\n",
    "    chunks.append(chunk)\n",
    "gene_df = pd.concat(chunks, axis=0)\n",
    "del chunks  # Free memory\n",
    "\n",
    "# # Only sample 1000 rows for now\n",
    "# tf_df = tf_df.sample(1000)\n",
    "# landmark_df = landmark_df.sample(1000)\n",
    "# gene_df = gene_df.sample(1000)\n",
    "\n",
    "# Split Data\n",
    "logging.info(\"Splitting datasets into train/val/test...\")\n",
    "X_tf_train, y_tf_train, X_tf_val, y_tf_val, X_tf_test, y_tf_test = split_data(\n",
    "    tf_df, target_name=\"viability\", config=config, stratify_by=\"cell_mfc_name\"\n",
    ")\n",
    "(\n",
    "    X_landmark_train,\n",
    "    y_landmark_train,\n",
    "    X_landmark_val,\n",
    "    y_landmark_val,\n",
    "    X_landmark_test,\n",
    "    y_landmark_test,\n",
    ") = split_data(landmark_df, target_name=\"viability\", config=config, stratify_by=\"cell_mfc_name\")\n",
    "X_gene_train, y_gene_train, X_gene_val, y_gene_val, X_gene_test, y_gene_test = (\n",
    "    split_data(gene_df, target_name=\"viability\", config=config, stratify_by=\"cell_mfc_name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 14:33:58,306 - INFO - Creating DataLoaders...\n"
     ]
    }
   ],
   "source": [
    "def create_dataloader(X, y, batch_size=32):\n",
    "    # Ensuring X and y are pandas DataFrames/Series:\n",
    "    # If they are arrays, adjust accordingly.\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(X.values, dtype=torch.float32),\n",
    "        torch.tensor(y.values, dtype=torch.float32),\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "logging.info(\"Creating DataLoaders...\")\n",
    "tf_train_loader = create_dataloader(X_tf_train, y_tf_train)\n",
    "tf_val_loader = create_dataloader(X_tf_val, y_tf_val)\n",
    "tf_test_loader = create_dataloader(X_tf_test, y_tf_test)\n",
    "\n",
    "landmark_train_loader = create_dataloader(X_landmark_train, y_landmark_train)\n",
    "landmark_val_loader = create_dataloader(X_landmark_val, y_landmark_val)\n",
    "landmark_test_loader = create_dataloader(X_landmark_test, y_landmark_test)\n",
    "\n",
    "gene_train_loader = create_dataloader(X_gene_train, y_gene_train)\n",
    "gene_val_loader = create_dataloader(X_gene_val, y_gene_val)\n",
    "gene_test_loader = create_dataloader(X_gene_test, y_gene_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import CNNRegressor, MLPMixer, TransformerRegressor\n",
    "\n",
    "\n",
    "feature_sets = {\n",
    "    # \"TF Data\": (tf_train_loader, tf_val_loader, tf_test_loader),\n",
    "    # \"Landmark Data\": (landmark_train_loader, landmark_val_loader, landmark_test_loader),\n",
    "    \"Gene Data\": (gene_train_loader, gene_val_loader, gene_test_loader),\n",
    "}\n",
    "\n",
    "# Define your model configurations\n",
    "model_configs = {\n",
    "    \"FCNN_Model\": {\n",
    "        \"model_class\": FlexibleFCNN,\n",
    "        \"model_params\": {\n",
    "            \"hidden_dims\": [512, 256, 128, 64],\n",
    "            \"output_dim\": 1,\n",
    "            \"activation_fn\": \"prelu\",\n",
    "            \"dropout_prob\": 0.2,\n",
    "            \"residual\": True,\n",
    "            \"norm_type\": \"batchnorm\",\n",
    "            \"weight_init\": \"xavier\",\n",
    "        },\n",
    "        \"criterion\": nn.MSELoss(),\n",
    "        \"optimizer_class\": optim.AdamW,\n",
    "        \"optimizer_params\": {\"lr\": 0.001, \"weight_decay\": 1e-4},\n",
    "        \"scheduler_class\": ReduceLROnPlateau,\n",
    "        \"scheduler_params\": {\"mode\": \"min\", \"patience\": 5},\n",
    "        \"train_params\": {\n",
    "            \"epochs\": 20,\n",
    "            \"gradient_clipping\": 1.0,\n",
    "            \"early_stopping_patience\": 10,\n",
    "        },\n",
    "    },\n",
    "    # \"Transformer_Model\": {\n",
    "    #     \"model_class\": TransformerRegressor,\n",
    "    #     \"model_params\": {\n",
    "    #         \"d_model\": 128,\n",
    "    #         \"nhead\": 4,\n",
    "    #         \"num_layers\": 2,\n",
    "    #         \"dim_feedforward\": 256,\n",
    "    #         \"dropout\": 0.1,\n",
    "    #         \"output_dim\": 1,\n",
    "    #     },\n",
    "    #     \"criterion\": nn.MSELoss(),\n",
    "    #     \"optimizer_class\": optim.AdamW,\n",
    "    #     \"optimizer_params\": {\"lr\": 0.001, \"weight_decay\": 1e-4},\n",
    "    #     \"scheduler_class\": ReduceLROnPlateau,\n",
    "    #     \"scheduler_params\": {\"mode\": \"min\", \"patience\": 5},\n",
    "    #     \"train_params\": {\n",
    "    #         \"epochs\": 10,\n",
    "    #         \"gradient_clipping\": 1.0,\n",
    "    #         \"early_stopping_patience\": 10,\n",
    "    #     },\n",
    "    # },\n",
    "    # \"CNN_Model\": {\n",
    "    #     \"model_class\": CNNRegressor,\n",
    "    #     \"model_params\": {\n",
    "    #         \"num_filters\": 64,\n",
    "    #         \"kernel_size\": 7,\n",
    "    #         \"num_layers\": 3,\n",
    "    #         \"dropout_prob\": 0.2,\n",
    "    #         \"output_dim\": 1,\n",
    "    #     },\n",
    "    #     \"criterion\": nn.MSELoss(),\n",
    "    #     \"optimizer_class\": optim.AdamW,\n",
    "    #     \"optimizer_params\": {\"lr\": 0.001, \"weight_decay\": 1e-4},\n",
    "    #     \"scheduler_class\": ReduceLROnPlateau,\n",
    "    #     \"scheduler_params\": {\"mode\": \"min\", \"patience\": 5},\n",
    "    #     \"train_params\": {\n",
    "    #         \"epochs\": 10,\n",
    "    #         \"gradient_clipping\": 1.0,\n",
    "    #         \"early_stopping_patience\": 10,\n",
    "    #     },\n",
    "    # },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now initialize the pipeline using model_configs instead of model_class & model_params\n",
    "pipeline = DLModelsPipeline(feature_sets=feature_sets, model_configs=model_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 14:34:05,166 - INFO - Starting training and evaluation...\n",
      "2024-12-19 14:34:31,593 - INFO - Epoch 1/20 - Model, Train Loss: 0.1780, Val Loss: 0.0369\n",
      "2024-12-19 14:34:49,123 - INFO - Epoch 2/20 - Model, Train Loss: 0.0432, Val Loss: 0.0363\n",
      "2024-12-19 14:35:06,758 - INFO - Epoch 3/20 - Model, Train Loss: 0.0347, Val Loss: 0.0355\n",
      "2024-12-19 14:35:23,858 - INFO - Epoch 4/20 - Model, Train Loss: 0.0297, Val Loss: 0.0363\n",
      "2024-12-19 14:35:41,558 - INFO - Epoch 5/20 - Model, Train Loss: 0.0254, Val Loss: 0.0373\n",
      "2024-12-19 14:36:00,831 - INFO - Epoch 6/20 - Model, Train Loss: 0.0226, Val Loss: 0.0397\n",
      "2024-12-19 14:36:19,724 - INFO - Epoch 7/20 - Model, Train Loss: 0.0208, Val Loss: 0.0390\n",
      "2024-12-19 14:36:38,655 - INFO - Epoch 8/20 - Model, Train Loss: 0.0186, Val Loss: 0.0374\n",
      "2024-12-19 14:36:57,118 - INFO - Epoch 9/20 - Model, Train Loss: 0.0169, Val Loss: 0.0400\n",
      "2024-12-19 14:37:15,256 - INFO - Epoch 10/20 - Model, Train Loss: 0.0134, Val Loss: 0.0376\n",
      "2024-12-19 14:37:36,170 - INFO - Epoch 11/20 - Model, Train Loss: 0.0124, Val Loss: 0.0386\n",
      "2024-12-19 14:37:54,356 - INFO - Epoch 12/20 - Model, Train Loss: 0.0114, Val Loss: 0.0379\n",
      "2024-12-19 14:38:13,111 - INFO - Epoch 13/20 - Model, Train Loss: 0.0105, Val Loss: 0.0388\n",
      "2024-12-19 14:38:13,112 - INFO - Early stopping triggered for Model at epoch 13.\n",
      "2024-12-19 14:38:15,297 - INFO - Collecting results...\n",
      "2024-12-19 14:38:15,346 - INFO - Results saved to combined_metrics.csv.\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the models\n",
    "logging.info(\"Starting training and evaluation...\")\n",
    "pipeline.train_and_evaluate()\n",
    "\n",
    "# Retrieve results\n",
    "logging.info(\"Collecting results...\")\n",
    "results_df = pipeline.get_results()\n",
    "\n",
    "# Save the results\n",
    "results_df.to_csv(\"combined_metrics.csv\", index=False)\n",
    "logging.info(\"Results saved to combined_metrics.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a2b1f_row0_col0, #T_a2b1f_row0_col1, #T_a2b1f_row0_col2, #T_a2b1f_row0_col3 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a2b1f\">\n",
       "  <caption>Regression Model Evaluation Metrics</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a2b1f_level0_col0\" class=\"col_heading level0 col0\" >MSE</th>\n",
       "      <th id=\"T_a2b1f_level0_col1\" class=\"col_heading level0 col1\" >MAE</th>\n",
       "      <th id=\"T_a2b1f_level0_col2\" class=\"col_heading level0 col2\" >R²</th>\n",
       "      <th id=\"T_a2b1f_level0_col3\" class=\"col_heading level0 col3\" >Pearson Correlation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Feature Set</th>\n",
       "      <th class=\"index_name level1\" >Model Name</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a2b1f_level0_row0\" class=\"row_heading level0 row0\" >Gene Data</th>\n",
       "      <th id=\"T_a2b1f_level1_row0\" class=\"row_heading level1 row0\" >FCNN_Model</th>\n",
       "      <td id=\"T_a2b1f_row0_col0\" class=\"data row0 col0\" >0.027</td>\n",
       "      <td id=\"T_a2b1f_row0_col1\" class=\"data row0 col1\" >0.103</td>\n",
       "      <td id=\"T_a2b1f_row0_col2\" class=\"data row0 col2\" >0.155</td>\n",
       "      <td id=\"T_a2b1f_row0_col3\" class=\"data row0 col3\" >0.602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c0373f3a60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "styled_results = (\n",
    "    results_df.style.format(precision=3)\n",
    "    .set_caption(\"Regression Model Evaluation Metrics\")\n",
    "    .highlight_max(\n",
    "        subset=[\"R²\", \"Pearson Correlation\"], color=\"lightgreen\"\n",
    "    )\n",
    "    .highlight_min(subset=[\"MAE\", \"MSE\"], color=\"lightgreen\")\n",
    ")\n",
    "styled_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseKnowledgeNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gene_tf_matrix: torch.Tensor,\n",
    "        hidden_dims: list,\n",
    "        output_dim: int = 1,\n",
    "        first_activation: str = \"tanh\",  # Activation for gene-to-TF layer: Tanh or Sigmoid\n",
    "        downstream_activation: str = \"relu\",  # Activation for downstream layers\n",
    "        dropout_prob: float = 0.2,\n",
    "        weight_init: str = \"xavier\",\n",
    "        use_batchnorm: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Knowledge-Informed Sparse Network with trainable gene-TF interactions.\n",
    "\n",
    "        Args:\n",
    "            gene_tf_matrix (torch.Tensor): Binary (-1, 0, 1) gene-TF connection matrix.\n",
    "            hidden_dims (list of int): Sizes of additional hidden layers after TF activations.\n",
    "            output_dim (int): Number of output features (1 for regression).\n",
    "            first_activation (str): Activation function after the gene-to-TF layer (Tanh or Sigmoid).\n",
    "            downstream_activation (str): Activation function for downstream layers (e.g., ReLU).\n",
    "            dropout_prob (float): Dropout probability.\n",
    "            weight_init (str): Weight initialization method (\"xavier\" or \"kaiming\").\n",
    "            use_batchnorm (bool): Whether to use batch normalization in downstream layers.\n",
    "        \"\"\"\n",
    "        super(SparseKnowledgeNetwork, self).__init__()\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "\n",
    "        # First activation function for gene-to-TF layer\n",
    "        if first_activation.lower() not in {\"tanh\", \"sigmoid\"}:\n",
    "            raise ValueError(\"First activation must be 'tanh' or 'sigmoid'.\")\n",
    "        self.first_activation = getattr(F, first_activation.lower())\n",
    "\n",
    "        # Downstream activation function\n",
    "        self.downstream_activation = getattr(F, downstream_activation.lower())\n",
    "\n",
    "        # Trainable gene-to-TF interaction weights (initialized with prior knowledge)\n",
    "        self.gene_to_tf_weights = nn.Parameter(gene_tf_matrix.clone().float())\n",
    "\n",
    "        # Define the hidden layers after the TF layer\n",
    "        tf_dim = gene_tf_matrix.shape[1]  # Number of TFs\n",
    "        hidden_dims = [tf_dim] + hidden_dims\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList() if use_batchnorm else None\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i + 1]))\n",
    "            if use_batchnorm:\n",
    "                self.batch_norms.append(nn.BatchNorm1d(hidden_dims[i + 1]))\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights(weight_init)\n",
    "\n",
    "    def _initialize_weights(self, method=\"xavier\"):\n",
    "        \"\"\"\n",
    "        Initialize model weights.\n",
    "        \"\"\"\n",
    "        for layer in list(self.hidden_layers) + [self.output_layer]:\n",
    "            if method == \"xavier\":\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "            elif method == \"kaiming\":\n",
    "                nn.init.kaiming_uniform_(layer.weight, nonlinearity=\"relu\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown weight initialization method: {method}\")\n",
    "            if layer.bias is not None:\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the sparse knowledge-informed network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor (gene expression data).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Model output.\n",
    "        \"\"\"\n",
    "        # Trainable gene-to-TF interaction layer\n",
    "        tf_activations = torch.matmul(x, self.gene_to_tf_weights)\n",
    "\n",
    "        # Apply the first activation function (Tanh or Sigmoid)\n",
    "        tf_activations = self.first_activation(tf_activations)\n",
    "\n",
    "        # Apply additional hidden layers\n",
    "        hidden_activations = tf_activations\n",
    "        for i, layer in enumerate(self.hidden_layers):\n",
    "            hidden_activations = layer(hidden_activations)\n",
    "            if self.use_batchnorm:\n",
    "                hidden_activations = self.batch_norms[i](hidden_activations)\n",
    "            hidden_activations = self.downstream_activation(hidden_activations)\n",
    "            hidden_activations = self.dropout(hidden_activations)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.output_layer(hidden_activations)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 14:38:28,424 - INFO - Downloading data from `https://omnipathdb.org/queries/enzsub?format=json`\n",
      "2024-12-19 14:38:31,493 - WARNING - Failed to download from `https://omnipathdb.org/`.\n",
      "2024-12-19 14:38:31,501 - WARNING - Traceback (most recent call last):\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\connectionpool.py\", line 466, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1095, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\connection.py\", line 730, in connect\n",
      "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\connection.py\", line 909, in _ssl_wrap_socket_and_match_hostname\n",
      "    ssl_sock = ssl_wrap_socket(\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 469, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 513, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\ssl.py\", line 513, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\ssl.py\", line 1104, in _create\n",
      "    self.do_handshake()\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\ssl.py\", line 1375, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\connectionpool.py\", line 490, in _make_request\n",
      "    raise new_e\n",
      "urllib3.exceptions.SSLError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='omnipathdb.org', port=443): Max retries exceeded with url: /queries/enzsub?format=json (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\omnipath\\_core\\downloader\\_downloader.py\", line 143, in maybe_download\n",
      "    res = self._download(req)\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\omnipath\\_core\\downloader\\_downloader.py\", line 178, in _download\n",
      "    with self._session.send(\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\requests\\adapters.py\", line 698, in send\n",
      "    raise SSLError(e, request=request)\n",
      "requests.exceptions.SSLError: HTTPSConnectionPool(host='omnipathdb.org', port=443): Max retries exceeded with url: /queries/enzsub?format=json (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "\n",
      "2024-12-19 14:38:31,503 - INFO - Downloading data from `http://no-tls.omnipathdb.org/queries/enzsub?format=json`\n",
      "2024-12-19 14:38:31,535 - INFO - Downloading data from `https://omnipathdb.org/queries/interactions?format=json`\n",
      "2024-12-19 14:38:31,586 - INFO - Downloading data from `https://omnipathdb.org/queries/complexes?format=json`\n",
      "2024-12-19 14:38:31,632 - INFO - Downloading data from `https://omnipathdb.org/queries/annotations?format=json`\n",
      "2024-12-19 14:38:31,678 - INFO - Downloading data from `https://omnipathdb.org/queries/intercell?format=json`\n",
      "2024-12-19 14:38:31,851 - INFO - Downloading data from `https://omnipathdb.org/about?format=text`\n",
      "2024-12-19 14:38:44,313 - INFO - Filtered dataset shape: (19416, 5576)\n",
      "2024-12-19 14:38:44,719 - INFO - Filtered dataset shape: (5379, 5576)\n",
      "2024-12-19 14:38:45,000 - INFO - Filtered dataset shape: (6364, 5576)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5576, 1186])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import decoupler as dc\n",
    "\n",
    "\n",
    "def create_gene_tf_matrix(\n",
    "    net: pd.DataFrame, genes: list = None, tfs: list = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates a PyTorch tensor representing the gene-TF regulatory matrix from a network dataframe.\n",
    "\n",
    "    Args:\n",
    "        net (pd.DataFrame): DataFrame containing the regulatory network with the following columns:\n",
    "            - \"source\": Transcription factors (TFs).\n",
    "            - \"target\": Genes regulated by the TFs.\n",
    "            - \"weight\": Interaction weight (1 for activation, -1 for inhibition).\n",
    "        genes (list, optional): List of genes to include in the matrix. If None, all unique genes in `net` are used.\n",
    "        tfs (list, optional): List of TFs to include in the matrix. If None, all unique TFs in `net` are used.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (num_genes, num_tfs) where:\n",
    "            - `1` indicates an activating interaction.\n",
    "            - `-1` indicates an inhibiting interaction.\n",
    "            - `0` indicates no interaction.\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    required_columns = {\"source\", \"target\", \"weight\"}\n",
    "    if not required_columns.issubset(net.columns):\n",
    "        raise ValueError(\n",
    "            f\"The `net` dataframe must contain the columns: {required_columns}\"\n",
    "        )\n",
    "\n",
    "    # Use all unique genes and TFs if not provided\n",
    "    available_genes = sorted(net[\"target\"].unique())\n",
    "    available_tfs = sorted(net[\"source\"].unique())\n",
    "\n",
    "    if genes is None:\n",
    "        genes = available_genes\n",
    "    else:\n",
    "        # Filter out genes not in the network\n",
    "        genes = [gene for gene in genes if gene in available_genes]\n",
    "\n",
    "    if tfs is None:\n",
    "        tfs = available_tfs\n",
    "    else:\n",
    "        # Filter out TFs not in the network\n",
    "        tfs = [tf for tf in tfs if tf in available_tfs]\n",
    "\n",
    "    # Initialize a DataFrame with zeros (default for no interaction)\n",
    "    gene_to_tf_df = pd.DataFrame(0, index=genes, columns=tfs, dtype=float)\n",
    "\n",
    "    # Populate the DataFrame with interaction weights\n",
    "    for _, row in net.iterrows():\n",
    "        gene = row[\"target\"]\n",
    "        tf = row[\"source\"]\n",
    "        weight = row[\"weight\"]\n",
    "        if gene in genes and tf in tfs:\n",
    "            gene_to_tf_df.at[gene, tf] = weight\n",
    "\n",
    "    # Convert the DataFrame to a PyTorch tensor\n",
    "    gene_to_tf_matrix = torch.tensor(gene_to_tf_df.values, dtype=torch.float32)\n",
    "    return gene_to_tf_matrix\n",
    "\n",
    "\n",
    "def filter_genes_to_collectri(dataset: pd.DataFrame, net: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the dataset to include only genes present in the Collectri network.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): Gene expression dataset (rows = samples, columns = genes).\n",
    "        net (pd.DataFrame): Regulatory network dataframe with a \"target\" column containing gene names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered dataset containing only Collectri genes.\n",
    "    \"\"\"\n",
    "    # Extract unique genes from the \"target\" column of the net dataframe\n",
    "    collectri_genes = net[\"target\"].unique()\n",
    "\n",
    "    # Find the intersection of dataset columns and Collectri genes\n",
    "    intersecting_genes = set(dataset.columns).intersection(collectri_genes)\n",
    "\n",
    "    # Filter the dataset to include only the intersecting genes\n",
    "    filtered_dataset = dataset[list(intersecting_genes)]\n",
    "\n",
    "    logging.info(f\"Filtered dataset shape: {filtered_dataset.shape}\")\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net = dc.get_collectri(organism='human', split_complexes=False)\n",
    "\n",
    "# Filter gene datasets to include only Collectri genes\n",
    "X_gene_train = filter_genes_to_collectri(X_gene_train, net)\n",
    "X_gene_val = filter_genes_to_collectri(X_gene_val, net)\n",
    "X_gene_test = filter_genes_to_collectri(X_gene_test, net)\n",
    "\n",
    "# Create new dataloaders\n",
    "gene_train_loader = create_dataloader(X_gene_train, y_gene_train)\n",
    "gene_val_loader = create_dataloader(X_gene_val, y_gene_val)\n",
    "gene_test_loader = create_dataloader(X_gene_test, y_gene_test)\n",
    "\n",
    "# Create gene-TF matrix\n",
    "gene_tf_matrix = create_gene_tf_matrix(net, X_gene_train.columns)\n",
    "gene_tf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "sparse_knowledge_model = SparseKnowledgeNetwork(gene_tf_matrix=gene_tf_matrix, hidden_dims=[512, 256, 128, 64, 32], output_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 15:01:45,412 - INFO - Epoch 1/20 - Model, Train Loss: 0.0356, Val Loss: 0.0396\n",
      "2024-12-19 15:02:07,570 - INFO - Epoch 2/20 - Model, Train Loss: 0.0318, Val Loss: 0.0403\n",
      "2024-12-19 15:02:29,897 - INFO - Epoch 3/20 - Model, Train Loss: 0.0298, Val Loss: 0.0390\n",
      "2024-12-19 15:02:54,356 - INFO - Epoch 4/20 - Model, Train Loss: 0.0276, Val Loss: 0.0404\n",
      "2024-12-19 15:03:16,173 - INFO - Epoch 5/20 - Model, Train Loss: 0.0247, Val Loss: 0.0400\n",
      "2024-12-19 15:03:39,394 - INFO - Epoch 6/20 - Model, Train Loss: 0.0245, Val Loss: 0.0456\n",
      "2024-12-19 15:04:00,600 - INFO - Epoch 7/20 - Model, Train Loss: 0.0227, Val Loss: 0.0405\n",
      "2024-12-19 15:04:23,478 - INFO - Epoch 8/20 - Model, Train Loss: 0.0217, Val Loss: 0.0413\n",
      "2024-12-19 15:04:41,375 - INFO - Epoch 9/20 - Model, Train Loss: 0.0206, Val Loss: 0.0432\n",
      "2024-12-19 15:04:58,485 - INFO - Epoch 10/20 - Model, Train Loss: 0.0195, Val Loss: 0.0427\n",
      "2024-12-19 15:05:15,186 - INFO - Epoch 11/20 - Model, Train Loss: 0.0188, Val Loss: 0.0407\n",
      "2024-12-19 15:05:34,390 - INFO - Epoch 12/20 - Model, Train Loss: 0.0181, Val Loss: 0.0422\n",
      "2024-12-19 15:05:53,285 - INFO - Epoch 13/20 - Model, Train Loss: 0.0175, Val Loss: 0.0476\n",
      "2024-12-19 15:05:53,285 - INFO - Early stopping triggered for Model at epoch 13.\n"
     ]
    }
   ],
   "source": [
    "from evaluation import evaluate_model\n",
    "from training import train_model\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optim.AdamW(sparse_knowledge_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the model\n",
    "train_model(\n",
    "    model=sparse_knowledge_model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=gene_train_loader,\n",
    "    val_loader=gene_val_loader,\n",
    "    device=device,\n",
    "    epochs=20,\n",
    "    gradient_clipping=1.0,\n",
    "    early_stopping_patience=10,\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "sparse_knowledge_results = evaluate_model(\n",
    "    model=sparse_knowledge_model,\n",
    "    criterion=criterion,\n",
    "    test_loader=gene_test_loader,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSE': 0.027327276876349633,\n",
       " 'MAE': 0.11145621538162231,\n",
       " 'R²': 0.1423114538192749,\n",
       " 'Pearson Correlation': 0.5572795307373255}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_knowledge_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5ARG45",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
