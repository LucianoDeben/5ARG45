{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - Unimodal Deep Learning Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 08:30:34,067 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "## Import required libraries and modules\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import importlib\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"src\")))\n",
    "\n",
    "\n",
    "from utils import load_config\n",
    "from preprocess.preprocess import split_data\n",
    "from models import FlexibleFCNN\n",
    "from pipelines import DLModelsPipeline\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Load Config\n",
    "config = load_config(\"../config.yaml\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 08:31:06,157 - INFO - Loading datasets with sampling...\n",
      "2025-01-14 08:31:06,157 - INFO - Sampling 1000 rows from ..\\data/processed/preprocessed_tf.csv...\n",
      "2025-01-14 08:31:06,514 - INFO - Sampling 1000 rows from ..\\data/processed/preprocessed_landmark.csv...\n",
      "2025-01-14 08:31:06,686 - INFO - Sampling 1000 rows from ..\\data/processed/preprocessed_best_inferred.csv...\n",
      "2025-01-14 08:31:09,361 - INFO - Loading ..\\data/processed/preprocessed_gene.csv in chunks...\n",
      "2025-01-14 08:31:15,974 - INFO - Splitting datasets into train/val/test...\n",
      "2025-01-14 08:31:15,990 - INFO - Train Groups: 22 unique values.\n",
      "2025-01-14 08:31:15,993 - INFO - Validation Groups: 6 unique values.\n",
      "2025-01-14 08:31:15,994 - INFO - Test Groups: 4 unique values.\n",
      "2025-01-14 08:31:16,010 - INFO - Train Groups: 22 unique values.\n",
      "2025-01-14 08:31:16,011 - INFO - Validation Groups: 6 unique values.\n",
      "2025-01-14 08:31:16,011 - INFO - Test Groups: 4 unique values.\n",
      "2025-01-14 08:31:16,107 - INFO - Train Groups: 22 unique values.\n",
      "2025-01-14 08:31:16,108 - INFO - Validation Groups: 6 unique values.\n",
      "2025-01-14 08:31:16,108 - INFO - Test Groups: 4 unique values.\n",
      "2025-01-14 08:31:16,211 - INFO - Train Groups: 22 unique values.\n",
      "2025-01-14 08:31:16,220 - INFO - Validation Groups: 6 unique values.\n",
      "2025-01-14 08:31:16,223 - INFO - Test Groups: 4 unique values.\n"
     ]
    }
   ],
   "source": [
    "## Load, Split, and Preprocess Datase\n",
    "# Configurable parameters\n",
    "sample_size = 1000  # Number of rows to sample from each dataset\n",
    "chunk_size = 1000  # Chunk size for loading large datasets\n",
    "\n",
    "# Load datasets\n",
    "logging.info(\"Loading datasets with sampling...\")\n",
    "\n",
    "\n",
    "def load_sampled_data(file_path, sample_size, use_chunks=False, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Load and sample a dataset, with optional chunked loading for large files.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the dataset file.\n",
    "        sample_size (int): Number of rows to sample.\n",
    "        use_chunks (bool): Whether to load the dataset in chunks.\n",
    "        chunk_size (int, optional): Size of chunks if `use_chunks` is True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Sampled DataFrame.\n",
    "    \"\"\"\n",
    "    if use_chunks:\n",
    "        logging.info(f\"Loading {file_path} in chunks...\")\n",
    "        chunks = []\n",
    "        total_loaded = 0\n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "            if total_loaded >= sample_size:\n",
    "                break\n",
    "\n",
    "            # Determine how many rows to sample from this chunk\n",
    "            sample_rows = min(sample_size - total_loaded, len(chunk))\n",
    "            chunks.append(chunk.sample(sample_rows))\n",
    "            total_loaded += sample_rows\n",
    "\n",
    "        sampled_df = pd.concat(chunks, axis=0)\n",
    "        del chunks  # Free memory\n",
    "    else:\n",
    "        logging.info(f\"Sampling {sample_size} rows from {file_path}...\")\n",
    "        sampled_df = pd.read_csv(file_path, nrows=sample_size)\n",
    "\n",
    "    return sampled_df\n",
    "\n",
    "\n",
    "# Load data with sampling\n",
    "\n",
    "tf_df = load_sampled_data(config[\"data_paths\"][\"preprocessed_tf_file\"], sample_size)\n",
    "landmark_df = load_sampled_data(\n",
    "    config[\"data_paths\"][\"preprocessed_landmark_file\"], sample_size\n",
    ")\n",
    "best_inferred_df = load_sampled_data(\n",
    "    config[\"data_paths\"][\"preprocessed_best_inferred_file\"], sample_size\n",
    ")\n",
    "\n",
    "gene_df = load_sampled_data(\n",
    "    config[\"data_paths\"][\"preprocessed_gene_file\"],\n",
    "    sample_size,\n",
    "    use_chunks=True,\n",
    "    chunk_size=chunk_size,\n",
    ")\n",
    "\n",
    "# Split Data\n",
    "logging.info(\"Splitting datasets into train/val/test...\")\n",
    "\n",
    "X_tf_train, y_tf_train, X_tf_val, y_tf_val, X_tf_test, y_tf_test = split_data(\n",
    "    tf_df, target_name=\"viability\", config=config, stratify_by=\"cell_mfc_name\"\n",
    ")\n",
    "(\n",
    "    X_landmark_train,\n",
    "    y_landmark_train,\n",
    "    X_landmark_val,\n",
    "    y_landmark_val,\n",
    "    X_landmark_test,\n",
    "    y_landmark_test,\n",
    ") = split_data(\n",
    "    landmark_df, target_name=\"viability\", config=config, stratify_by=\"cell_mfc_name\"\n",
    ")\n",
    "(\n",
    "    X_best_inferred_train,\n",
    "    y_best_inferred_train,\n",
    "    X_best_inferred_val,\n",
    "    y_best_inferred_val,\n",
    "    X_best_inferred_test,\n",
    "    y_best_inferred_test,\n",
    ") = split_data(\n",
    "    best_inferred_df,\n",
    "    target_name=\"viability\",\n",
    "    config=config,\n",
    "    stratify_by=\"cell_mfc_name\",\n",
    ")\n",
    "X_gene_train, y_gene_train, X_gene_val, y_gene_val, X_gene_test, y_gene_test = (\n",
    "    split_data(\n",
    "        gene_df, target_name=\"viability\", config=config, stratify_by=\"cell_mfc_name\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 08:31:40,024 - INFO - Creating DataLoaders...\n"
     ]
    }
   ],
   "source": [
    "def create_dataloader(X, y, batch_size=32):\n",
    "    # Ensuring X and y are pandas DataFrames/Series:\n",
    "    # If they are arrays, adjust accordingly.\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(X.values, dtype=torch.float32),\n",
    "        torch.tensor(y.values, dtype=torch.float32),\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "logging.info(\"Creating DataLoaders...\")\n",
    "tf_train_loader = create_dataloader(X_tf_train, y_tf_train)\n",
    "tf_val_loader = create_dataloader(X_tf_val, y_tf_val)\n",
    "tf_test_loader = create_dataloader(X_tf_test, y_tf_test)\n",
    "\n",
    "landmark_train_loader = create_dataloader(X_landmark_train, y_landmark_train)\n",
    "landmark_val_loader = create_dataloader(X_landmark_val, y_landmark_val)\n",
    "landmark_test_loader = create_dataloader(X_landmark_test, y_landmark_test)\n",
    "\n",
    "best_inferred_train_loader = create_dataloader(X_best_inferred_train, y_best_inferred_train)\n",
    "best_inferred_val_loader = create_dataloader(X_best_inferred_val, y_best_inferred_val)\n",
    "best_inferred_test_loader = create_dataloader(X_best_inferred_test, y_best_inferred_test)\n",
    "\n",
    "gene_train_loader = create_dataloader(X_gene_train, y_gene_train)\n",
    "gene_val_loader = create_dataloader(X_gene_val, y_gene_val)\n",
    "gene_test_loader = create_dataloader(X_gene_test, y_gene_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = {\n",
    "    \"TF Data\": (tf_train_loader, tf_val_loader, tf_test_loader),\n",
    "    \"Landmark Data\": (landmark_train_loader, landmark_val_loader, landmark_test_loader),\n",
    "    \"Best Inferred Data\": (best_inferred_train_loader, best_inferred_val_loader, best_inferred_test_loader),\n",
    "    \"Gene Data\": (gene_train_loader, gene_val_loader, gene_test_loader),\n",
    "}\n",
    "\n",
    "# Define your model configurations\n",
    "model_configs = {\n",
    "    \"FCNN_Model\": {\n",
    "        \"model_class\": FlexibleFCNN,\n",
    "        \"model_params\": {\n",
    "            \"input_dim\": None,\n",
    "            \"hidden_dims\": [512, 256, 128, 64],\n",
    "            \"output_dim\": 1,\n",
    "            \"activation_fn\": \"prelu\",\n",
    "            \"dropout_prob\": 0.2,\n",
    "            \"residual\": True,\n",
    "            \"norm_type\": \"batchnorm\",\n",
    "            \"weight_init\": \"xavier\",\n",
    "        },\n",
    "        \"criterion\": nn.MSELoss(),\n",
    "        \"optimizer_class\": optim.AdamW,\n",
    "        \"optimizer_params\": {\"lr\": 0.001, \"weight_decay\": 1e-4},\n",
    "        \"scheduler_class\": ReduceLROnPlateau,\n",
    "        \"scheduler_params\": {\"mode\": \"min\", \"patience\": 5},\n",
    "        \"train_params\": {\n",
    "            \"epochs\": 20,\n",
    "            \"gradient_clipping\": 1.0,\n",
    "            \"early_stopping_patience\": 10,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now initialize the pipeline using model_configs instead of model_class & model_params\n",
    "pipeline = DLModelsPipeline(feature_sets=feature_sets, model_configs=model_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 08:37:01,512 - INFO - Starting training and evaluation...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DLModelsPipeline.train_and_evaluate() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train and evaluate the models\u001b[39;00m\n\u001b[0;32m      2\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training and evaluation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Retrieve results\u001b[39;00m\n\u001b[0;32m      6\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollecting results...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: DLModelsPipeline.train_and_evaluate() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the models\n",
    "logging.info(\"Starting training and evaluation...\")\n",
    "pipeline.train_and_evaluate(device=device)\n",
    "\n",
    "# Retrieve results\n",
    "logging.info(\"Collecting results...\")\n",
    "results_df = pipeline.get_results()\n",
    "\n",
    "# Save the results\n",
    "results_df.to_csv(\"combined_metrics.csv\", index=False)\n",
    "logging.info(\"Results saved to combined_metrics.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_bba41_row0_col0, #T_bba41_row0_col1, #T_bba41_row0_col2, #T_bba41_row0_col3 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_bba41\">\n",
       "  <caption>Regression Model Evaluation Metrics</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_bba41_level0_col0\" class=\"col_heading level0 col0\" >MSE</th>\n",
       "      <th id=\"T_bba41_level0_col1\" class=\"col_heading level0 col1\" >MAE</th>\n",
       "      <th id=\"T_bba41_level0_col2\" class=\"col_heading level0 col2\" >R²</th>\n",
       "      <th id=\"T_bba41_level0_col3\" class=\"col_heading level0 col3\" >Pearson Correlation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Feature Set</th>\n",
       "      <th class=\"index_name level1\" >Model Name</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bba41_level0_row0\" class=\"row_heading level0 row0\" >Best Inferred Data</th>\n",
       "      <th id=\"T_bba41_level1_row0\" class=\"row_heading level1 row0\" >FCNN_Model</th>\n",
       "      <td id=\"T_bba41_row0_col0\" class=\"data row0 col0\" >0.039</td>\n",
       "      <td id=\"T_bba41_row0_col1\" class=\"data row0 col1\" >0.141</td>\n",
       "      <td id=\"T_bba41_row0_col2\" class=\"data row0 col2\" >0.397</td>\n",
       "      <td id=\"T_bba41_row0_col3\" class=\"data row0 col3\" >0.645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bba41_level0_row1\" class=\"row_heading level0 row1\" >Gene Data</th>\n",
       "      <th id=\"T_bba41_level1_row1\" class=\"row_heading level1 row1\" >FCNN_Model</th>\n",
       "      <td id=\"T_bba41_row1_col0\" class=\"data row1 col0\" >0.040</td>\n",
       "      <td id=\"T_bba41_row1_col1\" class=\"data row1 col1\" >0.155</td>\n",
       "      <td id=\"T_bba41_row1_col2\" class=\"data row1 col2\" >0.384</td>\n",
       "      <td id=\"T_bba41_row1_col3\" class=\"data row1 col3\" >0.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bba41_level0_row2\" class=\"row_heading level0 row2\" >Landmark Data</th>\n",
       "      <th id=\"T_bba41_level1_row2\" class=\"row_heading level1 row2\" >FCNN_Model</th>\n",
       "      <td id=\"T_bba41_row2_col0\" class=\"data row2 col0\" >0.044</td>\n",
       "      <td id=\"T_bba41_row2_col1\" class=\"data row2 col1\" >0.151</td>\n",
       "      <td id=\"T_bba41_row2_col2\" class=\"data row2 col2\" >0.328</td>\n",
       "      <td id=\"T_bba41_row2_col3\" class=\"data row2 col3\" >0.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bba41_level0_row3\" class=\"row_heading level0 row3\" >TF Data</th>\n",
       "      <th id=\"T_bba41_level1_row3\" class=\"row_heading level1 row3\" >FCNN_Model</th>\n",
       "      <td id=\"T_bba41_row3_col0\" class=\"data row3 col0\" >0.048</td>\n",
       "      <td id=\"T_bba41_row3_col1\" class=\"data row3 col1\" >0.172</td>\n",
       "      <td id=\"T_bba41_row3_col2\" class=\"data row3 col2\" >0.260</td>\n",
       "      <td id=\"T_bba41_row3_col3\" class=\"data row3 col3\" >0.520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1ac381d8f10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "styled_results = (\n",
    "    results_df.style.format(precision=3)\n",
    "    .set_caption(\"Regression Model Evaluation Metrics\")\n",
    "    .highlight_max(\n",
    "        subset=[\"R²\", \"Pearson Correlation\"], color=\"lightgreen\"\n",
    "    )\n",
    "    .highlight_min(subset=[\"MAE\", \"MSE\"], color=\"lightgreen\")\n",
    ")\n",
    "styled_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decoupler as dc\n",
    "\n",
    "\n",
    "def create_gene_tf_matrix(\n",
    "    net: pd.DataFrame, genes: list = None, tfs: list = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates a PyTorch tensor representing the gene-TF regulatory matrix from a network dataframe.\n",
    "\n",
    "    Args:\n",
    "        net (pd.DataFrame): DataFrame containing the regulatory network with the following columns:\n",
    "            - \"source\": Transcription factors (TFs).\n",
    "            - \"target\": Genes regulated by the TFs.\n",
    "            - \"weight\": Interaction weight (1 for activation, -1 for inhibition).\n",
    "        genes (list, optional): List of genes to include in the matrix. If None, all unique genes in `net` are used.\n",
    "        tfs (list, optional): List of TFs to include in the matrix. If None, all unique TFs in `net` are used.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (num_genes, num_tfs) where:\n",
    "            - `1` indicates an activating interaction.\n",
    "            - `-1` indicates an inhibiting interaction.\n",
    "            - `0` indicates no interaction.\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    required_columns = {\"source\", \"target\", \"weight\"}\n",
    "    if not required_columns.issubset(net.columns):\n",
    "        raise ValueError(\n",
    "            f\"The `net` dataframe must contain the columns: {required_columns}\"\n",
    "        )\n",
    "\n",
    "    # Use all unique genes and TFs if not provided\n",
    "    available_genes = sorted(net[\"target\"].unique())\n",
    "    available_tfs = sorted(net[\"source\"].unique())\n",
    "\n",
    "    if genes is None:\n",
    "        genes = available_genes\n",
    "    else:\n",
    "        # Filter out genes not in the network\n",
    "        genes = [gene for gene in genes if gene in available_genes]\n",
    "\n",
    "    if tfs is None:\n",
    "        tfs = available_tfs\n",
    "    else:\n",
    "        # Filter out TFs not in the network\n",
    "        tfs = [tf for tf in tfs if tf in available_tfs]\n",
    "\n",
    "    # Initialize a DataFrame with zeros (default for no interaction)\n",
    "    gene_to_tf_df = pd.DataFrame(0, index=genes, columns=tfs, dtype=float)\n",
    "\n",
    "    # Populate the DataFrame with interaction weights\n",
    "    for _, row in net.iterrows():\n",
    "        gene = row[\"target\"]\n",
    "        tf = row[\"source\"]\n",
    "        weight = row[\"weight\"]\n",
    "        if gene in genes and tf in tfs:\n",
    "            gene_to_tf_df.at[gene, tf] = weight\n",
    "\n",
    "    # Convert the DataFrame to a PyTorch tensor\n",
    "    gene_to_tf_matrix = torch.tensor(gene_to_tf_df.values, dtype=torch.float32)\n",
    "    return gene_to_tf_matrix\n",
    "\n",
    "\n",
    "# Define gene-TF matrix generator for SparseKnowledgeNetwork\n",
    "def gene_tf_matrix_generator(feature_name, train_loader):\n",
    "    if feature_name in {\"Gene Data\", \"Best Inferred Data\", \"Landmark Data\"}:\n",
    "        # Extract gene names from training dataset\n",
    "        gene_names = train_loader.dataset.dataset.columns\n",
    "        return create_gene_tf_matrix(net, genes=list(gene_names))\n",
    "    return None  # No gene-TF matrix needed for other feature sets\n",
    "\n",
    "\n",
    "def filter_genes_to_collectri(dataset: pd.DataFrame, net: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the dataset to include only genes present in the Collectri network.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): Gene expression dataset (rows = samples, columns = genes).\n",
    "        net (pd.DataFrame): Regulatory network dataframe with a \"target\" column containing gene names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered dataset containing only Collectri genes.\n",
    "    \"\"\"\n",
    "    # Extract unique genes from the \"target\" column of the net dataframe\n",
    "    collectri_genes = net[\"target\"].unique()\n",
    "\n",
    "    # Find the intersection of dataset columns and Collectri genes\n",
    "    intersecting_genes = set(dataset.columns).intersection(collectri_genes)\n",
    "\n",
    "    # Check if any genes are found; warn if not\n",
    "    if not intersecting_genes:\n",
    "        logging.warning(\"No overlapping genes between dataset and Collectri network.\")\n",
    "\n",
    "    # Filter the dataset to include only the intersecting genes\n",
    "    filtered_dataset = dataset[list(intersecting_genes)]\n",
    "\n",
    "    logging.info(\n",
    "        f\"Filtered dataset shape: {filtered_dataset.shape} (Retained {len(intersecting_genes)} genes)\"\n",
    "    )\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "net = dc.get_collectri(organism='human', split_complexes=False)\n",
    "\n",
    "# Filter gene datasets to include only Collectri genes\n",
    "X_gene_train = filter_genes_to_collectri(X_gene_train, net)\n",
    "X_gene_val = filter_genes_to_collectri(X_gene_val, net)\n",
    "X_gene_test = filter_genes_to_collectri(X_gene_test, net)\n",
    "\n",
    "X_best_inferred_train = filter_genes_to_collectri(X_best_inferred_train, net)\n",
    "X_best_inferred_val = filter_genes_to_collectri(X_best_inferred_val, net)\n",
    "X_best_inferred_test = filter_genes_to_collectri(X_best_inferred_test, net)\n",
    "\n",
    "X_landmark_train = filter_genes_to_collectri(X_landmark_train, net)\n",
    "X_landmark_val = filter_genes_to_collectri(X_landmark_val, net)\n",
    "X_landmark_test = filter_genes_to_collectri(X_landmark_test, net)\n",
    "\n",
    "# Create new dataloaders\n",
    "gene_train_loader = create_dataloader(X_gene_train, y_gene_train)\n",
    "gene_val_loader = create_dataloader(X_gene_val, y_gene_val)\n",
    "gene_test_loader = create_dataloader(X_gene_test, y_gene_test)\n",
    "\n",
    "best_inferred_train_loader = create_dataloader(X_best_inferred_train, y_best_inferred_train)\n",
    "best_inferred_val_loader = create_dataloader(X_best_inferred_val, y_best_inferred_val)\n",
    "best_inferred_test_loader = create_dataloader(X_best_inferred_test, y_best_inferred_test)\n",
    "\n",
    "landmark_train_loader = create_dataloader(X_landmark_train, y_landmark_train)\n",
    "landmark_val_loader = create_dataloader(X_landmark_val, y_landmark_val)\n",
    "landmark_test_loader = create_dataloader(X_landmark_test, y_landmark_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decoupler as dc\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from models import SparseKnowledgeNetwork\n",
    "\n",
    "# Define feature sets with dataloader tuples\n",
    "feature_sets = {\n",
    "    \"Landmark Data\": (landmark_train_loader, landmark_val_loader, landmark_test_loader),\n",
    "    \"Best Inferred Data\": (\n",
    "        best_inferred_train_loader,\n",
    "        best_inferred_val_loader,\n",
    "        best_inferred_test_loader,\n",
    "    ),\n",
    "    \"Gene Data\": (gene_train_loader, gene_val_loader, gene_test_loader),\n",
    "}\n",
    "\n",
    "# Define model configurations\n",
    "model_configs = {\n",
    "    \"FlexibleFCNN\": {\n",
    "        \"model_class\": FlexibleFCNN,\n",
    "        \"model_params\": {\n",
    "            \"hidden_dims\": [512, 256, 128, 64],\n",
    "            \"output_dim\": 1,\n",
    "            \"activation_fn\": \"relu\",\n",
    "            \"dropout_prob\": 0.2,\n",
    "            \"residual\": True,\n",
    "            \"norm_type\": \"batchnorm\",\n",
    "            \"weight_init\": \"xavier\",\n",
    "        },\n",
    "        \"criterion\": nn.MSELoss(),\n",
    "        \"optimizer_class\": optim.AdamW,\n",
    "        \"optimizer_params\": {\"lr\": 0.001, \"weight_decay\": 1e-4},\n",
    "        \"scheduler_class\": ReduceLROnPlateau,\n",
    "        \"scheduler_params\": {\"mode\": \"min\", \"patience\": 5},\n",
    "        \"train_params\": {\n",
    "            \"epochs\": 20,\n",
    "            \"gradient_clipping\": 1.0,\n",
    "            \"early_stopping_patience\": 10,\n",
    "        },\n",
    "    },\n",
    "    \"SparseKnowledgeNetwork\": {\n",
    "        \"model_class\": SparseKnowledgeNetwork,\n",
    "        \"model_params\": {\n",
    "            \"hidden_dims\": [512, 256, 128, 64],\n",
    "            \"output_dim\": 1,\n",
    "            \"first_activation\": \"tanh\",\n",
    "            \"downstream_activation\": \"relu\",\n",
    "            \"dropout_prob\": 0.2,\n",
    "            \"weight_init\": \"xavier\",\n",
    "            \"use_batchnorm\": True,\n",
    "        },\n",
    "        \"criterion\": nn.MSELoss(),\n",
    "        \"optimizer_class\": optim.AdamW,\n",
    "        \"optimizer_params\": {\"lr\": 0.001, \"weight_decay\": 1e-4},\n",
    "        \"scheduler_class\": ReduceLROnPlateau,\n",
    "        \"scheduler_params\": {\"mode\": \"min\", \"patience\": 5},\n",
    "        \"train_params\": {\n",
    "            \"epochs\": 20,\n",
    "            \"gradient_clipping\": 1.0,\n",
    "            \"early_stopping_patience\": 10,\n",
    "        },\n",
    "        \"gene_tf_matrix_generator\": gene_tf_matrix_generator,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Initialize and run the pipeline\n",
    "pipeline = DLModelsPipeline(feature_sets, model_configs)\n",
    "pipeline.train_and_evaluate()\n",
    "\n",
    "# Retrieve results\n",
    "results_df = pipeline.get_results()\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5ARG45",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
