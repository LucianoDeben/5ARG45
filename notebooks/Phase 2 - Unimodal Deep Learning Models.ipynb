{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - Unimodal Deep Learning Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 14:38:30,217 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "## Import required libraries and modules\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import importlib\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"src\")))\n",
    "\n",
    "\n",
    "from utils import load_config\n",
    "from preprocess.preprocess import split_data\n",
    "from models import FlexibleFCNN\n",
    "from pipelines import DLModelsPipeline\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Load Config\n",
    "config = load_config(\"../config.yaml\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 14:56:57,869 - INFO - Loading datasets with sampling...\n",
      "2025-01-13 14:56:57,872 - INFO - Sampling 1000 rows from ..\\data/processed/preprocessed_tf.csv...\n",
      "2025-01-13 14:56:58,115 - INFO - Sampling 1000 rows from ..\\data/processed/preprocessed_landmark.csv...\n",
      "2025-01-13 14:56:58,284 - INFO - Sampling 1000 rows from ..\\data/processed/preprocessed_best_inferred.csv...\n",
      "2025-01-13 14:57:00,907 - INFO - Loading ..\\data/processed/preprocessed_gene.csv in chunks...\n",
      "2025-01-13 14:57:07,397 - INFO - Splitting datasets into train/val/test...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (520, 683), Validation Shape: (222, 683), Test Shape: (258, 683)\n",
      "Train Shape: (520, 979), Validation Shape: (222, 979), Test Shape: (258, 979)\n",
      "Train Shape: (520, 10175), Validation Shape: (222, 10175), Test Shape: (258, 10175)\n",
      "Train Shape: (520, 12329), Validation Shape: (222, 12329), Test Shape: (258, 12329)\n"
     ]
    }
   ],
   "source": [
    "## Load, Split, and Preprocess Datase\n",
    "# Configurable parameters\n",
    "sample_size = 1000  # Number of rows to sample from each dataset\n",
    "chunk_size = 1000  # Chunk size for loading large datasets\n",
    "\n",
    "# Load datasets\n",
    "logging.info(\"Loading datasets with sampling...\")\n",
    "\n",
    "\n",
    "def load_sampled_data(file_path, sample_size, use_chunks=False, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Load and sample a dataset, with optional chunked loading for large files.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the dataset file.\n",
    "        sample_size (int): Number of rows to sample.\n",
    "        use_chunks (bool): Whether to load the dataset in chunks.\n",
    "        chunk_size (int, optional): Size of chunks if `use_chunks` is True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Sampled DataFrame.\n",
    "    \"\"\"\n",
    "    if use_chunks:\n",
    "        logging.info(f\"Loading {file_path} in chunks...\")\n",
    "        chunks = []\n",
    "        total_loaded = 0\n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "            if total_loaded >= sample_size:\n",
    "                break\n",
    "\n",
    "            # Determine how many rows to sample from this chunk\n",
    "            sample_rows = min(sample_size - total_loaded, len(chunk))\n",
    "            chunks.append(chunk.sample(sample_rows))\n",
    "            total_loaded += sample_rows\n",
    "\n",
    "        sampled_df = pd.concat(chunks, axis=0)\n",
    "        del chunks  # Free memory\n",
    "    else:\n",
    "        logging.info(f\"Sampling {sample_size} rows from {file_path}...\")\n",
    "        sampled_df = pd.read_csv(file_path, nrows=sample_size)\n",
    "\n",
    "    return sampled_df\n",
    "\n",
    "\n",
    "# Load data with sampling\n",
    "\n",
    "tf_df = load_sampled_data(config[\"data_paths\"][\"preprocessed_tf_file\"], sample_size)\n",
    "landmark_df = load_sampled_data(\n",
    "    config[\"data_paths\"][\"preprocessed_landmark_file\"], sample_size\n",
    ")\n",
    "best_inferred_df = load_sampled_data(\n",
    "    config[\"data_paths\"][\"preprocessed_best_inferred_file\"], sample_size\n",
    ")\n",
    "\n",
    "gene_df = load_sampled_data(\n",
    "    config[\"data_paths\"][\"preprocessed_gene_file\"],\n",
    "    sample_size,\n",
    "    use_chunks=True,\n",
    "    chunk_size=chunk_size,\n",
    ")\n",
    "\n",
    "# Split Data\n",
    "logging.info(\"Splitting datasets into train/val/test...\")\n",
    "\n",
    "X_tf_train, y_tf_train, X_tf_val, y_tf_val, X_tf_test, y_tf_test = split_data(\n",
    "    tf_df, target_name=\"viability\", config=config, stratify_by=\"cell_mfc_name\"\n",
    ")\n",
    "(\n",
    "    X_landmark_train,\n",
    "    y_landmark_train,\n",
    "    X_landmark_val,\n",
    "    y_landmark_val,\n",
    "    X_landmark_test,\n",
    "    y_landmark_test,\n",
    ") = split_data(\n",
    "    landmark_df, target_name=\"viability\", config=config, stratify_by=\"cell_mfc_name\"\n",
    ")\n",
    "(\n",
    "    X_best_inferred_train,\n",
    "    y_best_inferred_train,\n",
    "    X_best_inferred_val,\n",
    "    y_best_inferred_val,\n",
    "    X_best_inferred_test,\n",
    "    y_best_inferred_test,\n",
    ") = split_data(\n",
    "    best_inferred_df,\n",
    "    target_name=\"viability\",\n",
    "    config=config,\n",
    "    stratify_by=\"cell_mfc_name\",\n",
    ")\n",
    "X_gene_train, y_gene_train, X_gene_val, y_gene_val, X_gene_test, y_gene_test = (\n",
    "    split_data(\n",
    "        gene_df, target_name=\"viability\", config=config, stratify_by=\"cell_mfc_name\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 14:57:13,483 - INFO - Creating DataLoaders...\n"
     ]
    }
   ],
   "source": [
    "def create_dataloader(X, y, batch_size=32):\n",
    "    # Ensuring X and y are pandas DataFrames/Series:\n",
    "    # If they are arrays, adjust accordingly.\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(X.values, dtype=torch.float32),\n",
    "        torch.tensor(y.values, dtype=torch.float32),\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "logging.info(\"Creating DataLoaders...\")\n",
    "tf_train_loader = create_dataloader(X_tf_train, y_tf_train)\n",
    "tf_val_loader = create_dataloader(X_tf_val, y_tf_val)\n",
    "tf_test_loader = create_dataloader(X_tf_test, y_tf_test)\n",
    "\n",
    "landmark_train_loader = create_dataloader(X_landmark_train, y_landmark_train)\n",
    "landmark_val_loader = create_dataloader(X_landmark_val, y_landmark_val)\n",
    "landmark_test_loader = create_dataloader(X_landmark_test, y_landmark_test)\n",
    "\n",
    "best_inferred_train_loader = create_dataloader(X_best_inferred_train, y_best_inferred_train)\n",
    "best_inferred_val_loader = create_dataloader(X_best_inferred_val, y_best_inferred_val)\n",
    "best_inferred_test_loader = create_dataloader(X_best_inferred_test, y_best_inferred_test)\n",
    "\n",
    "gene_train_loader = create_dataloader(X_gene_train, y_gene_train)\n",
    "gene_val_loader = create_dataloader(X_gene_val, y_gene_val)\n",
    "gene_test_loader = create_dataloader(X_gene_test, y_gene_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = {\n",
    "    \"TF Data\": (tf_train_loader, tf_val_loader, tf_test_loader),\n",
    "    \"Landmark Data\": (landmark_train_loader, landmark_val_loader, landmark_test_loader),\n",
    "    \"Best Inferred Data\": (best_inferred_train_loader, best_inferred_val_loader, best_inferred_test_loader),\n",
    "    \"Gene Data\": (gene_train_loader, gene_val_loader, gene_test_loader),\n",
    "}\n",
    "\n",
    "# Define your model configurations\n",
    "model_configs = {\n",
    "    \"FCNN_Model\": {\n",
    "        \"model_class\": FlexibleFCNN,\n",
    "        \"model_params\": {\n",
    "            \"hidden_dims\": [512, 256, 128, 64],\n",
    "            \"output_dim\": 1,\n",
    "            \"activation_fn\": \"prelu\",\n",
    "            \"dropout_prob\": 0.2,\n",
    "            \"residual\": True,\n",
    "            \"norm_type\": \"batchnorm\",\n",
    "            \"weight_init\": \"xavier\",\n",
    "        },\n",
    "        \"criterion\": nn.MSELoss(),\n",
    "        \"optimizer_class\": optim.AdamW,\n",
    "        \"optimizer_params\": {\"lr\": 0.001, \"weight_decay\": 1e-4},\n",
    "        \"scheduler_class\": ReduceLROnPlateau,\n",
    "        \"scheduler_params\": {\"mode\": \"min\", \"patience\": 5},\n",
    "        \"train_params\": {\n",
    "            \"epochs\": 20,\n",
    "            \"gradient_clipping\": 1.0,\n",
    "            \"early_stopping_patience\": 10,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now initialize the pipeline using model_configs instead of model_class & model_params\n",
    "pipeline = DLModelsPipeline(feature_sets=feature_sets, model_configs=model_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 14:57:32,904 - INFO - Starting training and evaluation...\n",
      "d:\\Programming\\5ARG45\\5ARG45\\src\\training.py:69: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_mixed_precision)\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]d:\\Programming\\5ARG45\\5ARG45\\src\\training.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_mixed_precision):\n",
      "d:\\Programming\\5ARG45\\5ARG45\\src\\training.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast(enabled=use_mixed_precision):\n",
      "2025-01-13 14:57:33,202 - INFO - Epoch 1/20 - Model, Train Loss: 1.8064, Val Loss: 1.0470\n",
      "  5%|▌         | 1/20 [00:00<00:05,  3.55it/s]2025-01-13 14:57:33,367 - INFO - Epoch 2/20 - Model, Train Loss: 0.8240, Val Loss: 0.3460\n",
      " 10%|█         | 2/20 [00:00<00:03,  4.71it/s]2025-01-13 14:57:33,538 - INFO - Epoch 3/20 - Model, Train Loss: 0.4714, Val Loss: 0.1874\n",
      " 15%|█▌        | 3/20 [00:00<00:03,  5.17it/s]2025-01-13 14:57:33,716 - INFO - Epoch 4/20 - Model, Train Loss: 0.3498, Val Loss: 0.1111\n",
      " 20%|██        | 4/20 [00:00<00:02,  5.36it/s]2025-01-13 14:57:33,883 - INFO - Epoch 5/20 - Model, Train Loss: 0.3078, Val Loss: 0.1128\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  5.57it/s]2025-01-13 14:57:34,041 - INFO - Epoch 6/20 - Model, Train Loss: 0.2981, Val Loss: 0.1021\n",
      " 30%|███       | 6/20 [00:01<00:02,  5.79it/s]2025-01-13 14:57:34,222 - INFO - Epoch 7/20 - Model, Train Loss: 0.2484, Val Loss: 0.0961\n",
      " 35%|███▌      | 7/20 [00:01<00:02,  5.70it/s]2025-01-13 14:57:34,397 - INFO - Epoch 8/20 - Model, Train Loss: 0.2148, Val Loss: 0.0944\n",
      " 40%|████      | 8/20 [00:01<00:02,  5.70it/s]2025-01-13 14:57:34,579 - INFO - Epoch 9/20 - Model, Train Loss: 0.2090, Val Loss: 0.0649\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  5.64it/s]2025-01-13 14:57:34,765 - INFO - Epoch 10/20 - Model, Train Loss: 0.1756, Val Loss: 0.0751\n",
      " 50%|█████     | 10/20 [00:01<00:01,  5.57it/s]2025-01-13 14:57:34,928 - INFO - Epoch 11/20 - Model, Train Loss: 0.1446, Val Loss: 0.0845\n",
      " 55%|█████▌    | 11/20 [00:02<00:01,  5.73it/s]2025-01-13 14:57:35,104 - INFO - Epoch 12/20 - Model, Train Loss: 0.1375, Val Loss: 0.0713\n",
      " 60%|██████    | 12/20 [00:02<00:01,  5.73it/s]2025-01-13 14:57:35,281 - INFO - Epoch 13/20 - Model, Train Loss: 0.1314, Val Loss: 0.0663\n",
      " 65%|██████▌   | 13/20 [00:02<00:01,  5.70it/s]2025-01-13 14:57:35,463 - INFO - Epoch 14/20 - Model, Train Loss: 0.1218, Val Loss: 0.0709\n",
      " 70%|███████   | 14/20 [00:02<00:01,  5.63it/s]2025-01-13 14:57:35,695 - INFO - Epoch 15/20 - Model, Train Loss: 0.0923, Val Loss: 0.0721\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  5.15it/s]2025-01-13 14:57:35,868 - INFO - Epoch 16/20 - Model, Train Loss: 0.0951, Val Loss: 0.0649\n",
      " 80%|████████  | 16/20 [00:02<00:00,  5.33it/s]2025-01-13 14:57:36,068 - INFO - Epoch 17/20 - Model, Train Loss: 0.0955, Val Loss: 0.0632\n",
      " 85%|████████▌ | 17/20 [00:03<00:00,  5.20it/s]2025-01-13 14:57:36,246 - INFO - Epoch 18/20 - Model, Train Loss: 0.0902, Val Loss: 0.0604\n",
      " 90%|█████████ | 18/20 [00:03<00:00,  5.32it/s]2025-01-13 14:57:36,410 - INFO - Epoch 19/20 - Model, Train Loss: 0.0747, Val Loss: 0.0613\n",
      " 95%|█████████▌| 19/20 [00:03<00:00,  5.56it/s]2025-01-13 14:57:36,570 - INFO - Epoch 20/20 - Model, Train Loss: 0.0863, Val Loss: 0.0612\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.48it/s]\n",
      "d:\\Programming\\5ARG45\\5ARG45\\src\\training.py:69: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_mixed_precision)\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]d:\\Programming\\5ARG45\\5ARG45\\src\\training.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_mixed_precision):\n",
      "d:\\Programming\\5ARG45\\5ARG45\\src\\training.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast(enabled=use_mixed_precision):\n",
      "2025-01-13 14:57:36,788 - INFO - Epoch 1/20 - Model, Train Loss: 2.7273, Val Loss: 1.1740\n",
      "  5%|▌         | 1/20 [00:00<00:03,  5.54it/s]2025-01-13 14:57:36,986 - INFO - Epoch 2/20 - Model, Train Loss: 1.2849, Val Loss: 0.6652\n",
      " 10%|█         | 2/20 [00:00<00:03,  5.27it/s]2025-01-13 14:57:37,178 - INFO - Epoch 3/20 - Model, Train Loss: 0.6887, Val Loss: 0.4180\n",
      " 15%|█▌        | 3/20 [00:00<00:03,  5.25it/s]2025-01-13 14:57:37,366 - INFO - Epoch 4/20 - Model, Train Loss: 0.3554, Val Loss: 0.1816\n",
      " 20%|██        | 4/20 [00:00<00:03,  5.24it/s]2025-01-13 14:57:37,564 - INFO - Epoch 5/20 - Model, Train Loss: 0.3555, Val Loss: 0.1192\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  5.18it/s]2025-01-13 14:57:37,774 - INFO - Epoch 6/20 - Model, Train Loss: 0.2718, Val Loss: 0.0898\n",
      " 30%|███       | 6/20 [00:01<00:02,  5.05it/s]2025-01-13 14:57:37,963 - INFO - Epoch 7/20 - Model, Train Loss: 0.2267, Val Loss: 0.1238\n",
      " 35%|███▌      | 7/20 [00:01<00:02,  5.14it/s]2025-01-13 14:57:38,149 - INFO - Epoch 8/20 - Model, Train Loss: 0.2571, Val Loss: 0.0847\n",
      " 40%|████      | 8/20 [00:01<00:02,  5.20it/s]2025-01-13 14:57:38,358 - INFO - Epoch 9/20 - Model, Train Loss: 0.2191, Val Loss: 0.0838\n",
      " 45%|████▌     | 9/20 [00:01<00:02,  5.06it/s]2025-01-13 14:57:38,533 - INFO - Epoch 10/20 - Model, Train Loss: 0.2137, Val Loss: 0.0843\n",
      " 50%|█████     | 10/20 [00:01<00:01,  5.26it/s]2025-01-13 14:57:38,737 - INFO - Epoch 11/20 - Model, Train Loss: 0.1696, Val Loss: 0.0725\n",
      " 55%|█████▌    | 11/20 [00:02<00:01,  5.13it/s]2025-01-13 14:57:38,939 - INFO - Epoch 12/20 - Model, Train Loss: 0.1303, Val Loss: 0.0874\n",
      " 60%|██████    | 12/20 [00:02<00:01,  5.09it/s]2025-01-13 14:57:39,129 - INFO - Epoch 13/20 - Model, Train Loss: 0.1525, Val Loss: 0.0730\n",
      " 65%|██████▌   | 13/20 [00:02<00:01,  5.14it/s]2025-01-13 14:57:39,305 - INFO - Epoch 14/20 - Model, Train Loss: 0.1362, Val Loss: 0.0828\n",
      " 70%|███████   | 14/20 [00:02<00:01,  5.30it/s]2025-01-13 14:57:39,487 - INFO - Epoch 15/20 - Model, Train Loss: 0.1223, Val Loss: 0.0741\n",
      " 75%|███████▌  | 15/20 [00:02<00:00,  5.36it/s]2025-01-13 14:57:39,667 - INFO - Epoch 16/20 - Model, Train Loss: 0.0986, Val Loss: 0.0595\n",
      " 80%|████████  | 16/20 [00:03<00:00,  5.39it/s]2025-01-13 14:57:39,851 - INFO - Epoch 17/20 - Model, Train Loss: 0.0906, Val Loss: 0.0670\n",
      " 85%|████████▌ | 17/20 [00:03<00:00,  5.43it/s]2025-01-13 14:57:40,039 - INFO - Epoch 18/20 - Model, Train Loss: 0.0847, Val Loss: 0.0586\n",
      " 90%|█████████ | 18/20 [00:03<00:00,  5.37it/s]2025-01-13 14:57:40,222 - INFO - Epoch 19/20 - Model, Train Loss: 0.0896, Val Loss: 0.0557\n",
      " 95%|█████████▌| 19/20 [00:03<00:00,  5.38it/s]2025-01-13 14:57:40,412 - INFO - Epoch 20/20 - Model, Train Loss: 0.0714, Val Loss: 0.0662\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.26it/s]\n",
      "d:\\Programming\\5ARG45\\5ARG45\\src\\training.py:69: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_mixed_precision)\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]d:\\Programming\\5ARG45\\5ARG45\\src\\training.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_mixed_precision):\n",
      "d:\\Programming\\5ARG45\\5ARG45\\src\\training.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast(enabled=use_mixed_precision):\n",
      "2025-01-13 14:57:40,946 - INFO - Epoch 1/20 - Model, Train Loss: 1.6002, Val Loss: 0.5988\n",
      "  5%|▌         | 1/20 [00:00<00:08,  2.30it/s]2025-01-13 14:57:41,335 - INFO - Epoch 2/20 - Model, Train Loss: 0.6392, Val Loss: 0.2256\n",
      " 10%|█         | 2/20 [00:00<00:07,  2.46it/s]2025-01-13 14:57:41,786 - INFO - Epoch 3/20 - Model, Train Loss: 0.4756, Val Loss: 0.1307\n",
      " 15%|█▌        | 3/20 [00:01<00:07,  2.35it/s]2025-01-13 14:57:42,173 - INFO - Epoch 4/20 - Model, Train Loss: 0.3974, Val Loss: 0.0879\n",
      " 20%|██        | 4/20 [00:01<00:06,  2.43it/s]2025-01-13 14:57:42,571 - INFO - Epoch 5/20 - Model, Train Loss: 0.2952, Val Loss: 0.0916\n",
      " 25%|██▌       | 5/20 [00:02<00:06,  2.49it/s]2025-01-13 14:57:42,978 - INFO - Epoch 6/20 - Model, Train Loss: 0.2809, Val Loss: 0.0807\n",
      " 30%|███       | 6/20 [00:02<00:05,  2.44it/s]2025-01-13 14:57:43,420 - INFO - Epoch 7/20 - Model, Train Loss: 0.2442, Val Loss: 0.1079\n",
      " 35%|███▌      | 7/20 [00:02<00:05,  2.41it/s]2025-01-13 14:57:43,791 - INFO - Epoch 8/20 - Model, Train Loss: 0.2229, Val Loss: 0.0822\n",
      " 40%|████      | 8/20 [00:03<00:04,  2.50it/s]2025-01-13 14:57:44,147 - INFO - Epoch 9/20 - Model, Train Loss: 0.1748, Val Loss: 0.0606\n",
      " 45%|████▌     | 9/20 [00:03<00:04,  2.56it/s]2025-01-13 14:57:44,501 - INFO - Epoch 10/20 - Model, Train Loss: 0.1634, Val Loss: 0.0514\n",
      " 50%|█████     | 10/20 [00:03<00:03,  2.65it/s]2025-01-13 14:57:44,910 - INFO - Epoch 11/20 - Model, Train Loss: 0.1733, Val Loss: 0.0437\n",
      " 55%|█████▌    | 11/20 [00:04<00:03,  2.57it/s]2025-01-13 14:57:45,318 - INFO - Epoch 12/20 - Model, Train Loss: 0.1459, Val Loss: 0.0629\n",
      " 60%|██████    | 12/20 [00:04<00:03,  2.56it/s]2025-01-13 14:57:45,667 - INFO - Epoch 13/20 - Model, Train Loss: 0.1417, Val Loss: 0.0518\n",
      " 65%|██████▌   | 13/20 [00:05<00:02,  2.64it/s]2025-01-13 14:57:46,020 - INFO - Epoch 14/20 - Model, Train Loss: 0.1154, Val Loss: 0.0525\n",
      " 70%|███████   | 14/20 [00:05<00:02,  2.70it/s]2025-01-13 14:57:46,411 - INFO - Epoch 15/20 - Model, Train Loss: 0.0879, Val Loss: 0.0631\n",
      " 75%|███████▌  | 15/20 [00:05<00:01,  2.65it/s]2025-01-13 14:57:46,769 - INFO - Epoch 16/20 - Model, Train Loss: 0.0946, Val Loss: 0.0520\n",
      " 80%|████████  | 16/20 [00:06<00:01,  2.70it/s]2025-01-13 14:57:47,126 - INFO - Epoch 17/20 - Model, Train Loss: 0.0833, Val Loss: 0.0485\n",
      " 85%|████████▌ | 17/20 [00:06<00:01,  2.73it/s]2025-01-13 14:57:47,498 - INFO - Epoch 18/20 - Model, Train Loss: 0.0841, Val Loss: 0.0494\n",
      " 90%|█████████ | 18/20 [00:06<00:00,  2.72it/s]2025-01-13 14:57:47,905 - INFO - Epoch 19/20 - Model, Train Loss: 0.0774, Val Loss: 0.0490\n",
      " 95%|█████████▌| 19/20 [00:07<00:00,  2.63it/s]2025-01-13 14:57:48,263 - INFO - Epoch 20/20 - Model, Train Loss: 0.0702, Val Loss: 0.0493\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.58it/s]\n",
      "d:\\Programming\\5ARG45\\5ARG45\\src\\training.py:69: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_mixed_precision)\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]d:\\Programming\\5ARG45\\5ARG45\\src\\training.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_mixed_precision):\n",
      "d:\\Programming\\5ARG45\\5ARG45\\src\\training.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast(enabled=use_mixed_precision):\n",
      "2025-01-13 14:57:48,810 - INFO - Epoch 1/20 - Model, Train Loss: 1.9473, Val Loss: 1.9884\n",
      "  5%|▌         | 1/20 [00:00<00:07,  2.42it/s]2025-01-13 14:57:49,291 - INFO - Epoch 2/20 - Model, Train Loss: 0.9136, Val Loss: 0.2632\n",
      " 10%|█         | 2/20 [00:00<00:08,  2.18it/s]2025-01-13 14:57:49,774 - INFO - Epoch 3/20 - Model, Train Loss: 0.4525, Val Loss: 0.1643\n",
      " 15%|█▌        | 3/20 [00:01<00:07,  2.14it/s]2025-01-13 14:57:50,240 - INFO - Epoch 4/20 - Model, Train Loss: 0.2969, Val Loss: 0.0850\n",
      " 20%|██        | 4/20 [00:01<00:07,  2.15it/s]2025-01-13 14:57:50,684 - INFO - Epoch 5/20 - Model, Train Loss: 0.3462, Val Loss: 0.0750\n",
      " 25%|██▌       | 5/20 [00:02<00:06,  2.18it/s]2025-01-13 14:57:51,138 - INFO - Epoch 6/20 - Model, Train Loss: 0.2328, Val Loss: 0.0571\n",
      " 30%|███       | 6/20 [00:02<00:06,  2.19it/s]2025-01-13 14:57:51,574 - INFO - Epoch 7/20 - Model, Train Loss: 0.2583, Val Loss: 0.0735\n",
      " 35%|███▌      | 7/20 [00:03<00:05,  2.24it/s]2025-01-13 14:57:52,101 - INFO - Epoch 8/20 - Model, Train Loss: 0.2347, Val Loss: 0.1119\n",
      " 40%|████      | 8/20 [00:03<00:05,  2.12it/s]2025-01-13 14:57:52,561 - INFO - Epoch 9/20 - Model, Train Loss: 0.2289, Val Loss: 0.1144\n",
      " 45%|████▌     | 9/20 [00:04<00:05,  2.13it/s]2025-01-13 14:57:52,990 - INFO - Epoch 10/20 - Model, Train Loss: 0.1925, Val Loss: 0.0697\n",
      " 50%|█████     | 10/20 [00:04<00:04,  2.19it/s]2025-01-13 14:57:53,442 - INFO - Epoch 11/20 - Model, Train Loss: 0.1898, Val Loss: 0.0679\n",
      " 55%|█████▌    | 11/20 [00:05<00:04,  2.20it/s]2025-01-13 14:57:53,878 - INFO - Epoch 12/20 - Model, Train Loss: 0.1652, Val Loss: 0.0617\n",
      " 60%|██████    | 12/20 [00:05<00:03,  2.23it/s]2025-01-13 14:57:54,348 - INFO - Epoch 13/20 - Model, Train Loss: 0.1526, Val Loss: 0.0529\n",
      " 65%|██████▌   | 13/20 [00:05<00:03,  2.17it/s]2025-01-13 14:57:54,820 - INFO - Epoch 14/20 - Model, Train Loss: 0.1340, Val Loss: 0.0537\n",
      " 70%|███████   | 14/20 [00:06<00:02,  2.18it/s]2025-01-13 14:57:55,277 - INFO - Epoch 15/20 - Model, Train Loss: 0.1311, Val Loss: 0.0518\n",
      " 75%|███████▌  | 15/20 [00:06<00:02,  2.17it/s]2025-01-13 14:57:55,767 - INFO - Epoch 16/20 - Model, Train Loss: 0.1486, Val Loss: 0.0528\n",
      " 80%|████████  | 16/20 [00:07<00:01,  2.14it/s]2025-01-13 14:57:56,274 - INFO - Epoch 17/20 - Model, Train Loss: 0.1238, Val Loss: 0.0496\n",
      " 85%|████████▌ | 17/20 [00:07<00:01,  2.06it/s]2025-01-13 14:57:56,850 - INFO - Epoch 18/20 - Model, Train Loss: 0.1388, Val Loss: 0.0511\n",
      " 90%|█████████ | 18/20 [00:08<00:01,  1.98it/s]2025-01-13 14:57:57,289 - INFO - Epoch 19/20 - Model, Train Loss: 0.1384, Val Loss: 0.0476\n",
      " 95%|█████████▌| 19/20 [00:08<00:00,  2.05it/s]2025-01-13 14:57:57,713 - INFO - Epoch 20/20 - Model, Train Loss: 0.1016, Val Loss: 0.0465\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.15it/s]\n",
      "2025-01-13 14:57:57,793 - INFO - Collecting results...\n",
      "2025-01-13 14:57:57,801 - INFO - Results saved to combined_metrics.csv.\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the models\n",
    "logging.info(\"Starting training and evaluation...\")\n",
    "pipeline.train_and_evaluate()\n",
    "\n",
    "# Retrieve results\n",
    "logging.info(\"Collecting results...\")\n",
    "results_df = pipeline.get_results()\n",
    "\n",
    "# Save the results\n",
    "results_df.to_csv(\"combined_metrics.csv\", index=False)\n",
    "logging.info(\"Results saved to combined_metrics.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_06dfb_row1_col0, #T_06dfb_row1_col1, #T_06dfb_row1_col2, #T_06dfb_row1_col3 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_06dfb\">\n",
       "  <caption>Regression Model Evaluation Metrics</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_06dfb_level0_col0\" class=\"col_heading level0 col0\" >MSE</th>\n",
       "      <th id=\"T_06dfb_level0_col1\" class=\"col_heading level0 col1\" >MAE</th>\n",
       "      <th id=\"T_06dfb_level0_col2\" class=\"col_heading level0 col2\" >R²</th>\n",
       "      <th id=\"T_06dfb_level0_col3\" class=\"col_heading level0 col3\" >Pearson Correlation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Feature Set</th>\n",
       "      <th class=\"index_name level1\" >Model Name</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_06dfb_level0_row0\" class=\"row_heading level0 row0\" >Best Inferred Data</th>\n",
       "      <th id=\"T_06dfb_level1_row0\" class=\"row_heading level1 row0\" >FCNN_Model</th>\n",
       "      <td id=\"T_06dfb_row0_col0\" class=\"data row0 col0\" >0.044</td>\n",
       "      <td id=\"T_06dfb_row0_col1\" class=\"data row0 col1\" >0.154</td>\n",
       "      <td id=\"T_06dfb_row0_col2\" class=\"data row0 col2\" >0.314</td>\n",
       "      <td id=\"T_06dfb_row0_col3\" class=\"data row0 col3\" >0.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06dfb_level0_row1\" class=\"row_heading level0 row1\" >Gene Data</th>\n",
       "      <th id=\"T_06dfb_level1_row1\" class=\"row_heading level1 row1\" >FCNN_Model</th>\n",
       "      <td id=\"T_06dfb_row1_col0\" class=\"data row1 col0\" >0.036</td>\n",
       "      <td id=\"T_06dfb_row1_col1\" class=\"data row1 col1\" >0.147</td>\n",
       "      <td id=\"T_06dfb_row1_col2\" class=\"data row1 col2\" >0.443</td>\n",
       "      <td id=\"T_06dfb_row1_col3\" class=\"data row1 col3\" >0.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06dfb_level0_row2\" class=\"row_heading level0 row2\" >Landmark Data</th>\n",
       "      <th id=\"T_06dfb_level1_row2\" class=\"row_heading level1 row2\" >FCNN_Model</th>\n",
       "      <td id=\"T_06dfb_row2_col0\" class=\"data row2 col0\" >0.039</td>\n",
       "      <td id=\"T_06dfb_row2_col1\" class=\"data row2 col1\" >0.148</td>\n",
       "      <td id=\"T_06dfb_row2_col2\" class=\"data row2 col2\" >0.401</td>\n",
       "      <td id=\"T_06dfb_row2_col3\" class=\"data row2 col3\" >0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06dfb_level0_row3\" class=\"row_heading level0 row3\" >TF Data</th>\n",
       "      <th id=\"T_06dfb_level1_row3\" class=\"row_heading level1 row3\" >FCNN_Model</th>\n",
       "      <td id=\"T_06dfb_row3_col0\" class=\"data row3 col0\" >0.050</td>\n",
       "      <td id=\"T_06dfb_row3_col1\" class=\"data row3 col1\" >0.174</td>\n",
       "      <td id=\"T_06dfb_row3_col2\" class=\"data row3 col2\" >0.233</td>\n",
       "      <td id=\"T_06dfb_row3_col3\" class=\"data row3 col3\" >0.495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1fa0ee68fd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "styled_results = (\n",
    "    results_df.style.format(precision=3)\n",
    "    .set_caption(\"Regression Model Evaluation Metrics\")\n",
    "    .highlight_max(\n",
    "        subset=[\"R²\", \"Pearson Correlation\"], color=\"lightgreen\"\n",
    "    )\n",
    "    .highlight_min(subset=[\"MAE\", \"MSE\"], color=\"lightgreen\")\n",
    ")\n",
    "styled_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 14:38:28,424 - INFO - Downloading data from `https://omnipathdb.org/queries/enzsub?format=json`\n",
      "2024-12-19 14:38:31,493 - WARNING - Failed to download from `https://omnipathdb.org/`.\n",
      "2024-12-19 14:38:31,501 - WARNING - Traceback (most recent call last):\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\connectionpool.py\", line 466, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1095, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\connection.py\", line 730, in connect\n",
      "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\connection.py\", line 909, in _ssl_wrap_socket_and_match_hostname\n",
      "    ssl_sock = ssl_wrap_socket(\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 469, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 513, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\ssl.py\", line 513, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\ssl.py\", line 1104, in _create\n",
      "    self.do_handshake()\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\ssl.py\", line 1375, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\connectionpool.py\", line 490, in _make_request\n",
      "    raise new_e\n",
      "urllib3.exceptions.SSLError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\requests\\adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\urllib3\\util\\retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='omnipathdb.org', port=443): Max retries exceeded with url: /queries/enzsub?format=json (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\omnipath\\_core\\downloader\\_downloader.py\", line 143, in maybe_download\n",
      "    res = self._download(req)\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\omnipath\\_core\\downloader\\_downloader.py\", line 178, in _download\n",
      "    with self._session.send(\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\requests\\adapters.py\", line 698, in send\n",
      "    raise SSLError(e, request=request)\n",
      "requests.exceptions.SSLError: HTTPSConnectionPool(host='omnipathdb.org', port=443): Max retries exceeded with url: /queries/enzsub?format=json (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "\n",
      "2024-12-19 14:38:31,503 - INFO - Downloading data from `http://no-tls.omnipathdb.org/queries/enzsub?format=json`\n",
      "2024-12-19 14:38:31,535 - INFO - Downloading data from `https://omnipathdb.org/queries/interactions?format=json`\n",
      "2024-12-19 14:38:31,586 - INFO - Downloading data from `https://omnipathdb.org/queries/complexes?format=json`\n",
      "2024-12-19 14:38:31,632 - INFO - Downloading data from `https://omnipathdb.org/queries/annotations?format=json`\n",
      "2024-12-19 14:38:31,678 - INFO - Downloading data from `https://omnipathdb.org/queries/intercell?format=json`\n",
      "2024-12-19 14:38:31,851 - INFO - Downloading data from `https://omnipathdb.org/about?format=text`\n",
      "2024-12-19 14:38:44,313 - INFO - Filtered dataset shape: (19416, 5576)\n",
      "2024-12-19 14:38:44,719 - INFO - Filtered dataset shape: (5379, 5576)\n",
      "2024-12-19 14:38:45,000 - INFO - Filtered dataset shape: (6364, 5576)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5576, 1186])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import decoupler as dc\n",
    "\n",
    "\n",
    "def create_gene_tf_matrix(\n",
    "    net: pd.DataFrame, genes: list = None, tfs: list = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates a PyTorch tensor representing the gene-TF regulatory matrix from a network dataframe.\n",
    "\n",
    "    Args:\n",
    "        net (pd.DataFrame): DataFrame containing the regulatory network with the following columns:\n",
    "            - \"source\": Transcription factors (TFs).\n",
    "            - \"target\": Genes regulated by the TFs.\n",
    "            - \"weight\": Interaction weight (1 for activation, -1 for inhibition).\n",
    "        genes (list, optional): List of genes to include in the matrix. If None, all unique genes in `net` are used.\n",
    "        tfs (list, optional): List of TFs to include in the matrix. If None, all unique TFs in `net` are used.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (num_genes, num_tfs) where:\n",
    "            - `1` indicates an activating interaction.\n",
    "            - `-1` indicates an inhibiting interaction.\n",
    "            - `0` indicates no interaction.\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    required_columns = {\"source\", \"target\", \"weight\"}\n",
    "    if not required_columns.issubset(net.columns):\n",
    "        raise ValueError(\n",
    "            f\"The `net` dataframe must contain the columns: {required_columns}\"\n",
    "        )\n",
    "\n",
    "    # Use all unique genes and TFs if not provided\n",
    "    available_genes = sorted(net[\"target\"].unique())\n",
    "    available_tfs = sorted(net[\"source\"].unique())\n",
    "\n",
    "    if genes is None:\n",
    "        genes = available_genes\n",
    "    else:\n",
    "        # Filter out genes not in the network\n",
    "        genes = [gene for gene in genes if gene in available_genes]\n",
    "\n",
    "    if tfs is None:\n",
    "        tfs = available_tfs\n",
    "    else:\n",
    "        # Filter out TFs not in the network\n",
    "        tfs = [tf for tf in tfs if tf in available_tfs]\n",
    "\n",
    "    # Initialize a DataFrame with zeros (default for no interaction)\n",
    "    gene_to_tf_df = pd.DataFrame(0, index=genes, columns=tfs, dtype=float)\n",
    "\n",
    "    # Populate the DataFrame with interaction weights\n",
    "    for _, row in net.iterrows():\n",
    "        gene = row[\"target\"]\n",
    "        tf = row[\"source\"]\n",
    "        weight = row[\"weight\"]\n",
    "        if gene in genes and tf in tfs:\n",
    "            gene_to_tf_df.at[gene, tf] = weight\n",
    "\n",
    "    # Convert the DataFrame to a PyTorch tensor\n",
    "    gene_to_tf_matrix = torch.tensor(gene_to_tf_df.values, dtype=torch.float32)\n",
    "    return gene_to_tf_matrix\n",
    "\n",
    "\n",
    "def filter_genes_to_collectri(dataset: pd.DataFrame, net: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the dataset to include only genes present in the Collectri network.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): Gene expression dataset (rows = samples, columns = genes).\n",
    "        net (pd.DataFrame): Regulatory network dataframe with a \"target\" column containing gene names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered dataset containing only Collectri genes.\n",
    "    \"\"\"\n",
    "    # Extract unique genes from the \"target\" column of the net dataframe\n",
    "    collectri_genes = net[\"target\"].unique()\n",
    "\n",
    "    # Find the intersection of dataset columns and Collectri genes\n",
    "    intersecting_genes = set(dataset.columns).intersection(collectri_genes)\n",
    "\n",
    "    # Filter the dataset to include only the intersecting genes\n",
    "    filtered_dataset = dataset[list(intersecting_genes)]\n",
    "\n",
    "    logging.info(f\"Filtered dataset shape: {filtered_dataset.shape}\")\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net = dc.get_collectri(organism='human', split_complexes=False)\n",
    "\n",
    "# Filter gene datasets to include only Collectri genes\n",
    "X_gene_train = filter_genes_to_collectri(X_gene_train, net)\n",
    "X_gene_val = filter_genes_to_collectri(X_gene_val, net)\n",
    "X_gene_test = filter_genes_to_collectri(X_gene_test, net)\n",
    "\n",
    "# Create new dataloaders\n",
    "gene_train_loader = create_dataloader(X_gene_train, y_gene_train)\n",
    "gene_val_loader = create_dataloader(X_gene_val, y_gene_val)\n",
    "gene_test_loader = create_dataloader(X_gene_test, y_gene_test)\n",
    "\n",
    "# Create gene-TF matrix\n",
    "gene_tf_matrix = create_gene_tf_matrix(net, X_gene_train.columns)\n",
    "gene_tf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model configurations\n",
    "from models import SparseKnowledgeNetwork\n",
    "\n",
    "\n",
    "model_configs = {\n",
    "    \"SparseKnowledgeNetwork_Model\": {\n",
    "        \"model_class\": SparseKnowledgeNetwork,\n",
    "        \"model_params\": {\n",
    "            \"gene_tf_matrix\": None,  # To be dynamically set\n",
    "            \"hidden_dims\": [512, 256, 128, 64],\n",
    "            \"output_dim\": 1,\n",
    "            \"first_activation\": \"tanh\",  # Matches biological interpretation of TF interactions\n",
    "            \"downstream_activation\": \"prelu\",\n",
    "            \"dropout_prob\": 0.2,\n",
    "            \"residual\": True,\n",
    "            \"norm_type\": \"batchnorm\",\n",
    "            \"weight_init\": \"xavier\",\n",
    "        },\n",
    "        \"criterion\": nn.MSELoss(),\n",
    "        \"optimizer_class\": optim.AdamW,\n",
    "        \"optimizer_params\": {\"lr\": 0.001, \"weight_decay\": 1e-4},\n",
    "        \"scheduler_class\": ReduceLROnPlateau,\n",
    "        \"scheduler_params\": {\"mode\": \"min\", \"patience\": 5},\n",
    "        \"train_params\": {\n",
    "            \"epochs\": 20,\n",
    "            \"gradient_clipping\": 1.0,\n",
    "            \"early_stopping_patience\": 10,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5ARG45",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
