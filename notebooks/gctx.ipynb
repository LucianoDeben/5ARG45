{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Step 1: Load the data matrix and metadata from the .gctx file\n",
    "# gctx_file = \"../data/raw/level3_beta_ctl_n188708x12328.gctx\"\n",
    "# h5_file = \"../data/raw/compound_control_dataset_all.h5\"\n",
    "\n",
    "# chunk_size = 10000  # Number of rows to process at a time\n",
    "\n",
    "# with h5py.File(gctx_file, \"r\") as f:\n",
    "#     # Access data matrix and metadata\n",
    "#     data = f[\"/0/DATA/0/matrix\"]\n",
    "#     row_ids = f[\"/0/META/COL/id\"][:].astype(str)  # All sample IDs (rows)\n",
    "#     col_ids = f[\"/0/META/ROW/id\"][:].astype(int)  # All gene IDs (columns)\n",
    "\n",
    "#     # Load metadata files\n",
    "#     gene_metadata = pd.read_csv(\"../data/raw/geneinfo_beta.txt\", sep=\"\\t\")\n",
    "#     pert_info = pd.read_csv(\"../data/raw/instinfo_beta.txt\", sep=\"\\t\")\n",
    "\n",
    "#     # Filter perturbation metadata\n",
    "#     filtered_pert_info = pert_info[\n",
    "#         (pert_info[\"qc_pass\"] == 1.0) & (pert_info[\"pert_type\"] == \"ctl_vehicle\")\n",
    "#     ].copy()\n",
    "#     filtered_pert_info.set_index(\"sample_id\", inplace=True)\n",
    "\n",
    "#     # Filter valid sample IDs\n",
    "#     valid_sample_ids = pd.Index(row_ids).intersection(filtered_pert_info.index)\n",
    "#     filtered_pert_info = filtered_pert_info.loc[valid_sample_ids]\n",
    "#     filtered_pert_info.sort_index(inplace=True)\n",
    "\n",
    "#     # Create a mapping of sample IDs to row indices\n",
    "#     row_idx_map = {rid: idx for idx, rid in enumerate(row_ids)}\n",
    "\n",
    "#     # Compute row indices for valid samples\n",
    "#     filtered_row_indices = [row_idx_map[sid] for sid in valid_sample_ids]\n",
    "\n",
    "# # Step 2: Write the filtered data matrix and metadata in chunks\n",
    "# with h5py.File(h5_file, \"w\") as h5f:\n",
    "#     # Pre-allocate space for the dataset in the HDF5 file\n",
    "#     data_shape = (len(filtered_row_indices), len(col_ids))\n",
    "#     data_dset = h5f.create_dataset(\n",
    "#         \"data\", shape=data_shape, dtype=\"float32\", compression=\"gzip\"\n",
    "#     )\n",
    "\n",
    "#     # Write row (sample) metadata\n",
    "#     for column in filtered_pert_info.columns:\n",
    "#         h5f.create_dataset(\n",
    "#             f\"row_metadata/{column}\",\n",
    "#             data=filtered_pert_info[column].values.astype(\"S\"),\n",
    "#         )\n",
    "\n",
    "#     # Write column (gene) metadata\n",
    "#     for column in gene_metadata.columns:\n",
    "#         h5f.create_dataset(\n",
    "#             f\"col_metadata/{column}\",\n",
    "#             data=gene_metadata[column].values.astype(\"S\"),\n",
    "#         )\n",
    "\n",
    "#     # Write row and column IDs\n",
    "#     h5f.create_dataset(\"row_ids\", data=valid_sample_ids.values.astype(\"S\"))\n",
    "#     gene_mapping = gene_metadata.set_index(\"gene_id\")[\"gene_symbol\"].to_dict()\n",
    "#     filtered_col_symbols = [gene_mapping.get(gid, f\"Gene_{gid}\") for gid in col_ids]\n",
    "#     h5f.create_dataset(\n",
    "#         \"col_ids\", data=[x.encode(\"utf-8\") for x in filtered_col_symbols]\n",
    "#     )\n",
    "\n",
    "#     # Process and write data in chunks\n",
    "#     for i in range(0, len(filtered_row_indices), chunk_size):\n",
    "#         chunk_indices = filtered_row_indices[i : i + chunk_size]  # Current chunk\n",
    "#         chunk_data = data[chunk_indices, :]  # Load chunk from the data matrix\n",
    "#         data_dset[i : i + len(chunk_indices), :] = chunk_data  # Write to dataset\n",
    "\n",
    "# print(f\"Filtered data and metadata successfully written to {h5_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# import pandas as pd\n",
    "\n",
    "# # Step 1: Load the data matrix and metadata from the .gctx file\n",
    "# gctx_file = \"../data/raw/level3_beta_ctl_n188708x12328.gctx\"\n",
    "\n",
    "# with h5py.File(gctx_file, \"r\") as f:\n",
    "#     # Access the entire data matrix and metadata\n",
    "#     data = f[\"/0/DATA/0/matrix\"]\n",
    "#     row_ids = f[\"/0/META/COL/id\"][:].astype(str)  # Sample IDs (rows)\n",
    "#     col_ids = f[\"/0/META/ROW/id\"][:].astype(int)  # Gene IDs (columns)\n",
    "\n",
    "#     # Load metadata files\n",
    "#     gene_metadata = pd.read_csv(\"../data/raw/geneinfo_beta.txt\", sep=\"\\t\")\n",
    "#     pert_info = pd.read_csv(\"../data/raw/instinfo_beta.txt\", sep=\"\\t\")\n",
    "\n",
    "#     # Filter perturbation metadata\n",
    "#     filtered_pert_info = pert_info[\n",
    "#         (pert_info[\"qc_pass\"] == 1.0) & (pert_info[\"pert_type\"] == \"ctl_vehicle\")\n",
    "#     ].copy()\n",
    "#     filtered_pert_info.set_index(\"sample_id\", inplace=True)\n",
    "\n",
    "#     # STEP A: Reduce filtered_pert_info to match row_ids\n",
    "#     subset_row_ids = pd.Index(row_ids)\n",
    "#     subset_row_ids = subset_row_ids.intersection(filtered_pert_info.index)\n",
    "#     subset_pert_info = filtered_pert_info.loc[subset_row_ids].copy()\n",
    "#     subset_pert_info.sort_index(inplace=True)\n",
    "\n",
    "#     # Keep a separate column for sample_id (while retaining it as index)\n",
    "#     subset_pert_info[\"sample_id\"] = subset_pert_info.index\n",
    "\n",
    "#     # STEP B: Recompute the row indices in the data matrix for valid sample_ids\n",
    "#     row_idx_map = {rid: idx for idx, rid in enumerate(row_ids)}\n",
    "#     filtered_row_indices = [row_idx_map[sid] for sid in subset_pert_info.index]\n",
    "\n",
    "#     # Filter the data matrix to match the final subset of row indices\n",
    "#     filtered_data_matrix = data[filtered_row_indices, :]\n",
    "\n",
    "# # Step 2: Map gene IDs to gene symbols\n",
    "# gene_mapping = gene_metadata.set_index(\"gene_id\")[\"gene_symbol\"].to_dict()\n",
    "# filtered_col_symbols = [gene_mapping.get(gid, f\"Gene_{gid}\") for gid in col_ids]\n",
    "\n",
    "# # Step 3: Write the filtered data and metadata to an HDF5 file\n",
    "# h5_file = \"../data/raw/compound_control_dataset.h5\"\n",
    "\n",
    "# with h5py.File(h5_file, \"w\") as h5f:\n",
    "#     # Write the filtered data matrix\n",
    "#     h5f.create_dataset(\"data\", data=filtered_data_matrix, compression=\"gzip\")\n",
    "\n",
    "#     # Write row (sample) metadata\n",
    "#     for column in subset_pert_info.columns:\n",
    "#         h5f.create_dataset(\n",
    "#             f\"row_metadata/{column}\",\n",
    "#             data=subset_pert_info[column].values.astype(\"S\"),\n",
    "#         )\n",
    "\n",
    "#     # Write column (gene) metadata\n",
    "#     for column in gene_metadata.columns:\n",
    "#         h5f.create_dataset(\n",
    "#             f\"col_metadata/{column}\",\n",
    "#             data=gene_metadata[column].values.astype(\"S\"),\n",
    "#         )\n",
    "\n",
    "#     # Write final row and column IDs\n",
    "#     h5f.create_dataset(\"row_ids\", data=subset_pert_info.index.values.astype(\"S\"))\n",
    "#     h5f.create_dataset(\n",
    "#         \"col_ids\", data=[x.encode(\"utf-8\") for x in filtered_col_symbols]\n",
    "#     )\n",
    "\n",
    "# print(f\"Filtered data and metadata successfully written to {h5_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5_file = \"../data/raw/compound_control_dataset.h5\"\n",
    "# with h5py.File(h5_file, \"r\") as h5f:\n",
    "#     # Load the data matrix\n",
    "#     data_matrix = h5f[\"data\"][:]\n",
    "\n",
    "#     # Decode row metadata\n",
    "#     row_metadata = {\n",
    "#         key: h5f[f\"row_metadata/{key}\"][:].astype(str) for key in h5f[\"row_metadata\"]\n",
    "#     }\n",
    "\n",
    "#     # Decode column metadata\n",
    "#     col_metadata = {\n",
    "#         key: h5f[f\"col_metadata/{key}\"][:].astype(str) for key in h5f[\"col_metadata\"]\n",
    "#     }\n",
    "\n",
    "#     # Load and decode row and column IDs\n",
    "#     row_ids = h5f[\"row_ids\"][:].astype(str)\n",
    "#     col_ids = h5f[\"col_ids\"][:].astype(str)\n",
    "\n",
    "# # Convert metadata to DataFrames for ease of use (optional)\n",
    "# import pandas as pd\n",
    "\n",
    "# row_metadata_df = pd.DataFrame(row_metadata)\n",
    "# col_metadata_df = pd.DataFrame(col_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Dataset has 9970 (control, pert) pairs.\n",
      "Train Dataset size: 7976\n",
      "Val Dataset size: 1994\n"
     ]
    }
   ],
   "source": [
    "# Add src to path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"src\")))\n",
    "\n",
    "from torch.utils.data import  DataLoader, random_split\n",
    "from data_sets import PerturbationDataset\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Create the dataset as you showed\n",
    "controls = \"../data/raw/compound_control_dataset.h5\"\n",
    "perturbations = \"../data/raw/compound_pertubation_dataset.h5\"\n",
    "\n",
    "# Load the data\n",
    "smiles_df = pd.read_csv(\"../data/raw/compoundinfo_beta.txt\", sep=\"\\t\")\n",
    "\n",
    "# Ensure no leading/trailing spaces\n",
    "smiles_df[\"pert_id\"] = smiles_df[\"pert_id\"].str.strip()\n",
    "smiles_df[\"canonical_smiles\"] = smiles_df[\"canonical_smiles\"].str.strip()\n",
    "\n",
    "# Remove duplicates, keeping the first occurrence\n",
    "smiles_df = smiles_df.drop_duplicates(subset=\"pert_id\", keep=\"first\")\n",
    "\n",
    "# Check for missing values and handle them\n",
    "if smiles_df[\"pert_id\"].isnull().any() or smiles_df[\"canonical_smiles\"].isnull().any():\n",
    "    # Option 1: Drop rows with missing values\n",
    "    # smiles_df = smiles_df.dropna(subset=[\"pert_id\", \"canonical_smiles\"])\n",
    "\n",
    "    # Option 2: Fill missing values with a placeholder (e.g., 'Unknown')\n",
    "    smiles_df[\"canonical_smiles\"] = smiles_df[\"canonical_smiles\"].fillna(\"Unknown\")\n",
    "\n",
    "# Create the mapping dictionary\n",
    "smiles_dict = dict(zip(smiles_df[\"pert_id\"], smiles_df[\"canonical_smiles\"]))\n",
    "\n",
    "dataset = PerturbationDataset(\n",
    "    controls_file=controls,\n",
    "    perturbations_file=perturbations,\n",
    "    smiles_dict=smiles_dict,\n",
    "    plate_column=\"det_plate\",\n",
    "    normalize=True,\n",
    "    n_rows=10000,\n",
    "    pairing=\"random\",\n",
    ")\n",
    "\n",
    "print(f\"Full Dataset has {len(dataset)} (control, pert) pairs.\")\n",
    "\n",
    "# 2) Decide how you want to split\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.2\n",
    "\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# 3) Use random_split to partition into two subsets\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Train Dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val Dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Shape: torch.Size([32, 12328])\n",
      "Labels Shape: torch.Size([32, 12328])\n",
      "Metadata sample: {'bead_batch': ['b32', 'b34', 'b34', 'b32', 'b32', 'b32', 'b32', 'b34', 'b15', 'b32', 'b34', 'b32', 'b15', 'b32', 'b32', 'b15', 'b15', 'b32', 'b32', 'b32', 'b15', 'b34', 'b32', 'b32', 'b34', 'b32', 'b32', 'b32', 'b32', 'b15', 'b32', 'b34'], 'build_name': ['nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan'], 'cell_iname': ['TMD8', 'NALM6', 'SKMEL5', 'OCILY3', 'BJAB', 'THP1', 'OCILY3', 'SKMEL5', 'NCIH508', 'THP1', 'KMS34', 'K562', 'PC3', 'BJAB', 'BJAB', 'A549', 'HT29', 'OCILY19', 'BJAB', 'THP1', 'A375', 'SKMEL5', 'OCILY19', 'TMD8', 'KMS34', 'OCILY3', 'OCILY19', 'TMD8', 'TMD8', 'H1975', 'BJAB', 'NALM6'], 'cell_mfc_name': ['TMD8', 'NALM6', 'SKMEL5', 'OCILY3', 'BJAB', 'THP1', 'OCILY3', 'SKMEL5', 'NCIH508', 'THP1', 'KMS34', 'K562', 'PC3', 'BJAB', 'BJAB', 'A549', 'HT29', 'OCILY19', 'BJAB', 'THP1', 'A375', 'SKMEL5', 'OCILY19', 'TMD8', 'KMS34', 'OCILY3', 'OCILY19', 'TMD8', 'TMD8', 'H1975', 'BJAB', 'NALM6'], 'cmap_name': ['DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO'], 'count_cv': ['20', '16', '15', '18', '22', '21', '20', '15', '22', '17', '16', '24', '21', '20', '19', '19', '20', '23', '20', '22', '19', '15', '23', '22', '17', '17', '26', '19', '22', '20', '19', '16'], 'count_mean': ['95', '67', '85', '104', '96', '122', '99', '86', '47', '121', '87', '95', '55', '114', '115', '71', '55', '106', '118', '100', '62', '86', '92', '77', '81', '124', '79', '96', '83', '63', '119', '91'], 'det_plate': ['AICHI001_TMD8_24H_X2_B32', 'AICHI001_NALM6_24H_X1_B34', 'AICHI001_SKMEL5_24H_X2_B34', 'AICHI001_OCILY3_24H_X1_B32', 'AICHI001_BJAB_24H_X1_B32', 'AICHI001_THP1_24H_X1_B32', 'AICHI001_OCILY3_24H_X1_B32', 'AICHI001_SKMEL5_24H_X3_B34', 'ABY001_NCIH508_XH_X1_B15', 'AICHI001_THP1_24H_X3_B32', 'AICHI001_KMS34_24H_X1_B34', 'AICHI001_K562_24H_X2_B32', 'ABY001_PC3_XH_X1_B15', 'AICHI002_BJAB_24H_X2_B32', 'AICHI002_BJAB_24H_X2_B32', 'ABY001_A549_XH_X1_B15', 'ABY001_HT29_XH_X1_B15', 'AICHI001_OCILY19_4H_X1_B32', 'AICHI002_BJAB_24H_X2_B32', 'AICHI001_THP1_24H_X1_B32', 'ABY001_A375_XH_X1_B15', 'AICHI001_SKMEL5_24H_X3_B34', 'AICHI001_OCILY19_4H_X1_B32', 'AICHI001_TMD8_24H_X2_B32', 'AICHI001_KMS34_24H_X2_B34', 'AICHI001_OCILY3_24H_X2_B32', 'AICHI001_OCILY19_24H_X1_B32', 'AICHI001_TMD8_4H_X1_B32', 'AICHI001_TMD8_24H_X1_B32', 'ABY001_H1975_XH_X1_B15', 'AICHI002_BJAB_24H_X2_B32', 'AICHI001_NALM6_24H_X2_B34'], 'det_well': ['P22', 'L20', 'F09', 'A05', 'N20', 'A04', 'B03', 'N21', 'A08', 'J15', 'N23', 'N24', 'A07', 'O19', 'J18', 'A05', 'A05', 'N22', 'P24', 'L21', 'A04', 'N23', 'J18', 'L21', 'P21', 'A04', 'A06', 'F07', 'F12', 'O04', 'B03', 'B04'], 'dyn_range': ['24.4831', '17.5701', '13.2308', '28.9133', '23.6181', '22.1676', '28.2969', '18.4009', '11.8971', '23.9722', '21.5182', '26.559', '14.5786', '30.609', '30.3114', '15.8525', '12.1435', '19.5824', '27.3951', '20.5378', '14.1181', '17.95', '25.1726', '16.6892', '23.1879', '30.3567', '23.0202', '28.0069', '27.6718', '10.0838', '20.0653', '17.7543'], 'failure_mode': ['nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan'], 'inv_level_10': ['4333.5', '2881.5', '3440.0', '4337.0', '3401.0', '3968.0', '4527.5', '2042.5', '4104.5', '3883.5', '2948.0', '4276.0', '3615.5', '4071.0', '5062.0', '2901.0', '2793.0', '3564.0', '4438.0', '3799.5', '3826.0', '2872.0', '4229.0', '4322.5', '3455.0', '4553.5', '5122.0', '4033.0', '3625.0', '2647.0', '3993.0', '2059.5'], 'nearest_dose': ['nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan'], 'pert_dose': ['nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan'], 'pert_dose_unit': ['nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan'], 'pert_id': ['DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO'], 'pert_idose': ['nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan'], 'pert_itime': ['24 h', '24 h', '24 h', '24 h', '24 h', '24 h', '24 h', '24 h', '3 h', '24 h', '24 h', '24 h', '3 h', '24 h', '24 h', '3 h', '3 h', '4 h', '24 h', '24 h', '3 h', '24 h', '4 h', '24 h', '24 h', '24 h', '24 h', '4 h', '24 h', '3 h', '24 h', '24 h'], 'pert_mfc_id': ['DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO', 'DMSO'], 'pert_time': ['24.0', '24.0', '24.0', '24.0', '24.0', '24.0', '24.0', '24.0', '3.0', '24.0', '24.0', '24.0', '3.0', '24.0', '24.0', '3.0', '3.0', '4.0', '24.0', '24.0', '3.0', '24.0', '4.0', '24.0', '24.0', '24.0', '24.0', '4.0', '24.0', '3.0', '24.0', '24.0'], 'pert_time_unit': ['h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h'], 'pert_type': ['ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle', 'ctl_vehicle'], 'project_code': ['AICHI', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'ABY', 'AICHI', 'AICHI', 'AICHI', 'ABY', 'AICHI', 'AICHI', 'ABY', 'ABY', 'AICHI', 'AICHI', 'AICHI', 'ABY', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'ABY', 'AICHI', 'AICHI'], 'qc_f_logp': ['5.2', '6.2', '6.9', '5.5', '6.6', '6.1', '5.2', '7.1', '7.0', '6.4', '6.4', '6.1', '7.8', '6.0', '6.0', '7.9', '6.4', '5.1', '5.8', '6.1', '7.1', '7.7', '4.9', '5.8', '6.6', '5.3', '5.3', '5.3', '5.3', '5.3', '5.3', '6.3'], 'qc_iqr': ['11.68', '12.44', '9.56', '11.76', '11.36', '8.53', '12.97', '10.09', '8.45', '9.52', '10.77', '9.69', '7.19', '11.29', '12.88', '8.43', '8.94', '11.0', '11.9', '9.62', '9.11', '8.69', '10.87', '11.22', '11.41', '12.29', '10.33', '13.47', '12.29', '7.89', '12.02', '14.04'], 'qc_pass': ['1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0'], 'qc_slope': ['57', '57', '58', '56', '55', '54', '56', '56', '61', '56', '56', '55', '59', '54', '54', '60', '61', '55', '55', '57', '61', '54', '56', '57', '55', '56', '56', '55', '56', '64', '57', '56'], 'rna_plate': ['AICHI001_TMD8_24H_X2', 'AICHI001_NALM6_24H_X1', 'AICHI001_SKMEL5_24H_X2', 'AICHI001_OCILY3_24H_X1', 'AICHI001_BJAB_24H_X1', 'AICHI001_THP1_24H_X1', 'AICHI001_OCILY3_24H_X1', 'AICHI001_SKMEL5_24H_X3', 'ABY001_NCIH508_XH_X1', 'AICHI001_THP1_24H_X3', 'AICHI001_KMS34_24H_X1', 'AICHI001_K562_24H_X2', 'ABY001_PC3_XH_X1', 'AICHI002_BJAB_24H_X2', 'AICHI002_BJAB_24H_X2', 'ABY001_A549_XH_X1', 'ABY001_HT29_XH_X1', 'AICHI001_OCILY19_4H_X1', 'AICHI002_BJAB_24H_X2', 'AICHI001_THP1_24H_X1', 'ABY001_A375_XH_X1', 'AICHI001_SKMEL5_24H_X3', 'AICHI001_OCILY19_4H_X1', 'AICHI001_TMD8_24H_X2', 'AICHI001_KMS34_24H_X2', 'AICHI001_OCILY3_24H_X2', 'AICHI001_OCILY19_24H_X1', 'AICHI001_TMD8_4H_X1', 'AICHI001_TMD8_24H_X1', 'ABY001_H1975_XH_X1', 'AICHI002_BJAB_24H_X2', 'AICHI001_NALM6_24H_X2'], 'rna_well': ['P22', 'L20', 'F09', 'A05', 'N20', 'A04', 'B03', 'N21', 'A08', 'J15', 'N23', 'N24', 'A07', 'O19', 'J18', 'A05', 'A05', 'N22', 'P24', 'L21', 'A04', 'N23', 'J18', 'L21', 'P21', 'A04', 'A06', 'F07', 'F12', 'O04', 'B03', 'B04']} {'bead_batch': ['b32', 'b34', 'b34', 'b32', 'b32', 'b32', 'b32', 'b34', 'b15', 'b32', 'b34', 'b32', 'b15', 'b32', 'b32', 'b15', 'b15', 'b32', 'b32', 'b32', 'b15', 'b34', 'b32', 'b32', 'b34', 'b32', 'b32', 'b32', 'b32', 'b15', 'b32', 'b34'], 'build_name': ['nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan'], 'cell_iname': ['TMD8', 'NALM6', 'SKMEL5', 'OCILY3', 'BJAB', 'THP1', 'OCILY3', 'SKMEL5', 'NCIH508', 'THP1', 'KMS34', 'K562', 'PC3', 'BJAB', 'BJAB', 'A549', 'HT29', 'OCILY19', 'BJAB', 'THP1', 'A375', 'SKMEL5', 'OCILY19', 'TMD8', 'KMS34', 'OCILY3', 'OCILY19', 'TMD8', 'TMD8', 'H1975', 'BJAB', 'NALM6'], 'cell_mfc_name': ['TMD8', 'NALM6', 'SKMEL5', 'OCILY3', 'BJAB', 'THP1', 'OCILY3', 'SKMEL5', 'NCIH508', 'THP1', 'KMS34', 'K562', 'PC3', 'BJAB', 'BJAB', 'A549', 'HT29', 'OCILY19', 'BJAB', 'THP1', 'A375', 'SKMEL5', 'OCILY19', 'TMD8', 'KMS34', 'OCILY3', 'OCILY19', 'TMD8', 'TMD8', 'H1975', 'BJAB', 'NALM6'], 'cmap_name': ['ruxolitinib', 'BRD-A04845303', 'IPI-145', 'BRD-K16277767', 'sotrastaurin', 'sotrastaurin', 'BI-D1870', 'BRD-A04845303', 'HMN-214', 'pazopanib', 'WZ-4002', 'MI-2', 'lapatinib', 'GSK-1838705A', 'CINK-4', 'afatinib', 'pazopanib', 'trametinib', 'selumetinib', 'LJH-685', 'fulvestrant', 'barasertib-HQPA', 'AZ-628', 'pazopanib', 'A-443654', 'pazopanib', 'BRD-A75976779', 'afatinib', 'AZD-1480', 'mitoxantrone', 'sirolimus', 'BRD-A19037878'], 'count_cv': ['21', '17', '16', '19', '21', '22', '18', '15', '22', '19', '15', '22', '24', '19', '19', '19', '20', '23', '19', '22', '20', '14', '23', '21', '17', '18', '25', '18', '22', '24', '19', '15'], 'count_mean': ['81', '74', '82', '104', '95', '97', '117', '78', '42', '101', '87', '118', '55', '116', '147', '55', '53', '108', '109', '78', '63', '89', '81', '76', '79', '131', '101', '109', '92', '45', '123', '100'], 'det_plate': ['AICHI001_TMD8_24H_X2_B32', 'AICHI001_NALM6_24H_X1_B34', 'AICHI001_SKMEL5_24H_X2_B34', 'AICHI001_OCILY3_24H_X1_B32', 'AICHI001_BJAB_24H_X1_B32', 'AICHI001_THP1_24H_X1_B32', 'AICHI001_OCILY3_24H_X1_B32', 'AICHI001_SKMEL5_24H_X3_B34', 'ABY001_NCIH508_XH_X1_B15', 'AICHI001_THP1_24H_X3_B32', 'AICHI001_KMS34_24H_X1_B34', 'AICHI001_K562_24H_X2_B32', 'ABY001_PC3_XH_X1_B15', 'AICHI002_BJAB_24H_X2_B32', 'AICHI002_BJAB_24H_X2_B32', 'ABY001_A549_XH_X1_B15', 'ABY001_HT29_XH_X1_B15', 'AICHI001_OCILY19_4H_X1_B32', 'AICHI002_BJAB_24H_X2_B32', 'AICHI001_THP1_24H_X1_B32', 'ABY001_A375_XH_X1_B15', 'AICHI001_SKMEL5_24H_X3_B34', 'AICHI001_OCILY19_4H_X1_B32', 'AICHI001_TMD8_24H_X2_B32', 'AICHI001_KMS34_24H_X2_B34', 'AICHI001_OCILY3_24H_X2_B32', 'AICHI001_OCILY19_24H_X1_B32', 'AICHI001_TMD8_4H_X1_B32', 'AICHI001_TMD8_24H_X1_B32', 'ABY001_H1975_XH_X1_B15', 'AICHI002_BJAB_24H_X2_B32', 'AICHI001_NALM6_24H_X2_B34'], 'det_well': ['H05', 'C18', 'P18', 'L11', 'D20', 'D22', 'D06', 'C13', 'F14', 'N07', 'H11', 'G11', 'J04', 'H06', 'M15', 'C21', 'F13', 'B22', 'B24', 'O12', 'M03', 'D08', 'M18', 'N09', 'P05', 'N10', 'I09', 'K22', 'L04', 'O09', 'C22', 'M19'], 'dyn_range': ['26.8889', '14.2369', '20.1362', '22.3032', '19.4149', '29.1929', '24.2595', '23.5294', '17.0112', '19.1729', '25.6797', '30.1677', '16.0574', '22.801', '26.1869', '10.9296', '10.5756', '21.9343', '33.5735', '22.1105', '13.07', '22.9631', '16.9585', '18.8333', '26.3958', '21.634', '18.2683', '19.3333', '14.793', '10.6091', '20.4907', '10.1201'], 'failure_mode': ['nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan'], 'inv_level_10': ['4840.0', '3125.0', '3252.0', '4193.0', '3650.0', '4087.0', '3833.0', '2400.0', '3802.0', '4103.0', '3287.0', '4676.0', '3356.0', '4469.0', '3993.5', '3104.0', '3289.0', '3005.0', '3542.0', '4002.0', '3829.5', '2801.5', '4087.0', '5198.0', '3167.5', '4078.0', '4494.0', '4640.0', '3358.0', '2090.0', '4426.0', '1685.0'], 'nearest_dose': ['0.04', '0.01', '0.01', '0.04', '2.5', '0.125', '0.01', '10.0', '0.66', '10.0', '0.04', '0.04', '2.5', '0.01', '0.66', '0.66', '0.66', '0.125', '0.01', '0.01', '2.5', '2.5', '0.01', '0.66', '0.04', '0.125', '0.66', '0.125', '0.125', '10.0', '0.125', '10.0'], 'pert_dose': ['0.0390625', '0.00976563', '0.00976563', '0.0390625', '2.5', '0.15625', '0.00976562', '10.0', '0.625', '10.0', '0.0390625', '0.0390625', '2.5', '0.00976562', '0.625', '0.625', '0.625', '0.15625', '0.00976562', '0.00976562', '2.5', '2.5', '0.00976562', '0.625', '0.0390625', '0.15625', '0.625', '0.15625', '0.15625', '10.0', '0.156221', '10.0'], 'pert_dose_unit': ['uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM', 'uM'], 'pert_id': ['BRD-K53972329', 'BRD-A04845303', 'BRD-K93468883', 'BRD-K16277767', 'BRD-K16761703', 'BRD-K16761703', 'BRD-A81370665', 'BRD-A04845303', 'BRD-K70511574', 'BRD-K74514084', 'BRD-K72420232', 'BRD-K37142460', 'BRD-K19687926', 'BRD-K06749501', 'BRD-K67392871', 'BRD-K66175015', 'BRD-K74514084', 'BRD-K12343256', 'BRD-K57080016', 'BRD-K00258161', 'BRD-A90490067', 'BRD-K63923597', 'BRD-K05804044', 'BRD-K74514084', 'BRD-K88573743', 'BRD-K74514084', 'BRD-A75976779', 'BRD-K66175015', 'BRD-K65928735', 'BRD-K21680192', 'BRD-A79768653', 'BRD-A19037878'], 'pert_idose': ['0.04 uM', '0.01 uM', '0.01 uM', '0.04 uM', '2.5 uM', '0.125 uM', '0.01 uM', '10 uM', '0.66 uM', '10 uM', '0.04 uM', '0.04 uM', '2.5 uM', '0.01 uM', '0.66 uM', '0.66 uM', '0.66 uM', '0.125 uM', '0.01 uM', '0.01 uM', '2.5 uM', '2.5 uM', '0.01 uM', '0.66 uM', '0.04 uM', '0.125 uM', '0.66 uM', '0.125 uM', '0.125 uM', '10 uM', '0.125 uM', '10 uM'], 'pert_itime': ['24 h', '24 h', '24 h', '24 h', '24 h', '24 h', '24 h', '24 h', '24 h', '24 h', '24 h', '24 h', '3 h', '24 h', '24 h', '24 h', '24 h', '4 h', '24 h', '24 h', '3 h', '24 h', '4 h', '24 h', '24 h', '24 h', '24 h', '4 h', '24 h', '3 h', '24 h', '24 h'], 'pert_mfc_id': ['BRD-K53972329', 'BRD-A04845303', 'BRD-K93468883', 'BRD-K16277767', 'BRD-K16761703', 'BRD-K16761703', 'BRD-A81370665', 'BRD-A04845303', 'BRD-K70511574', 'BRD-K74514084', 'BRD-K72420232', 'BRD-K37142460', 'BRD-K19687926', 'BRD-K06749501', 'BRD-K67392871', 'BRD-K66175015', 'BRD-K74514084', 'BRD-K12343256', 'BRD-K57080016', 'BRD-K00258161', 'BRD-A90490067', 'BRD-K63923597', 'BRD-K05804044', 'BRD-K74514084', 'BRD-K88573743', 'BRD-K74514084', 'BRD-A75976779', 'BRD-K66175015', 'BRD-K65928735', 'BRD-K21680192', 'BRD-A79768653', 'BRD-A19037878'], 'pert_time': ['24.0', '24.0', '24.0', '24.0', '24.0', '24.0', '24.0', '24.0', '24.0', '24.0', '24.0', '24.0', '3.0', '24.0', '24.0', '24.0', '24.0', '4.0', '24.0', '24.0', '3.0', '24.0', '4.0', '24.0', '24.0', '24.0', '24.0', '4.0', '24.0', '3.0', '24.0', '24.0'], 'pert_time_unit': ['h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h'], 'pert_type': ['trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp', 'trt_cp'], 'project_code': ['AICHI', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'ABY', 'AICHI', 'AICHI', 'AICHI', 'ABY', 'AICHI', 'AICHI', 'ABY', 'ABY', 'AICHI', 'AICHI', 'AICHI', 'ABY', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'AICHI', 'ABY', 'AICHI', 'AICHI'], 'qc_f_logp': ['5.1', '5.6', '6.6', '5.2', '5.9', '6.8', '4.9', '7.1', '7.0', '6.0', '7.4', '6.2', '7.2', '6.1', '5.5', '6.6', '6.9', '5.2', '6.3', '5.7', '7.4', '7.1', '5.6', '5.4', '7.2', '6.4', '5.3', '4.9', '5.8', '5.7', '5.7', '5.7'], 'qc_iqr': ['11.41', '11.88', '8.72', '14.1', '12.04', '7.83', '12.86', '9.25', '7.99', '9.49', '11.42', '9.77', '7.65', '11.13', '11.58', '9.21', '10.43', '12.98', '11.31', '9.25', '9.22', '7.99', '13.8', '10.02', '12.02', '11.24', '10.61', '11.6', '11.21', '9.36', '10.53', '13.87'], 'qc_pass': ['1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0'], 'qc_slope': ['56', '57', '54', '56', '57', '54', '56', '55', '59', '58', '56', '54', '60', '54', '55', '63', '61', '57', '53', '56', '60', '54', '59', '56', '55', '55', '57', '54', '59', '65', '56', '58'], 'rna_plate': ['AICHI001_TMD8_24H_X2', 'AICHI001_NALM6_24H_X1', 'AICHI001_SKMEL5_24H_X2', 'AICHI001_OCILY3_24H_X1', 'AICHI001_BJAB_24H_X1', 'AICHI001_THP1_24H_X1', 'AICHI001_OCILY3_24H_X1', 'AICHI001_SKMEL5_24H_X3', 'ABY001_NCIH508_XH_X1', 'AICHI001_THP1_24H_X3', 'AICHI001_KMS34_24H_X1', 'AICHI001_K562_24H_X2', 'ABY001_PC3_XH_X1', 'AICHI002_BJAB_24H_X2', 'AICHI002_BJAB_24H_X2', 'ABY001_A549_XH_X1', 'ABY001_HT29_XH_X1', 'AICHI001_OCILY19_4H_X1', 'AICHI002_BJAB_24H_X2', 'AICHI001_THP1_24H_X1', 'ABY001_A375_XH_X1', 'AICHI001_SKMEL5_24H_X3', 'AICHI001_OCILY19_4H_X1', 'AICHI001_TMD8_24H_X2', 'AICHI001_KMS34_24H_X2', 'AICHI001_OCILY3_24H_X2', 'AICHI001_OCILY19_24H_X1', 'AICHI001_TMD8_4H_X1', 'AICHI001_TMD8_24H_X1', 'ABY001_H1975_XH_X1', 'AICHI002_BJAB_24H_X2', 'AICHI001_NALM6_24H_X2'], 'rna_well': ['H05', 'C18', 'P18', 'L11', 'D20', 'D22', 'D06', 'C13', 'F14', 'N07', 'H11', 'G11', 'J04', 'H06', 'M15', 'C21', 'F13', 'B22', 'B24', 'O12', 'M03', 'D08', 'M18', 'N09', 'P05', 'N10', 'I09', 'K22', 'L04', 'O09', 'C22', 'M19']}\n"
     ]
    }
   ],
   "source": [
    "# 4) Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "# Example iteration\n",
    "for batch in train_loader:\n",
    "    features = batch[\"features\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    meta = batch[\"metadata\"]\n",
    "    print(\"Features Shape:\", features.shape)  # (batch_size, num_genes)\n",
    "    print(\"Labels Shape:\", labels.shape)  # (batch_size, num_genes)\n",
    "    print(\"Metadata sample:\", meta[\"control_metadata\"], meta[\"pert_metadata\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N#CC[C@H](C1CCCC1)n1cc(cn1)-c1ncnc2[nH]ccc12',\n",
       " 'Unknown',\n",
       " 'C[C@H](Nc1ncnc2[nH]cnc12)c1cc2cccc(Cl)c2c(=O)n1-c1ccccc1',\n",
       " 'Unknown',\n",
       " 'CN1CCN(CC1)c1nc(C2=C(C(=O)NC2=O)c2c[nH]c3ccccc23)c2ccccc2n1',\n",
       " 'CN1CCN(CC1)c1nc(C2=C(C(=O)NC2=O)c2c[nH]c3ccccc23)c2ccccc2n1',\n",
       " 'CC(C)CCN1C(C)C(=O)N(C)c2cnc(Nc3cc(F)c(O)c(F)c3)nc12',\n",
       " 'Unknown',\n",
       " 'COc1ccc(cc1)S(=O)(=O)N(C(C)=O)c1ccccc1C=Cc1cc[n+]([O-])cc1',\n",
       " 'CN(c1ccc2c(C)n(C)nc2c1)c1ccnc(Nc2ccc(C)c(c2)S(N)(=O)=O)n1',\n",
       " 'COc1cc(ccc1Nc1ncc(Cl)c(Oc2cccc(NC(=O)C=C)c2)n1)N1CCN(C)CC1',\n",
       " 'Unknown',\n",
       " 'CS(=O)(=O)CCNCc1ccc(o1)-c1ccc2ncnc(Nc3ccc(OCc4cccc(F)c4)c(Cl)c3)c2c1',\n",
       " 'CNC(=O)c1c(F)cccc1Nc1nc(Nc2cc3N(CCc3cc2OC)C(=O)CN(C)C)nc2[nH]ccc12',\n",
       " 'Unknown',\n",
       " 'CN(C)CC=CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1O[C@H]1CCOC1',\n",
       " 'CN(c1ccc2c(C)n(C)nc2c1)c1ccnc(Nc2ccc(C)c(c2)S(N)(=O)=O)n1',\n",
       " 'CC(=O)Nc1cccc(c1)-n1c2c(C)c(=O)n(C)c(Nc3ccc(I)cc3F)c2c(=O)n(C2CC2)c1=O',\n",
       " 'Cn1cnc2c(F)c(Nc3ccc(Br)cc3Cl)c(cc12)C(=O)NOCCO',\n",
       " 'Unknown',\n",
       " 'CC12CCC3C(C2CCC1O)C(CCCCCCCCCS(=O)CCCC(F)(F)C(F)(F)F)Cc4cc(O)ccc34',\n",
       " 'CCN(CCO)CCCOc1ccc2c(Nc3cc(CC(=O)Nc4cccc(F)c4)[nH]n3)ncnc2c1',\n",
       " 'Cc1ccc(NC(=O)c2cccc(c2)C(C)(C)C#N)cc1Nc1ccc2ncn(C)c(=O)c2c1',\n",
       " 'CN(c1ccc2c(C)n(C)nc2c1)c1ccnc(Nc2ccc(C)c(c2)S(N)(=O)=O)n1',\n",
       " 'CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OC[C@H](CC4=CNC5=CC=CC=C54)N',\n",
       " 'CN(c1ccc2c(C)n(C)nc2c1)c1ccnc(Nc2ccc(C)c(c2)S(N)(=O)=O)n1',\n",
       " 'Unknown',\n",
       " 'CN(C)CC=CC(=O)Nc1cc2c(Nc3ccc(F)c(Cl)c3)ncnc2cc1O[C@H]1CCOC1',\n",
       " 'C[C@H](Nc1ncc(Cl)c(Nc2cc(C)[nH]n2)n1)c1ncc(F)cn1',\n",
       " 'OCCNCCNc1ccc(NCCNCCO)c2C(=O)c3c(O)ccc(O)c3C(=O)c12',\n",
       " 'COC1CC(CC(C)C2CC(=O)C(C)C=C(C)C(O)C(OC)C(=O)C(C)CC(C)C=CC=CC=C(C)C(CC3CCC(C)C(O)(O3)C(=O)C(=O)N3CCCCC3C(=O)O2)OC)CCC1O',\n",
       " 'CC(C=C(C)C=CC(=O)NO)C(=O)c1ccc(cc1)N(C)C']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"smiles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import string\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# 1) Simple SMILES Encoder\n",
    "##############################################################################\n",
    "class SimpleSMILESEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A toy SMILES encoder that one-hot encodes each character\n",
    "    and processes it via a small MLP or CNN. Here we use a trivial MLP\n",
    "    for demonstration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab, embed_dim=64, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab (str or list): A string or list of allowed SMILES characters.\n",
    "            embed_dim (int): Size of the per-character embedding.\n",
    "            hidden_dim (int): Size of the hidden layer in the MLP that encodes the SMILES.\n",
    "        \"\"\"\n",
    "        super(SimpleSMILESEncoder, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # A simple embedding layer for each character:\n",
    "        self.embedding = nn.Embedding(self.vocab_size, embed_dim)\n",
    "\n",
    "        # Then a small MLP to compress all characters into a single vector\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # A final pooling transformation to get a single vector\n",
    "        # (In practice, you might do average/attention pooling across characters.)\n",
    "        # For simplicity, we apply MLP to each character embedding and average.\n",
    "        # More advanced: CNN or RNN or Transformer.\n",
    "\n",
    "\n",
    "    def forward(self, smiles_batch):\n",
    "        tokens, _ = self.smiles_to_indices(smiles_batch)\n",
    "\n",
    "        # Make sure tokens are on the same device as the embedding layer\n",
    "        tokens = tokens.to(self.embedding.weight.device)\n",
    "\n",
    "        embedded = self.embedding(tokens)  # (B, max_len, embed_dim)\n",
    "        B, L, E = embedded.shape\n",
    "        embedded = embedded.view(B * L, E)  # Flatten\n",
    "        embedded = self.mlp(embedded)  # (B * L, hidden_dim)\n",
    "        embedded = embedded.view(B, L, -1)  # (B, max_len, hidden_dim)\n",
    "        embedded = embedded.mean(dim=1)  # (B, hidden_dim)\n",
    "        return embedded\n",
    "\n",
    "    def smiles_to_indices(self, smiles_batch):\n",
    "        \"\"\"\n",
    "        Convert list of SMILES strings to a batch of indices for each character.\n",
    "        We'll simply pad them to max_len in this toy example.\n",
    "        \"\"\"\n",
    "        # Find max length\n",
    "        max_len = max(len(s) for s in smiles_batch)\n",
    "        token_ids = []\n",
    "        lengths = []\n",
    "\n",
    "        char_to_idx = {c: i for i, c in enumerate(self.vocab)}\n",
    "\n",
    "        for s in smiles_batch:\n",
    "            lengths.append(len(s))\n",
    "            row = []\n",
    "            for ch in s:\n",
    "                if ch in char_to_idx:\n",
    "                    row.append(char_to_idx[ch])\n",
    "                else:\n",
    "                    row.append(char_to_idx[\"?\"])  # unknown char\n",
    "            # pad\n",
    "            while len(row) < max_len:\n",
    "                row.append(char_to_idx[\" \"])  # space or a pad token\n",
    "            token_ids.append(row)\n",
    "\n",
    "        # Convert to torch tensor\n",
    "        tokens = torch.tensor(token_ids, dtype=torch.long)\n",
    "        return tokens, lengths\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# 2) Multimodal Model: Gene + SMILES\n",
    "##############################################################################\n",
    "class MultimodalModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model that encodes (unperturbed gene expression) + (drug SMILES)\n",
    "    and predicts (perturbed gene expression).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, gene_dim, gene_hidden_dim=512, drug_hidden_dim=128, smiles_vocab=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gene_dim (int): Dimensionality of the gene expression input.\n",
    "            gene_hidden_dim (int): hidden dimension for the gene MLP.\n",
    "            drug_hidden_dim (int): hidden dimension for the SMILES encoder output.\n",
    "            smiles_vocab (str or list): vocabulary for the SMILES.\n",
    "        \"\"\"\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        if smiles_vocab is None:\n",
    "            # A simple default vocabulary of typical SMILES chars + space + ?\n",
    "            # In practice, define a more robust set of tokens\n",
    "            smiles_vocab = \"ACGT()[]=+#@0123456789abcdefghijklmnopqrstuvwxyz\" + \"? \"\n",
    "\n",
    "        self.gene_dim = gene_dim\n",
    "        self.gene_hidden_dim = gene_hidden_dim\n",
    "        self.drug_hidden_dim = drug_hidden_dim\n",
    "\n",
    "        # 2.1) Gene MLP\n",
    "        self.gene_encoder = nn.Sequential(\n",
    "            nn.Linear(gene_dim, gene_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gene_hidden_dim, gene_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # 2.2) SMILES encoder\n",
    "        self.smiles_encoder = SimpleSMILESEncoder(\n",
    "            vocab=smiles_vocab, embed_dim=64, hidden_dim=drug_hidden_dim\n",
    "        )\n",
    "\n",
    "        # 2.3) Fusion -> final MLP to predict the same dimension as gene_dim\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(gene_hidden_dim + drug_hidden_dim, gene_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                gene_hidden_dim, gene_dim\n",
    "            ),  # we predict perturbed expression (size = gene_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, gene_expr, smiles_batch):\n",
    "        \"\"\"\n",
    "        gene_expr: Tensor of shape (B, gene_dim)\n",
    "        smiles_batch: List of length B containing SMILES strings\n",
    "        \"\"\"\n",
    "        # Encode gene\n",
    "        gene_emb = self.gene_encoder(gene_expr)  # (B, gene_hidden_dim)\n",
    "        # Encode SMILES\n",
    "        drug_emb = self.smiles_encoder(smiles_batch)  # (B, drug_hidden_dim)\n",
    "\n",
    "        fused = torch.cat(\n",
    "            [gene_emb, drug_emb], dim=1\n",
    "        )  # (B, gene_hidden_dim + drug_hidden_dim)\n",
    "        out = self.fusion(fused)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10260/1570398637.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m train_losses, val_losses = train_multimodal_model(\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programming\\5ARG45\\5ARG45\\src\\training.py\u001b[0m in \u001b[0;36mtrain_multimodal_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, device, gradient_clipping, early_stopping_patience, model_name, use_mixed_precision)\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[0menabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_mixed_precision\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         ):\n\u001b[1;32m--> 276\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    699\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             if (\n",
      "\u001b[1;32mc:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__getitems__\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    418\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programming\\5ARG45\\5ARG45\\src\\data_sets.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;31m# read row from HDF5 or cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[0mctrl_expr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrols_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m         \u001b[0mpert_expr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperturbations_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpert_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programming\\5ARG45\\5ARG45\\src\\data_sets.py\u001b[0m in \u001b[0;36mread_row_nocache\u001b[1;34m(h5_path, row_index)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mread_row_nocache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_row_nocache\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\h5py\\_hl\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, args, new_dtype)\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fast_read_ok\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnew_dtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fast_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    782\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m                 \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Fall back to Python read pathway below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from training import train_multimodal_model\n",
    "import logging\n",
    "\n",
    "# Set logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "model = MultimodalModel(gene_dim=12328)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", patience=3\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_losses, val_losses = train_multimodal_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    epochs=20,\n",
    "    device=device,\n",
    "    gradient_clipping=1.0,\n",
    "    early_stopping_patience=5,\n",
    "    model_name=\"MultimodalModel\",\n",
    "    use_mixed_precision=True,\n",
    ")\n",
    "\n",
    "print(\"Training complete. Best model restored. Final train and val losses:\")\n",
    "print(\"Train:\", train_losses)\n",
    "print(\"Val:\", val_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5ARG45",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
