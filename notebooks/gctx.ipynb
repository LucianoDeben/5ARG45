{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Step 1: Load the data matrix and metadata from the .gctx file\n",
    "# gctx_file = \"../data/raw/level3_beta_ctl_n188708x12328.gctx\"\n",
    "# h5_file = \"../data/raw/compound_control_dataset_all.h5\"\n",
    "\n",
    "# chunk_size = 10000  # Number of rows to process at a time\n",
    "\n",
    "# with h5py.File(gctx_file, \"r\") as f:\n",
    "#     # Access data matrix and metadata\n",
    "#     data = f[\"/0/DATA/0/matrix\"]\n",
    "#     row_ids = f[\"/0/META/COL/id\"][:].astype(str)  # All sample IDs (rows)\n",
    "#     col_ids = f[\"/0/META/ROW/id\"][:].astype(int)  # All gene IDs (columns)\n",
    "\n",
    "#     # Load metadata files\n",
    "#     gene_metadata = pd.read_csv(\"../data/raw/geneinfo_beta.txt\", sep=\"\\t\")\n",
    "#     pert_info = pd.read_csv(\"../data/raw/instinfo_beta.txt\", sep=\"\\t\")\n",
    "\n",
    "#     # Filter perturbation metadata\n",
    "#     filtered_pert_info = pert_info[\n",
    "#         (pert_info[\"qc_pass\"] == 1.0) & (pert_info[\"pert_type\"] == \"ctl_vehicle\")\n",
    "#     ].copy()\n",
    "#     filtered_pert_info.set_index(\"sample_id\", inplace=True)\n",
    "\n",
    "#     # Filter valid sample IDs\n",
    "#     valid_sample_ids = pd.Index(row_ids).intersection(filtered_pert_info.index)\n",
    "#     filtered_pert_info = filtered_pert_info.loc[valid_sample_ids]\n",
    "#     filtered_pert_info.sort_index(inplace=True)\n",
    "\n",
    "#     # Create a mapping of sample IDs to row indices\n",
    "#     row_idx_map = {rid: idx for idx, rid in enumerate(row_ids)}\n",
    "\n",
    "#     # Compute row indices for valid samples\n",
    "#     filtered_row_indices = [row_idx_map[sid] for sid in valid_sample_ids]\n",
    "\n",
    "# # Step 2: Write the filtered data matrix and metadata in chunks\n",
    "# with h5py.File(h5_file, \"w\") as h5f:\n",
    "#     # Pre-allocate space for the dataset in the HDF5 file\n",
    "#     data_shape = (len(filtered_row_indices), len(col_ids))\n",
    "#     data_dset = h5f.create_dataset(\n",
    "#         \"data\", shape=data_shape, dtype=\"float32\", compression=\"gzip\"\n",
    "#     )\n",
    "\n",
    "#     # Write row (sample) metadata\n",
    "#     for column in filtered_pert_info.columns:\n",
    "#         h5f.create_dataset(\n",
    "#             f\"row_metadata/{column}\",\n",
    "#             data=filtered_pert_info[column].values.astype(\"S\"),\n",
    "#         )\n",
    "\n",
    "#     # Write column (gene) metadata\n",
    "#     for column in gene_metadata.columns:\n",
    "#         h5f.create_dataset(\n",
    "#             f\"col_metadata/{column}\",\n",
    "#             data=gene_metadata[column].values.astype(\"S\"),\n",
    "#         )\n",
    "\n",
    "#     # Write row and column IDs\n",
    "#     h5f.create_dataset(\"row_ids\", data=valid_sample_ids.values.astype(\"S\"))\n",
    "#     gene_mapping = gene_metadata.set_index(\"gene_id\")[\"gene_symbol\"].to_dict()\n",
    "#     filtered_col_symbols = [gene_mapping.get(gid, f\"Gene_{gid}\") for gid in col_ids]\n",
    "#     h5f.create_dataset(\n",
    "#         \"col_ids\", data=[x.encode(\"utf-8\") for x in filtered_col_symbols]\n",
    "#     )\n",
    "\n",
    "#     # Process and write data in chunks\n",
    "#     for i in range(0, len(filtered_row_indices), chunk_size):\n",
    "#         chunk_indices = filtered_row_indices[i : i + chunk_size]  # Current chunk\n",
    "#         chunk_data = data[chunk_indices, :]  # Load chunk from the data matrix\n",
    "#         data_dset[i : i + len(chunk_indices), :] = chunk_data  # Write to dataset\n",
    "\n",
    "# print(f\"Filtered data and metadata successfully written to {h5_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data and metadata successfully written to ../data/raw/compound_control_dataset.h5.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the data matrix and metadata from the .gctx file\n",
    "gctx_file = \"../data/raw/level3_beta_trt_cp_n1805898x12328.gctx\"\n",
    "\n",
    "with h5py.File(gctx_file, \"r\") as f:\n",
    "    # Access the entire data matrix and metadata\n",
    "    data = f[\"/0/DATA/0/matrix\"]\n",
    "    row_ids = f[\"/0/META/COL/id\"][:].astype(str)  # Sample IDs (rows)\n",
    "    col_ids = f[\"/0/META/ROW/id\"][:].astype(int)  # Gene IDs (columns)\n",
    "\n",
    "    # Load metadata files\n",
    "    gene_metadata = pd.read_csv(\"../data/raw/geneinfo_beta.txt\", sep=\"\\t\")\n",
    "    pert_info = pd.read_csv(\"../data/raw/compound_perturbation_metadata.txt\", sep=\"\\t\")\n",
    "\n",
    "    # Filter perturbation metadata\n",
    "    filtered_pert_info = pert_info[\n",
    "        (pert_info[\"qc_pass\"] == 1.0) & (pert_info[\"pert_type\"] == \"trt_cp\")\n",
    "    ].copy()\n",
    "    filtered_pert_info.set_index(\"sample_id\", inplace=True)\n",
    "\n",
    "    # STEP A: Reduce filtered_pert_info to match row_ids\n",
    "    subset_row_ids = pd.Index(row_ids)\n",
    "    subset_row_ids = subset_row_ids.intersection(filtered_pert_info.index)\n",
    "    subset_pert_info = filtered_pert_info.loc[subset_row_ids].copy()\n",
    "    subset_pert_info.sort_index(inplace=True)\n",
    "\n",
    "    # Keep a separate column for sample_id (while retaining it as index)\n",
    "    subset_pert_info[\"sample_id\"] = subset_pert_info.index\n",
    "\n",
    "    # STEP B: Recompute the row indices in the data matrix for valid sample_ids\n",
    "    row_idx_map = {rid: idx for idx, rid in enumerate(row_ids)}\n",
    "    filtered_row_indices = [row_idx_map[sid] for sid in subset_pert_info.index]\n",
    "\n",
    "    # Filter the data matrix to match the final subset of row indices\n",
    "    filtered_data_matrix = data[filtered_row_indices, :]\n",
    "\n",
    "# Step 2: Map gene IDs to gene symbols\n",
    "gene_mapping = gene_metadata.set_index(\"gene_id\")[\"gene_symbol\"].to_dict()\n",
    "filtered_col_symbols = [gene_mapping.get(gid, f\"Gene_{gid}\") for gid in col_ids]\n",
    "\n",
    "# Step 3: Write the filtered data and metadata to an HDF5 file\n",
    "h5_file = \"../data/raw/compound_perturbation_dataset.h5\"\n",
    "\n",
    "rows, cols = filtered_data_matrix.shape\n",
    "# Decide on a chunk shape\n",
    "chunk_rows = 128  # e.g. 128 rows per chunk\n",
    "chunk_cols = cols  # store entire row block in each chunk, or you could do smaller\n",
    "chunk_shape = (chunk_rows, chunk_cols)\n",
    "\n",
    "with h5py.File(h5_file, \"w\") as h5f:\n",
    "    # Write the filtered data matrix\n",
    "    h5f.create_dataset(\"data\", data=filtered_data_matrix, compression=\"gzip\", chunks=chunk_shape)\n",
    "\n",
    "    # Write row (sample) metadata\n",
    "    for column in subset_pert_info.columns:\n",
    "        h5f.create_dataset(\n",
    "            f\"row_metadata/{column}\",\n",
    "            data=subset_pert_info[column].values.astype(\"S\"),\n",
    "        )\n",
    "\n",
    "    # Write column (gene) metadata\n",
    "    for column in gene_metadata.columns:\n",
    "        h5f.create_dataset(\n",
    "            f\"col_metadata/{column}\",\n",
    "            data=gene_metadata[column].values.astype(\"S\"),\n",
    "        )\n",
    "\n",
    "    # Write final row and column IDs\n",
    "    h5f.create_dataset(\"row_ids\", data=subset_pert_info.index.values.astype(\"S\"))\n",
    "    h5f.create_dataset(\n",
    "        \"col_ids\", data=[x.encode(\"utf-8\") for x in filtered_col_symbols]\n",
    "    )\n",
    "\n",
    "print(f\"Filtered data and metadata successfully written to {h5_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data and metadata successfully written to ../data/raw/compound_perturbation_dataset.h5 in chunks!\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example paths\n",
    "gctx_file = \"../data/raw/level3_beta_trt_cp_n1805898x12328.gctx\"\n",
    "h5_file = \"../data/raw/compound_perturbation_dataset.h5\"\n",
    "\n",
    "# Step 1: Read and filter row metadata, then determine which row indices to keep\n",
    "with h5py.File(gctx_file, \"r\") as f_in:\n",
    "    # Original row IDs\n",
    "    row_ids = f_in[\"/0/META/COL/id\"][:].astype(str)\n",
    "\n",
    "# Load external metadata (e.g., geneinfo, pert_info) as needed\n",
    "gene_metadata = pd.read_csv(\"../data/raw/geneinfo_beta.txt\", sep=\"\\t\")\n",
    "pert_info = pd.read_csv(\"../data/raw/compound_perturbation_metadata.txt\", sep=\"\\t\")\n",
    "\n",
    "filtered_pert_info = pert_info[\n",
    "    (pert_info[\"qc_pass\"] == 1.0) & (pert_info[\"pert_type\"] == \"trt_cp\")\n",
    "].copy()\n",
    "filtered_pert_info.set_index(\"sample_id\", inplace=True)\n",
    "\n",
    "subset_row_ids = pd.Index(row_ids).intersection(filtered_pert_info.index)\n",
    "subset_pert_info = filtered_pert_info.loc[subset_row_ids].copy()\n",
    "subset_pert_info.sort_index(inplace=True)\n",
    "subset_pert_info[\"sample_id\"] = subset_pert_info.index\n",
    "\n",
    "row_idx_map = {rid: idx for idx, rid in enumerate(row_ids)}\n",
    "filtered_row_indices = [row_idx_map[sid] for sid in subset_pert_info.index]\n",
    "\n",
    "# Step 2: Determine column subset or read all columns\n",
    "# (If you do partial columns, e.g. for landmark genes, define 'col_indices' here.\n",
    "# Otherwise, read all columns.)\n",
    "col_ids_all = None\n",
    "col_symbols_all = None\n",
    "with h5py.File(gctx_file, \"r\") as f_in:\n",
    "    col_ids_all = f_in[\"/0/META/ROW/id\"][:].astype(int)\n",
    "\n",
    "gene_mapping = gene_metadata.set_index(\"gene_id\")[\"gene_symbol\"].to_dict()\n",
    "col_symbols_all = [gene_mapping.get(gid, f\"Gene_{gid}\") for gid in col_ids_all]\n",
    "col_indices = np.arange(len(col_ids_all))  # or define a subset for partial columns\n",
    "\n",
    "# Step 3: Create a chunked output dataset, define chunk shape\n",
    "# We read in chunks from the big file and write them chunk by chunk.\n",
    "chunk_rows_read = 50000  # number of rows to read from the big file at once\n",
    "\n",
    "# We'll define the final shape for the output.\n",
    "# Number of rows is len(filtered_row_indices), columns is len(col_indices).\n",
    "n_rows_out = len(filtered_row_indices)\n",
    "n_cols_out = len(col_indices)\n",
    "\n",
    "# We'll set a chunk shape for the output as well (for final chunked HDF5).\n",
    "chunk_shape_out = (128, n_cols_out)\n",
    "\n",
    "# Step 4: Reading and writing chunk by chunk\n",
    "with h5py.File(gctx_file, \"r\") as f_in, h5py.File(h5_file, \"w\") as f_out:\n",
    "    data_in = f_in[\"/0/DATA/0/matrix\"]\n",
    "    # Create the output dataset in chunked form\n",
    "    dset_out = f_out.create_dataset(\n",
    "        \"data\",\n",
    "        shape=(n_rows_out, n_cols_out),\n",
    "        dtype=\"float32\",\n",
    "        compression=\"gzip\",\n",
    "        chunks=chunk_shape_out,\n",
    "    )\n",
    "\n",
    "    # We'll keep track of how many rows we've already written in the output\n",
    "    out_row_start = 0\n",
    "    total_rows = n_rows_out\n",
    "\n",
    "    for start_idx in range(0, total_rows, chunk_rows_read):\n",
    "        end_idx = min(start_idx + chunk_rows_read, total_rows)\n",
    "        chunk_size = end_idx - start_idx\n",
    "\n",
    "        # Subset of global row indices for this chunk\n",
    "        row_subset = filtered_row_indices[start_idx:end_idx]\n",
    "\n",
    "        # Read those rows from the big file, and only the desired columns\n",
    "        chunk_data = data_in[row_subset, :][\n",
    "            :, col_indices\n",
    "        ]  # shape: (chunk_size, n_cols_out)\n",
    "\n",
    "        # Write to the new dataset\n",
    "        dset_out[out_row_start : out_row_start + chunk_size, :] = chunk_data\n",
    "        out_row_start += chunk_size\n",
    "\n",
    "    # Step 5: Write row metadata\n",
    "    for column in subset_pert_info.columns:\n",
    "        f_out.create_dataset(\n",
    "            f\"row_metadata/{column}\",\n",
    "            data=subset_pert_info[column].values.astype(\"S\"),\n",
    "        )\n",
    "\n",
    "    # Step 6: Write column metadata\n",
    "    # For partial columns, you might subset your gene_metadata accordingly if needed.\n",
    "    for col_name in gene_metadata.columns:\n",
    "        f_out.create_dataset(\n",
    "            f\"col_metadata/{col_name}\",\n",
    "            data=gene_metadata[col_name].values.astype(\"S\"),\n",
    "        )\n",
    "\n",
    "    # Step 7: Write final row and column IDs\n",
    "    f_out.create_dataset(\"row_ids\", data=subset_pert_info.index.values.astype(\"S\"))\n",
    "    selected_col_symbols = [col_symbols_all[i] for i in col_indices]\n",
    "    f_out.create_dataset(\n",
    "        \"col_ids\", data=[x.encode(\"utf-8\") for x in selected_col_symbols]\n",
    "    )\n",
    "\n",
    "print(f\"Filtered data and metadata successfully written to {h5_file} in chunks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20191678\\AppData\\Local\\Temp/ipykernel_12524/1518093917.py:4: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sample_meta = pd.read_csv(\"../data/raw/instinfo_beta.txt\", sep=\"\\t\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "sample_meta = pd.read_csv(\"../data/raw/instinfo_beta.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pert_dose_unit\n",
       "Unknown    112\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First select only the samples that have passed QC and are trt_cp (compound treatment)\n",
    "sample_meta_filtered = sample_meta[\n",
    "    (sample_meta[\"qc_pass\"] == 1.0)\n",
    "    & (\n",
    "        (sample_meta[\"pert_type\"] == \"ctl_vehicle\")\n",
    "    )\n",
    "]\n",
    "\n",
    "# Now check the value counts for the pert_dose_unit\n",
    "sample_meta_filtered[\"pert_dose_unit\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76742"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_meta_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20191678\\AppData\\Local\\Temp/ipykernel_12524/2838101301.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_meta_filtered[\"pert_dose_unit\"] = \"uM\"\n",
      "C:\\Users\\20191678\\AppData\\Local\\Temp/ipykernel_12524/2838101301.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_meta_filtered[\"pert_dose\"] = 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pert_dose_unit\n",
       "uM    76742\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the pert_dose_unit to \"uM\" for all samples and the pert_dose to 0.0\n",
    "sample_meta_filtered[\"pert_dose_unit\"] = \"uM\"\n",
    "sample_meta_filtered[\"pert_dose\"] = 0.0\n",
    "\n",
    "# Check the value counts for the pert_dose_unit\n",
    "sample_meta_filtered[\"pert_dose_unit\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save this filtered metadata to a new file\n",
    "sample_meta_filtered.to_csv(\n",
    "    \"../data/raw/compound_control_metadata.txt\", sep=\"\\t\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all rows pert_dose to 0.0 where pert_dose_unit is \"Unknown\" or Nan/None types\n",
    "sample_meta_filtered.loc[\n",
    "    sample_meta_filtered[\"pert_dose_unit\"].isin([\"Unknown\", None]), \"pert_dose\"\n",
    "] = 0.0\n",
    "\n",
    "# Fill missing values with \"uM\" and replace \"Unknown\" with \"uM\"\n",
    "sample_meta_filtered[\"pert_dose_unit\"].fillna(\"uM\", inplace=True)\n",
    "sample_meta_filtered[\"pert_dose_unit\"].replace(\"Unknown\", \"uM\", inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5_file = \"../data/raw/compound_control_dataset.h5\"\n",
    "# with h5py.File(h5_file, \"r\") as h5f:\n",
    "#     # Load the data matrix\n",
    "#     data_matrix = h5f[\"data\"][:]\n",
    "\n",
    "#     # Decode row metadata\n",
    "#     row_metadata = {\n",
    "#         key: h5f[f\"row_metadata/{key}\"][:].astype(str) for key in h5f[\"row_metadata\"]\n",
    "#     }\n",
    "\n",
    "#     # Decode column metadata\n",
    "#     col_metadata = {\n",
    "#         key: h5f[f\"col_metadata/{key}\"][:].astype(str) for key in h5f[\"col_metadata\"]\n",
    "#     }\n",
    "\n",
    "#     # Load and decode row and column IDs\n",
    "#     row_ids = h5f[\"row_ids\"][:].astype(str)\n",
    "#     col_ids = h5f[\"col_ids\"][:].astype(str)\n",
    "\n",
    "# # Convert metadata to DataFrames for ease of use (optional)\n",
    "# import pandas as pd\n",
    "\n",
    "# row_metadata_df = pd.DataFrame(row_metadata)\n",
    "# col_metadata_df = pd.DataFrame(col_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\20191678\\AppData\\Local\\miniconda3\\envs\\5ARG45\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'dgl'\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n"
     ]
    }
   ],
   "source": [
    "# Add src to path\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"src\")))\n",
    "from utils import create_smiles_dict\n",
    "\n",
    "from deepchem.feat.smiles_tokenizer import SmilesTokenizer\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from data_sets import PerturbationDataset\n",
    "import pandas as pd\n",
    "from training import train_multimodal_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from evaluation import evaluate_multimodal_model\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 16, 19, 15, 20, 15, 15, 15, 15, 17, 19, 16, 18, 15, 20, 31, 15, 20,\n",
       "         15, 15, 17, 25, 25, 20, 31, 15, 20, 15, 15, 25, 15, 21, 15, 15, 17, 28,\n",
       "         18, 15, 15, 15, 20, 21, 18, 16, 17, 22, 19, 18, 23, 16, 20, 17, 16, 21,\n",
       "         16, 16, 26, 16, 16, 17, 16, 21, 18, 16, 16, 20, 16, 26, 18, 16, 17, 19,\n",
       "         18, 22, 19, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the datasets\n",
    "controls = \"../data/raw/compound_control_dataset.h5\"\n",
    "perturbations = \"../data/raw/compound_pertubation_dataset.h5\"\n",
    "\n",
    "# Load the SMILES data and create a dictionary mapping\n",
    "smiles_df = pd.read_csv(\"../data/raw/compoundinfo_beta.txt\", sep=\"\\t\")\n",
    "smiles_dict = create_smiles_dict(smiles_df)\n",
    "\n",
    "# Load the SMILES tokenizer\n",
    "vocab_file = (\"../data/raw/vocab.txt\" )\n",
    "smiles_tokenizer = SmilesTokenizer(vocab_file=vocab_file)\n",
    "\n",
    "# Analyze SMILES string lengths to determine an appropriate max_length\n",
    "smiles_lengths = [len(smile) for smile in smiles_dict.values()]\n",
    "max_length = min(max(smiles_lengths), 128)\n",
    "\n",
    "# Check some SMILES strings to tokens\n",
    "smiles = list(smiles_dict.values())\n",
    "tokens = smiles_tokenizer.encode(\n",
    "    smiles[13],\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_length,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PerturbationDataset(\n",
    "    controls_file=controls,\n",
    "    perturbations_file=perturbations,\n",
    "    smiles_dict=smiles_dict,\n",
    "    plate_column=\"det_plate\",\n",
    "    normalize=True,\n",
    "    n_rows=10000,\n",
    "    pairing=\"random\",\n",
    "    landmark_only=True,\n",
    "    tokenizer=smiles_tokenizer,\n",
    "    max_smiles_length=max_length,\n",
    ")\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "total_len = len(dataset)\n",
    "train_size = int(train_ratio * total_len)\n",
    "val_size = int(val_ratio * total_len)\n",
    "test_size = total_len - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Log the sizes of the datasets\n",
    "logging.debug(\n",
    "    f\"Train Dataset: {len(train_dataset)}, Val Dataset: {len(val_dataset)}, Test Dataset: {len(test_dataset)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class SimpleSMILESEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A toy SMILES encoder that one-hot encodes each character\n",
    "    and processes it via a small MLP or CNN. Here we use a trivial MLP\n",
    "    for demonstration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab, embed_dim=64, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab (str or list): A string or list of allowed SMILES characters.\n",
    "            embed_dim (int): Size of the per-character embedding.\n",
    "            hidden_dim (int): Size of the hidden layer in the MLP that encodes the SMILES.\n",
    "        \"\"\"\n",
    "        super(SimpleSMILESEncoder, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # A simple embedding layer for each character:\n",
    "        self.embedding = nn.Embedding(self.vocab_size, embed_dim)\n",
    "\n",
    "        # Then a small MLP to compress all characters into a single vector\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # A final pooling transformation to get a single vector\n",
    "        # (In practice, you might do average/attention pooling across characters.)\n",
    "        # For simplicity, we apply MLP to each character embedding and average.\n",
    "        # More advanced: CNN or RNN or Transformer.\n",
    "\n",
    "    def forward(self, smiles_batch):\n",
    "        tokens, _ = self.smiles_to_indices(smiles_batch)\n",
    "\n",
    "        # Make sure tokens are on the same device as the embedding layer\n",
    "        tokens = tokens.to(self.embedding.weight.device)\n",
    "\n",
    "        embedded = self.embedding(tokens)\n",
    "        B, L, E = embedded.shape\n",
    "        embedded = embedded.view(B * L, E)\n",
    "        embedded = self.mlp(embedded)\n",
    "        embedded = embedded.view(B, L, -1)\n",
    "        embedded = embedded.mean(dim=1)\n",
    "        return embedded\n",
    "\n",
    "    def smiles_to_indices(self, smiles_batch):\n",
    "        \"\"\"\n",
    "        Convert list of SMILES strings to a batch of indices for each character.\n",
    "        We'll simply pad them to max_len in this toy example.\n",
    "        \"\"\"\n",
    "        # Find max length\n",
    "        max_len = max(len(s) for s in smiles_batch)\n",
    "        token_ids = []\n",
    "        lengths = []\n",
    "\n",
    "        char_to_idx = {c: i for i, c in enumerate(self.vocab)}\n",
    "\n",
    "        for s in smiles_batch:\n",
    "            lengths.append(len(s))\n",
    "            row = []\n",
    "            for ch in s:\n",
    "                if ch in char_to_idx:\n",
    "                    row.append(char_to_idx[ch])\n",
    "                else:\n",
    "                    row.append(char_to_idx[\"?\"])\n",
    "            # pad\n",
    "            while len(row) < max_len:\n",
    "                row.append(char_to_idx[\" \"])\n",
    "            token_ids.append(row)\n",
    "\n",
    "        # Convert to torch tensor\n",
    "        tokens = torch.tensor(token_ids, dtype=torch.long)\n",
    "        return tokens, lengths\n",
    "\n",
    "\n",
    "class Perturbinator(nn.Module):\n",
    "    \"\"\"\n",
    "    A model that encodes (unperturbed gene expression) + (drug SMILES)\n",
    "    and predicts (perturbed gene expression).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, gene_dim, gene_hidden_dim=512, drug_hidden_dim=128, smiles_vocab=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gene_dim (int): Dimensionality of the gene expression input.\n",
    "            gene_hidden_dim (int): hidden dimension for the gene MLP.\n",
    "            drug_hidden_dim (int): hidden dimension for the SMILES encoder output.\n",
    "            smiles_vocab (str or list): vocabulary for the SMILES.\n",
    "        \"\"\"\n",
    "        super(Perturbinator, self).__init__()\n",
    "        if smiles_vocab is None:\n",
    "            # A simple default vocabulary of typical SMILES chars + space + ?\n",
    "            # In practice, define a more robust set of tokens\n",
    "            smiles_vocab = \"ACGT()[]=+#@0123456789abcdefghijklmnopqrstuvwxyz\" + \"? \"\n",
    "\n",
    "        self.gene_dim = gene_dim\n",
    "        self.gene_hidden_dim = gene_hidden_dim\n",
    "        self.drug_hidden_dim = drug_hidden_dim\n",
    "\n",
    "        # 2.1) Gene MLP\n",
    "        self.gene_encoder = nn.Sequential(\n",
    "            nn.Linear(gene_dim, gene_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gene_hidden_dim, gene_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # 2.2) SMILES encoder\n",
    "        self.smiles_encoder = SimpleSMILESEncoder(\n",
    "            vocab=smiles_vocab, embed_dim=64, hidden_dim=drug_hidden_dim\n",
    "        )\n",
    "\n",
    "        # 2.3) Fusion -> final MLP to predict the same dimension as gene_dim\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(gene_hidden_dim + drug_hidden_dim, gene_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                gene_hidden_dim, gene_dim\n",
    "            ),  # we predict perturbed expression (size = gene_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, gene_expr, smiles_batch):\n",
    "        \"\"\"\n",
    "        gene_expr: Tensor of shape (B, gene_dim)\n",
    "        smiles_batch: List of length B containing SMILES strings\n",
    "        \"\"\"\n",
    "        # Encode gene\n",
    "        gene_emb = self.gene_encoder(gene_expr)  # (B, gene_hidden_dim)\n",
    "        # Encode SMILES\n",
    "        drug_emb = self.smiles_encoder(smiles_batch)  # (B, drug_hidden_dim)\n",
    "\n",
    "        fused = torch.cat(\n",
    "            [gene_emb, drug_emb], dim=1\n",
    "        )  # (B, gene_hidden_dim + drug_hidden_dim)\n",
    "        out = self.fusion(fused)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/20 [00:00<?, ?it/s]2024-12-25 12:30:19 [INFO] - Epoch 1/20 - Perturbinator, Train Loss: 0.0481, Val Loss: 0.0136\n",
      "Training Progress:   5%|▌         | 1/20 [00:10<03:17, 10.39s/it]2024-12-25 12:30:29 [INFO] - Epoch 2/20 - Perturbinator, Train Loss: 0.0128, Val Loss: 0.0122\n",
      "Training Progress:  10%|█         | 2/20 [00:20<03:05, 10.31s/it]2024-12-25 12:30:40 [INFO] - Epoch 3/20 - Perturbinator, Train Loss: 0.0122, Val Loss: 0.0121\n",
      "Training Progress:  15%|█▌        | 3/20 [00:31<02:56, 10.36s/it]2024-12-25 12:30:50 [INFO] - Epoch 4/20 - Perturbinator, Train Loss: 0.0121, Val Loss: 0.0120\n",
      "Training Progress:  20%|██        | 4/20 [00:41<02:44, 10.29s/it]2024-12-25 12:31:00 [INFO] - Epoch 5/20 - Perturbinator, Train Loss: 0.0120, Val Loss: 0.0118\n",
      "Training Progress:  25%|██▌       | 5/20 [00:51<02:34, 10.29s/it]2024-12-25 12:31:09 [INFO] - Epoch 6/20 - Perturbinator, Train Loss: 0.0116, Val Loss: 0.0108\n",
      "Training Progress:  30%|███       | 6/20 [01:00<02:17,  9.85s/it]2024-12-25 12:31:18 [INFO] - Epoch 7/20 - Perturbinator, Train Loss: 0.0101, Val Loss: 0.0097\n",
      "Training Progress:  35%|███▌      | 7/20 [01:09<02:03,  9.51s/it]2024-12-25 12:31:27 [INFO] - Epoch 8/20 - Perturbinator, Train Loss: 0.0093, Val Loss: 0.0088\n",
      "Training Progress:  40%|████      | 8/20 [01:18<01:51,  9.32s/it]2024-12-25 12:31:36 [INFO] - Epoch 9/20 - Perturbinator, Train Loss: 0.0085, Val Loss: 0.0084\n",
      "Training Progress:  45%|████▌     | 9/20 [01:27<01:40,  9.16s/it]2024-12-25 12:31:44 [INFO] - Epoch 10/20 - Perturbinator, Train Loss: 0.0082, Val Loss: 0.0081\n",
      "Training Progress:  50%|█████     | 10/20 [01:35<01:30,  9.06s/it]2024-12-25 12:31:54 [INFO] - Epoch 11/20 - Perturbinator, Train Loss: 0.0079, Val Loss: 0.0078\n",
      "Training Progress:  55%|█████▌    | 11/20 [01:45<01:22,  9.19s/it]2024-12-25 12:32:04 [INFO] - Epoch 12/20 - Perturbinator, Train Loss: 0.0075, Val Loss: 0.0074\n",
      "Training Progress:  60%|██████    | 12/20 [01:55<01:15,  9.49s/it]2024-12-25 12:32:14 [INFO] - Epoch 13/20 - Perturbinator, Train Loss: 0.0072, Val Loss: 0.0071\n",
      "Training Progress:  65%|██████▌   | 13/20 [02:05<01:07,  9.66s/it]2024-12-25 12:32:23 [INFO] - Epoch 14/20 - Perturbinator, Train Loss: 0.0069, Val Loss: 0.0069\n",
      "Training Progress:  70%|███████   | 14/20 [02:14<00:56,  9.37s/it]2024-12-25 12:32:32 [INFO] - Epoch 15/20 - Perturbinator, Train Loss: 0.0067, Val Loss: 0.0067\n",
      "Training Progress:  75%|███████▌  | 15/20 [02:23<00:45,  9.18s/it]2024-12-25 12:32:40 [INFO] - Epoch 16/20 - Perturbinator, Train Loss: 0.0065, Val Loss: 0.0064\n",
      "Training Progress:  80%|████████  | 16/20 [02:31<00:36,  9.05s/it]2024-12-25 12:32:49 [INFO] - Epoch 17/20 - Perturbinator, Train Loss: 0.0062, Val Loss: 0.0062\n",
      "Training Progress:  85%|████████▌ | 17/20 [02:40<00:27,  9.05s/it]2024-12-25 12:32:58 [INFO] - Epoch 18/20 - Perturbinator, Train Loss: 0.0060, Val Loss: 0.0059\n",
      "Training Progress:  90%|█████████ | 18/20 [02:49<00:17,  8.94s/it]2024-12-25 12:33:07 [INFO] - Epoch 19/20 - Perturbinator, Train Loss: 0.0057, Val Loss: 0.0056\n",
      "Training Progress:  95%|█████████▌| 19/20 [02:58<00:08,  9.00s/it]2024-12-25 12:33:16 [INFO] - Epoch 20/20 - Perturbinator, Train Loss: 0.0054, Val Loss: 0.0054\n",
      "Training Progress: 100%|██████████| 20/20 [03:07<00:00,  9.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Perturbinator\n",
    "model = Perturbinator(\n",
    "    gene_dim=978,\n",
    "    gene_hidden_dim=512,\n",
    "    drug_hidden_dim=128,\n",
    ")\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", patience=3\n",
    ")\n",
    "\n",
    "train_losses, val_losses = train_multimodal_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    epochs=20,\n",
    "    device=device,\n",
    "    gradient_clipping=1.0,\n",
    "    early_stopping_patience=5,\n",
    "    model_name=\"Perturbinator\",\n",
    "    use_mixed_precision=True,\n",
    ")\n",
    "\n",
    "# Log the final train and validation losses\n",
    "logging.debug(\"Final train and validation losses:\")\n",
    "logging.debug(\"Train Losses: \", train_losses)\n",
    "logging.debug(\"Validation Losses: \", val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSE': 0.005319735416295854,\n",
       " 'MAE': 0.0537196584045887,\n",
       " 'R2': 0.7902430295944214,\n",
       " 'PCC': 0.889129982598392}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = evaluate_multimodal_model(model, test_loader,criterion=criterion, device=device)\n",
    "logging.debug(\"Test Metrics: \", test_metrics)\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2384/3023150983.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get some predictions from the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Get some predictions from the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_data in test_loader:\n",
    "        features = batch_data[\"features\"].to(device)\n",
    "        labels = batch_data[\"labels\"].to(device)\n",
    "        smiles_list = batch_data[\"smiles_tokens\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = model(features, smiles_list)  # shape (B, #genes)\n",
    "\n",
    "        # Move to CPU for easy concatenation\n",
    "        preds = preds.cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        break\n",
    "\n",
    "# Select a random sample\n",
    "sample_idx = np.random.randint(0, len(preds))\n",
    "sample_features = features[sample_idx]\n",
    "sample_pred = preds[sample_idx]\n",
    "sample_label = labels[sample_idx]\n",
    "\n",
    "# Plot the predicted vs. actual gene expression\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(sample_label, sample_pred, alpha=0.7)\n",
    "plt.xlabel(\"Actual Gene Expression\")\n",
    "plt.ylabel(\"Predicted Gene Expression\")\n",
    "plt.title(\"Predicted vs. Actual Gene Expression\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Base Expression: \", sample_features[:5])\n",
    "print(\"Predicted Pert. Expression: \", sample_pred[:5])\n",
    "print(\"Actual Pert. Expression: \", sample_label[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5ARG45",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
